{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\"><b>Probabilistic Machine Learning</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of Contents\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. _Probabilistic_ Linear Regression \n",
    "\n",
    "- each response generated by a linear model plus some Gaussian noise\n",
    "\n",
    "$$ y = \\omega^T x + \\varepsilon, \\quad \\varepsilon\\sim N \\left(0,\\sigma^2\\right)$$\n",
    "\n",
    "\n",
    "- each response $y$ then becomes a draw from the following Gaussian:\n",
    "\n",
    "$$ y \\sim \\left(\\omega^T x,\\sigma^2\\right) $$\n",
    "\n",
    "- Probability of each response variable\n",
    "\n",
    "$$P(y \\mid x, \\omega)= N \\left(\\omega^T x,\\sigma^2\\right)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{1}{2\\sigma^2}\\left(y-\\omega^T x \\right)^2\\right)$$\n",
    "\n",
    "\n",
    "- Given observed data $ D=\\{(x_1,y_1),(x_2,y_2),\\cdots,(x_m,y_m)\\}$, we want to estimate the weight vector $\\omega$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1.1. Maximum Likelihood Solution\n",
    "\n",
    "- Log-likelihood:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\log L(\\omega) = \\log P(D \\mid \\omega) = \\log P(Y \\mid X, \\omega) &= \\log \\prod\\limits^m_{n=1}P(y_n \\mid x_n, \\omega)\\\\\n",
    "&= \\sum\\limits^m_{n=1}\\log P(y_n \\mid x_n, \\omega)\\\\\n",
    "&= \\sum\\limits^m_{n=1}\\log \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp{\\bigg[-\\frac{(y_n - \\omega^Tx_n)^2}{2\\sigma^2}\\bigg]}\\\\\n",
    "&= \\sum\\limits^m_{n=1}\\bigg\\{ -\\frac{1}{2}\\log \\left(2\\pi\\sigma^2 \\right) - \\frac{(y_n - \\omega^Tx_n)^2}{2\\sigma^2}\\bigg\\}\n",
    "\\end{align*}$$\n",
    "\n",
    "- Maximum Likelihood Solution:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\hat{\\omega}_{MLE} &= \\arg\\max_{\\omega}\\log P(D \\mid \\omega)\\\\\n",
    "&= \\arg\\max_{\\omega} \\;- \\frac{1}{2\\sigma^2}\\sum\\limits^m_{n=1}(y_n-\\omega^Tx_n)^2\\\\\n",
    "&= \\arg\\min_{\\omega} \\frac{1}{2\\sigma^2}\\sum\\limits^m_{n=1}(y_n-\\omega^Tx_n)^2\\\\\n",
    "&= \\arg\\min_{\\omega} \\sum\\limits^m_{n=1}(y_n-\\omega^Tx_n)^2\n",
    "\\end{align*}$$\n",
    "\n",
    "- It's equivalent to the least-squares objective for linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Maximum-a-Posteriori Solution\n",
    "\n",
    "- Let's assume a Gaussian prior distribution over the weight vector $\\omega$\n",
    "\n",
    "$$P(\\omega) \\sim N\\left(\\omega \\mid 0, \\lambda^{-1}I\\right) = \\frac{1}{(2\\pi)^{D/2}}\\exp\\left( -\\frac{\\lambda}{2} \\omega^T\\omega\\right)$$\n",
    "\n",
    "- Log posterior probability:\n",
    "\n",
    "$$\\log P(\\omega \\mid D) = \\log\\frac{P(\\omega)P(D \\mid \\omega)}{P(D)} = \\log P(\\omega) + \\log P(D \\mid \\omega) - \\underbrace{\\log P(D)}_{\\text{constant}}$$\n",
    "\n",
    "- Maximum-a-Posteriori Solution:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\hat{\\omega}_{MAP} &= \\arg\\max_{\\omega} \\log P(\\omega \\mid D)\\\\\n",
    "&= \\arg\\max_{\\omega}\\{\\log P(\\omega) + \\log P(D \\mid \\omega)\\}\\\\\n",
    "&= \\arg\\max_{\\omega}\\bigg\\{ -\\frac{D}{2}\\log (2\\pi) - \\frac{\\lambda}{2}\\omega^T\\omega + \\sum\\limits^m_{n=1}\\bigg\\{ -\\frac{1}{2}\\log\\left(2\\pi\\sigma^2\\right) - \\frac{\\left(y_n-\\omega^Tx_n\\right)^2}{2\\sigma^2} \\bigg\\} \\bigg\\}\\\\\n",
    "&= \\arg\\min_{\\omega}\\frac{1}{2\\sigma^2}\\sum\\limits^m_{n=1}\\left(y_n - \\omega^Tx_n\\right)^2 + \\frac{\\lambda}{2}\\omega^T\\omega \\quad \\text{ (ignoring constants and changing max to min)}\n",
    "\\end{align*}$$\n",
    "\n",
    "- For $\\sigma = 1$ (or some constant) for each input, it’s equivalent to the <font color='red'>regularized</font> least-squares objective\n",
    "\n",
    "\n",
    "- BIG Lesson: MAP $= l_2$ norm regulization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1.3. Summary: MLE vs MAP\n",
    "\n",
    "- MLE solution:\n",
    "\n",
    "$$\\hat{\\omega}_{MLE} = \\arg\\min_{\\omega}\\frac{1}{2\\sigma^2}\\sum\\limits^m_{n=1}(y_n - \\omega^Tx_n)^2$$\n",
    "\n",
    "- MAP solution:\n",
    "\n",
    "$$\\hat{\\omega}_{MLE} = \\arg\\min_{\\omega}\\frac{1}{2\\sigma^2}\\sum\\limits^m_{n=1}(y_n - \\omega^Tx_n)^2 + \\frac{\\lambda}{2}\\omega^T\\omega$$\n",
    "\n",
    "- Take-Home messages:\n",
    "\n",
    "    - MLE estimation of a parameter leads to unregularized solutions\n",
    "    \n",
    "    - MAP estimation of a parameter leads to regularized solutions\n",
    "    \n",
    "    - The prior distribution acts as a regularizer in MAP estimation\n",
    "\n",
    "\n",
    "- Note : for MAP, different prior distributions lead to different regularizers\n",
    "\n",
    "    - Gaussian prior on $\\omega$ regularizes the $l_2$ norm of $\\omega$\n",
    "    \n",
    "    - Laplace prior $\\exp(-C\\lVert\\omega\\rVert_1)$ on $\\omega$ regularizes the $l_1$ norm of $\\omega$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. _Probabilistic_ Linear Classification: Logistic Regression\n",
    "\n",
    "- Often we don’t just care about predicting the label $y$ for an example\n",
    "\n",
    "- Rather, we want to predict the label probabilities $P(y \\mid x, \\omega)$\n",
    "\n",
    "    - E.g., $P(y = +1 \\mid x, \\omega)$: the probability that the label is $+1$\n",
    "    \n",
    "    - In a sense, it's our confidence in the predicted label\n",
    "    \n",
    "- Probabilistic classification models allow us do that\n",
    "\n",
    "- Consider the following function $(y = -1/+1)$:\n",
    "\n",
    "$$P(y \\mid x, \\omega) = \\sigma(y\\omega^Tx) = \\frac{1}{1 + \\exp(-y\\omega^Tx)}$$\n",
    "\n",
    "<br>\n",
    "<img src=\"./image_files/MAP01.png\" width = 250>\n",
    "<br>\n",
    "\n",
    "- $\\sigma$ is the logistic function which maps all real number into $(0,1)$\n",
    "\n",
    "- This is the Logistic Regression model\n",
    "\n",
    "    - Misnomer: Logistic Regression is a classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.1. Logistic Regression\n",
    "\n",
    "- What does the decision boundary look like for Logistic Regression?\n",
    "\n",
    "- At the decision boundary labels $+1/-1$ becomes equiprobable\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(y= + 1 \\mid x, \\omega) &= P(y = -1 \\mid x, \\omega)\\\\\n",
    "\\frac{1}{1+\\exp(-\\omega^Tx)} &= \\frac{1}{1+\\exp(\\omega^Tx)}\\\\\n",
    "\\exp(-\\omega^Tx) &= \\exp(\\omega^Tx)\\\\\n",
    "\\omega^T x&= 0\n",
    "\\end{align*}$$\n",
    "\n",
    "- The decision boundary is therefore linear $\\implies$ Logistic Regression is a linearclassifier (note: it’s possible to kernelize and make it nonlinear)\n",
    "\n",
    "<br>\n",
    "<img src=\"./image_files/MAP02.png\" width = 250>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.2. Maximum Likelihood Solution\n",
    "\n",
    "- Goal: Want to estimate $\\omega$ from the data $D = \\{ (x_1, y_1), \\cdots, (x_m, y_m)\\}$\n",
    "\n",
    "\n",
    "- Log-likelihood:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\log L(\\omega) = \\log P(D \\mid \\omega) = \\log P(Y \\mid X, \\omega) &= \\log\\prod\\limits^m_{n=1}P(y_n \\mid x_n, \\omega)\\\\\n",
    "&= \\sum^m_{n=1}\\log P(y_n \\mid x_n, \\omega)\\\\\n",
    "&= \\sum^m_{n=1}\\log \\frac{1}{1+\\exp(-y_n\\omega^Tx_n)}\\\\\n",
    "&= \\sum^m_{n=1}-\\log\\left[1+\\exp(-y_n\\omega^Tx_n)\\right]\n",
    "\\end{align*}$$\n",
    "\n",
    "- Maximum Likelihood Solution: \n",
    "\n",
    "$$\\hat{\\omega}_{MLE} = \\arg\\max_{\\omega}\\log L(\\omega)$$\n",
    "\n",
    "- No closed-form solution exists but we can do gradient descent on $\\omega$\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\nabla_{\\omega} \\log L(\\omega) &= \\sum^m_{n=1} -\\frac{1}{1 + \\exp\\left(-y_n\\omega^Tx_n\\right)}\\exp\\left(-y_n\\omega^Tx_n\\right)(-y_nx_n)\\\\\n",
    "&= \\sum^m_{n=1} \\frac{1}{1 + \\exp\\left(y_n\\omega^Tx_n\\right)}y_nx_n\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.3. Maximum-a-Posteriori Solution\n",
    "\n",
    "- Let's assume a Gaussian prior distribution over the weight vector $\\omega$\n",
    "\n",
    "$$P(\\omega) = N(\\omega \\mid 0, \\lambda^{-1}I) = \\frac{1}{(2\\pi)^{D/2}}\\exp\\left( -\\frac{\\lambda}{2} \\omega^T\\omega \\right)$$\n",
    "\n",
    "- Maximum-a-Posteriori Solution: \n",
    "\n",
    "$$\\begin{align*}\n",
    "\\hat{\\omega}_{MAP} &= \\arg\\max_{\\omega} \\log P(\\omega \\mid D) \\\\\n",
    "&= \\arg\\max_{\\omega}\\{\\log P(\\omega) + \\log P(D \\mid \\omega) - \\underbrace{\\log P(D)}_{\\text{constant}} \\}&\\\\\n",
    "&= \\arg\\max_{\\omega}\\{\\log P(\\omega) + \\log P(D \\mid \\omega)\\}&\\\\\n",
    "&= \\arg\\max_{\\omega}\\bigg\\{ -\\frac{D}{2}\\log (2\\pi) - \\frac{\\lambda}{2}\\omega^T\\omega + \\sum\\limits^m_{n=1} - \\log\\left[1 + \\exp\\left(-y_n\\omega^Tx_n\\right)\\right] \\bigg\\}&\\\\\n",
    "&= \\arg\\min_{\\omega}\\sum\\limits^m_{n=1} \\log\\left[1 + \\exp\\left(-y_n\\omega^Tx_n\\right)\\right] + \\frac{\\lambda}{2}\\omega^T\\omega \\quad \\text{ (ignoring constants and changing max to min)}&\n",
    "\\end{align*}$$\n",
    "\n",
    "- BIG Lesson: MAP $= l_2$ norm regularization\n",
    "\n",
    "\n",
    "- No closed-form solution exists but we can do gradient descent on $\\omega$\n",
    "\n",
    "\n",
    "- See “[A comparison of numerical optimizers for logistic regression](http://research.microsoft.com/en-us/um/people/minka/papers/logreg/minka-logreg.pdf)” by Tom Minka on optimization techniques (gradient descent and others) for logistic regression (both MLE and MAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Summary: MLE vs MAP \n",
    "\n",
    "- MLE solution:\n",
    "\n",
    "$$\\hat{\\omega}_{MLE} = arg\\min_{\\omega}\\sum\\limits^m_{n=1}\\log[1 + \\exp(-y\\omega^Tx_n)]$$\n",
    "\n",
    "- MAP solution:\n",
    "\n",
    "$$\\hat{\\omega}_{MAP} = arg\\min_{\\omega}\\sum\\limits^m_{n=1}\\log[1 + \\exp(-y\\omega^Tx_n)] + \\frac{\\lambda}{2}\\omega^T\\omega$$\n",
    "\n",
    "\n",
    "- Take-home messages (we already saw these before)\n",
    "\n",
    "    - MLE estimation of a parameter leads to unregularized solutions\n",
    "    \n",
    "    - MAP estimation of a parameter leads to regularized solutions\n",
    "    \n",
    "    - The prior distribution acts as a regularizer in MAP estimation\n",
    "    \n",
    "    \n",
    "- Note: For MAP, different prior distributions lead to different regularizers\n",
    "\n",
    "    - Gaussian prior on $\\omega$ regularizes the $l_2$ norm of $\\omega$\n",
    "    \n",
    "    - Laplace prior $\\exp(-C\\lVert\\omega\\rVert_1)$ on $\\omega$ regularizes the $l_1$ norm of $\\omega$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Matlab",
   "language": "matlab",
   "name": "matlab"
  },
  "language_info": {
   "codemirror_mode": "octave",
   "file_extension": ".m",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-matlab",
   "name": "matlab",
   "version": "0.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
