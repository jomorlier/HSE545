{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<font size='6'><b>Gaussian Process Regression</b></font><br><br>\n",
    "\n",
    "- S. Lall at Stanford (http://lall.stanford.edu/)\n",
    "    - <a href=\"./reference_files/continuous_random_vectors.pdf\" target=\"_blank\">continuous random vectors.pdf</a>\n",
    "    - <a href=\"./reference_files/conditional_density.pdf\" target=\"_blank\">conditional density.pdf</a>\n",
    "    - <a href=\"./reference_files/mmse_estimation.pdf\" target=\"_blank\">mmse estimation.pdf</a>\n",
    "    - <a href=\"./reference_files/linear_model.pdf\" target=\"_blank\">linear model.pdf</a>\n",
    "    \n",
    "    \n",
    "- Jeffrey Miller at Harvard (http://jwmi.github.io/)\n",
    "\n",
    "\n",
    "- Andrew Ng at Stanford (http://cs229.stanford.edu/)\n",
    "    - <a href=\"./reference_files/gaussians.pdf\" target=\"_blank\">Multivariate Gaussian.pdf</a>\n",
    "    - <a href=\"./reference_files/more_on_gaussians.pdf\" target=\"_blank\">More on Gaussian Distribution.pdf</a>\n",
    "    - <a href=\"./reference_files/cs229-gaussian_processes.pdf\" target=\"_blank\">Gaussian Processes.pdf</a>\n",
    "\n",
    "- Philipp Henning at Max Planck Institute for Intelligent Systems (https://pn.is.tuebingen.mpg.de/people/phennig)\n",
    "    - <a href=\"./reference_files/hennig_slides1.pdf\" target=\"_blank\">hennig slides1.pdf</a>\n",
    "\n",
    "<table style=\"border-style: hidden; border-collapse: collapse;\" width = \"80%\"> \n",
    "    <tr style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "        <td width = 60% style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "\n",
    "        </td>\n",
    "        <td width = 20%>\n",
    "        Collected by Seungchul Lee<br>iSystems Design Lab<br>http://isystems.unist.ac.kr/<br>UNIST\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Table of Contents\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Gaussian random vectors\n",
    "\n",
    "Suppose $x \\sim N(\\mu, \\Sigma)$, here $\\Sigma = \\Sigma^T$ and $\\Sigma > 0$. \n",
    "\n",
    "$$ p(x) = \\frac{1}{(2\\pi)^{\\frac{n}{2}}(\\text{det}\\,\\Sigma)^{\\frac{1}{2}}} \\exp \\left( -\\frac{1}{2} (x-\\mu)^T \\Sigma^{-1} (x-\\mu) \\right)$$\n",
    "\n",
    "<img src='./image_files/Gaussian_2D.png' width = 400>\n",
    "\n",
    "## 1.1. The marginal pdf of a Gaussian (is Gaussian)\n",
    "\n",
    "Suppose $x \\sim N(\\mu, \\Sigma)$, and\n",
    "\n",
    "$$ x = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} \\quad \\mu = \\begin{bmatrix} \\mu_1 \\\\ \\mu_2 \\end{bmatrix} \\quad \\Sigma = \\begin{bmatrix} \\Sigma_{11} & \\Sigma_{12} \\\\ \\Sigma_{21} & \\Sigma_{22} \\end{bmatrix}  $$\n",
    "\n",
    "Let's look at the component $x_1 = \\begin{bmatrix} I & 0 \\end{bmatrix} x$\n",
    "\n",
    "- mean of $x_1$\n",
    "\n",
    "$$E x_1 = \\begin{bmatrix} I & 0 \\end{bmatrix} \\mu= \\mu_1$$\n",
    "\n",
    "- convariance of $x_1$\n",
    "\n",
    "$$\\text{cov}(x_1) = \\begin{bmatrix} I & 0 \\end{bmatrix} \\Sigma \\begin{bmatrix} I \\\\ 0 \\end{bmatrix} = \\Sigma_{11}$$\n",
    "\n",
    "- In fact, the random variable $x_1$ is Gaussian, (this is not obvious.)\n",
    "\n",
    "<table style=\"border-style: hidden; border-collapse: collapse;\" width = \"90%\"> \n",
    "    <tr style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "        <td width = 45% style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "<img src='./image_files/Multivariate_normal_sample.svg' width = 300>\n",
    "        </td>\n",
    "        <td width = 45%>\n",
    "<img src='./image_files/noyfls.png' width = 300>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "## 1.2. Linear transformation of Gaussian (is Gaussian)\n",
    "\n",
    "Suppose $x \\sim N(\\mu_x, \\Sigma_x)$. Consider the linear function of $x$\n",
    "\n",
    "$$y = Ax + b$$\n",
    "\n",
    "- We already know how means and covariances transform. We have\n",
    "\n",
    "<br>\n",
    "$$\\mathbf{E}y = A \\, \\mathbf{E}x + b \\qquad \\mathbf{cov}(y) = A \\, \\mathbf{cov}(x) \\, A^{T}$$\n",
    "\n",
    "$$\\mu_y = A \\mu_x + b \\quad \\qquad \\Sigma_y = A \\Sigma_x A^{T}$$\n",
    "\n",
    "- The amazing fact is that $y$ is Gaussian.\n",
    "\n",
    "<img src='./image_files/LinearXform.png' width = 350>\n",
    "\n",
    "## 1.3. Components of a Gaussian random vector (is Gaussian)\n",
    "\n",
    "Suppose $x \\sim N(\\mu, \\Sigma)$, and let $c \\in \\mathbb{R}^n$ be a unit vector\n",
    "\n",
    "Let $y = c^T x$\n",
    "\n",
    "- $y$ is the component of $x$ in the direction $c$\n",
    "\n",
    "\n",
    "- $y$ is Gaussian, whit $\\mathbf{E} y = 0$ and $\\mathbf{cov}= c^T \\Sigma c$\n",
    "\n",
    "\n",
    "- So $\\mathbf{E}\\left(y^2\\right) = c^T \\Sigma c$\n",
    "\n",
    "\n",
    "- The unit vector $c$ that maximizes $c^T \\Sigma c$ is the eigenvector of $\\Sigma$ with the largest eigenvalue. Then\n",
    "\n",
    "<br>\n",
    "$$\\mathbf{E}\\left(y^2\\right) = \\lambda_{\\text{max}}$$\n",
    "\n",
    "<img src='./image_files/pca-example.png' width = 300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Conditional pdf for a Gaussian (is Gaussian)\n",
    "\n",
    "Suppose $x \\sim N(\\mu, \\Sigma)$, and\n",
    "\n",
    "$$ x = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} \\quad \\mu = \\begin{bmatrix} \\mu_1 \\\\ \\mu_2 \\end{bmatrix} \\quad \\Sigma = \\begin{bmatrix} \\Sigma_{11} & \\Sigma_{12} \\\\ \\Sigma_{21} & \\Sigma_{22} \\end{bmatrix}  $$\n",
    "\n",
    "Suppose we measure $x_2 = y$. We would like to find the conditional pdf of $x_1$ given $x_2 = y$\n",
    "\n",
    "- Is it Gaussian ?\n",
    "\n",
    "\n",
    "- What is the conditional mean $\\mathbf{E}(x_1 \\mid x_2= y)$ ?\n",
    "\n",
    "\n",
    "- What is the conditional covariance $\\mathbf{cov}(x_1 \\mid x_2= y)$ ?\n",
    "\n",
    "By the completion of squares formula\n",
    "\n",
    "$$\\Sigma^{-1} = \\begin{bmatrix} I & 0 \\\\ -\\Sigma^{-1}_{22}\\Sigma_{21} & I \\end{bmatrix} \n",
    "\\begin{bmatrix} \\left( \\Sigma_{11} - \\Sigma_{12}\\Sigma^{-1}_{22}\\Sigma_{21} \\right)^{-1} & 0 \\\\ 0 & \\Sigma^{-1}_{22}  \\end{bmatrix} \n",
    "\\begin{bmatrix} I & -\\Sigma_{12}\\Sigma^{-1}_{22} \\\\ 0 & I \\end{bmatrix}$$\n",
    "\n",
    "If $x \\sim N(0, \\Sigma)$, then the conditional pdf of $x_1$ given $x_2 = y$ is Gaussian\n",
    "\n",
    "- The conditional mean is\n",
    "\n",
    "$$\\mathbf{E}(x_1 \\mid x_2= y) = \\Sigma_{12} \\Sigma^{-1}_{22}\\,y$$\n",
    "\n",
    "$\\quad \\;$ It is a linear function of $y$.\n",
    "\n",
    "- The conditional convariance is\n",
    "\n",
    "$$\\mathbf{cov}(x_1 \\mid x_2= y) = \\Sigma_{11} - \\Sigma_{12}\\Sigma^{-1}_{22}\\Sigma_{21} < \\Sigma_{11}$$\n",
    "\n",
    "$\\quad \\;$ It is not a function of $y$. Instead, it is constant. \n",
    "\n",
    "$\\quad \\;$ Conditional confidence intervals are narrower. i.e., measuring $x_2$ gives information about $x_1$\n",
    "\n",
    "<table style=\"border-style: hidden; border-collapse: collapse;\" width = \"80%\"> \n",
    "    <tr style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "        <td width = 40% style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "<img src='./image_files/con_gau_02.png' width = 300>\n",
    "        </td>\n",
    "        <td width = 40%>\n",
    "<img src='./image_files/con_gau_05.png' width = 280>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Summary\n",
    "\n",
    "- Closure under multiplication\n",
    "    - multiple Gaussian factors form a Gaussian\n",
    "\n",
    "<img src = \"./image_files/multi.png\" width=500>\n",
    "\n",
    "- Closure under linear maps\n",
    "    - linear maps of Gaussians are Gaussians\n",
    "\n",
    "<img src = \"./image_files/linearmap.png\" width=500>\n",
    "\n",
    "- Closure under marginalization\n",
    "    - projections of Gausians are Gaussian\n",
    "\n",
    "<img src = \"./image_files/marginalization.png\" width=500>\n",
    "\n",
    "- Closure under conditioning\n",
    "    - cuts throuhg Gaussians are Gaussians\n",
    "\n",
    "<img src = \"./image_files/conditioning.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Linear model\n",
    "\n",
    "$$y = Ax + \\omega$$\n",
    "\n",
    "- $x$ and $\\omega$ are independent\n",
    "\n",
    "\n",
    "- We have induced pdfs $p_x$ and $p_{\\omega}$\n",
    "\n",
    "Estimation problem: we measure $y = y_{\\text{meas}}$ and would like to estimate $x$.\n",
    "\n",
    "<br>\n",
    "$$\\begin{bmatrix} x \\\\ y\\end{bmatrix} = \\begin{bmatrix} I & 0 \\\\ A &I\\end{bmatrix} \\begin{bmatrix} x \\\\ \\omega\\end{bmatrix}$$\n",
    "\n",
    "- We will measure $y = y_{\\text{meas}}$ and estimate $x$\n",
    "\n",
    "\n",
    "- To do this, we would like the conditional pdf of $x \\mid y = y_{\\text{meas}}$\n",
    "\n",
    "\n",
    "- For this, we need the joint pdf of $x$ and $y$\n",
    "\n",
    "## 2.1. Linear measurements with Gaussian noise\n",
    "\n",
    "<br>\n",
    "$$y = Ax + \\omega$$\n",
    "\n",
    "- Suppose $x \\sim N(0, \\Sigma_x)$ and $\\omega \\sim N(0, \\Sigma_{\\omega})$\n",
    "\n",
    "\n",
    "- So $\\begin{bmatrix} x \\\\ y\\end{bmatrix}$ is Gaussian with mean and covariance\n",
    "\n",
    "$$ \\mathbf{E}\\begin{bmatrix} x \\\\ y\\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0\\end{bmatrix}, \\quad \n",
    "\\mathbf{cov} \\begin{bmatrix} x \\\\ y\\end{bmatrix} = \\begin{bmatrix} \\Sigma_x & \\Sigma_x A^T\\\\ A \\Sigma_x & A\\Sigma_x A^T + \\Sigma_{\\omega}\\end{bmatrix}$$\n",
    "\n",
    "The MMSE estimate of $x$ given $y = y_{\\text{meas}}$ is\n",
    "\n",
    "<br>\n",
    "$$\\hat{x}_{\\text{mmse}} = \\mathbf{E}(x \\mid y= y_{\\text{meas}}) = \\Sigma_{12} \\Sigma^{-1}_{22}\\,y =  \\Sigma_x A^T \\left(  A\\Sigma_x A^T + \\Sigma_{\\omega}\\right)^{-1} \\, y_{\\text{meas}}$$\n",
    "\n",
    "The posterior covariance of $x$ given $y = y_{\\text{meas}}$ is\n",
    "\n",
    "<br>\n",
    "$$ \\mathbf{cov}(x \\mid y= y_{\\text{meas}}) = \\Sigma_x -   \\Sigma_x A^T \\left(  A\\Sigma_x A^T + \\Sigma_{\\omega}\\right)^{-1} A \\Sigma_x < \\Sigma_x$$\n",
    "\n",
    "<br>\n",
    "__Example: linear measurements with Gaussian noise__\n",
    "\n",
    "<img src='./image_files/linear meas 2.png' width = 600>\n",
    "\n",
    "### 2.1.1. Signal-to-noise ratio\n",
    "\n",
    "<table style=\"border-style: hidden; border-collapse: collapse;\" width = \"80%\"> \n",
    "    <tr style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "        <td width = 40% style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "<img src='./image_files/small noise.png' width = 200>\n",
    "        </td>\n",
    "        <td width = 40%>\n",
    "<img src='./image_files/large noise.png' width = 200>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "### 2.1.2. Matrix equality\n",
    "\n",
    "$$ \\mathbf{cov}(x \\mid y= y_{\\text{meas}}) = \\Sigma_x -   \\Sigma_x A^T \\left(  A\\Sigma_x A^T + \\Sigma_{\\omega}\\right)^{-1} A \\Sigma_x = \\left( \\Sigma_{x}^{-1} +  A^T\\Sigma_{\\omega}^{-1} A\\right)^{-1}$$\n",
    "\n",
    "<br>\n",
    "$$L=\\Sigma_{12}\\Sigma_{22}^{-1} = \\Sigma_x A^T \\left(  A\\Sigma_x A^T + \\Sigma_{\\omega} \\right)^{-1} =\n",
    "\\left( \\Sigma_{x}^{-1} + A^T \\Sigma_{\\omega}^{-1} A \\right)^{-1} A^T \\Sigma_{\\omega}^{-1}$$\n",
    "\n",
    "### 2.1.3. Non-zero means\n",
    "\n",
    "Suppose $x \\sim N(\\mu_x, \\Sigma_x)$ and $\\omega \\sim N(\\mu_{\\omega}, \\Sigma_{\\omega})$\n",
    "\n",
    "The MMSE estimate of $x$ given $y = y_{\\text{meas}}$ is\n",
    "\n",
    "$$\\hat{x}_{\\text{mmse}} =\\mu_x + \\Sigma_x A^T \\left(  A\\Sigma_x A^T + \\Sigma_{\\omega}\\right)^{-1} \\, \\left(y_{\\text{meas}} - A\\mu_x - \\mu_{\\omega} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. MMSE and Least-Squares\n",
    "\n",
    "For Gaussian, the MMSE estimate is equal to the MAP estimate.\n",
    "\n",
    "The least-squares approach minimizes\n",
    "\n",
    "$$\\lVert y - Ax\\rVert^2 = \\sum_{i = 1}^{m}\\left( y_i - a_i^T x\\right)^2$$\n",
    "\n",
    "where $A = [a_1 \\; a_2\\; \\cdots \\; a_m]^T$\n",
    "\n",
    "## 2.3. Weighted norms\n",
    "\n",
    "Suppose instead we minimize\n",
    "\n",
    "$$\\sum_{i = 1}^{m} \\omega_i \\left( y_i - a_i^T x\\right)^2$$\n",
    "\n",
    "where $\\omega_1,\\; \\omega_2,\\; \\cdots,\\; \\omega_m$ provide weights\n",
    "\n",
    "\n",
    "\n",
    "<table style=\"border-style: hidden; border-collapse: collapse;\" width = \"60%\"> \n",
    "<tr style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "        <td width = 30% style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "$$\\rVert x \\lVert_2 = \\sqrt{x^Tx}$$\n",
    "        </td>\n",
    "        <td width = 30%>\n",
    "$$\\rVert x \\lVert_W = \\sqrt{x^T W x}$$\n",
    "        </td>\n",
    "    </tr>\n",
    "    \n",
    "    <tr style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "        <td width = 30% style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "<img src='./image_files/2 norm.png' width = 200>\n",
    "        </td>\n",
    "        <td width = 30%>\n",
    "<img src='./image_files/weighted norm.png' width = 200>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "### 2.3.1. Weighted Least-Squares\n",
    "\n",
    "$$\\text{minimize} \\; \\lVert y_{\\text{meas}} - Ax\\rVert_W$$\n",
    "\n",
    "Then the optimum $x_{\\text{wls}}$ is\n",
    "\n",
    "$$x_{\\text{wls}} = (A^TWA)^{-1}A^TW\\, y_{\\text{meas}}$$\n",
    "\n",
    "- $Ax_{\\text{wls}}$ is the closest (in weighted-norm) point in range A to $y_{\\text{meas}}$\n",
    "\n",
    "<img src='./image_files/weighted.png' width = 400>\n",
    "\n",
    "### 2.3.2. MMSE and Weighted Least-Squares\n",
    "\n",
    "Suppose we choose weight $W = \\Sigma_{\\omega}^{-1}$, then \n",
    "\n",
    "WLS solution is\n",
    "\n",
    "$$x_{\\text{wls}} = (A^T\\Sigma_{\\omega}^{-1}A)^{-1}A^T\\Sigma_{\\omega}^{-1}\\, y_{\\text{meas}}$$\n",
    "\n",
    "MMSE estimate is\n",
    "\n",
    "$$x_{\\text{mmse}} = \\left( \\Sigma_{x}^{-1} + A^T \\Sigma_{\\omega}^{-1} A \\right)^{-1} A^T \\Sigma_{\\omega}^{-1} y_{\\text{meas}}$$\n",
    "\n",
    "- as the prior covariance $\\Sigma_x \\rightarrow \\infty$, the MMSE estimate tends to the WLS estimate\n",
    "\n",
    "\n",
    "- If $\\Sigma_{\\omega} = I$ then MMSE tends to usual least-squares solution as $\\Sigma_x \\rightarrow \\infty$\n",
    "\n",
    "\n",
    "- the weighted norm heavily penalizes the residual $y-Ax$ in low-noise direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gaussian process regression\n",
    "\n",
    "## 3.1. Bayesian linear regression\n",
    "\n",
    "reference: http://web.cse.ohio-state.edu/~kulis/teaching/788_sp12/scribe_notes/lecture5.pdf\n",
    "\n",
    "- Why not use MLE? overfitting\n",
    "\n",
    "- Why not use MAP? No representation of uncertainty.\n",
    "\n",
    "- Why Bayesian? we can optimize loss function\n",
    "\n",
    "$\\quad \\;$ It gives us $p(y \\mid x, D)$. This is what we really want\n",
    "\n",
    "Motivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<center><iframe src=\"https://www.youtube.com/embed/dtkGq9tdYcI\"\n",
       "width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen></iframe></center>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<center><iframe src=\"https://www.youtube.com/embed/dtkGq9tdYcI\"\n",
    "width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen></iframe></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight-space view\n",
    "\n",
    "<img src = \"./image_files/gp.gif\" width=700>\n",
    "\n",
    "Function-space view will be discussed at the following section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Gaussian process regression\n",
    "\n",
    "<br>\n",
    "$$D = \\left\\{ (x_1, y_1),\\cdots,(x_n, y_n)\\right\\}, \\; x_i \\in \\mathbb{R}^d, \\,y \\in \\mathbb{R}$$\n",
    "\n",
    "<br>\n",
    "$$ p(y_i \\mid x_i, \\omega) = N(y_i \\mid \\omega^T x_i, \\sigma^2) \\qquad \\omega \\sim N(0,\\gamma I)$$\n",
    "\n",
    "<br>\n",
    "$$ y_i = \\omega^T x_i + \\varepsilon_i$$\n",
    "\n",
    "<br>\n",
    "$$\\implies \\forall x \\in \\mathbb{R}^d, \\; \\omega^Tx \\;\\text{  is univariate Gaussian} $$ \n",
    "\n",
    "Then $Z_x = x^T \\omega$ is a GP on $S = \\mathbb{R}^d$\n",
    "\n",
    "$$\\mu (x) = \\mathbf{E}\\,Z_x = 0$$\n",
    "\n",
    "\n",
    "$$k(x,x') = \\mathbf{cov}\\,(Z_x, Z_{x'})= \\mathbf{E}\\, Z_x Z_{x'} - \\mathbf{E}\\, Z_x \\mathbf{E}\\,Z_{x'} = \\mathbf{E}\\,x^T \\omega \\omega^T x' = x^T \\mathbf{E}\\,\\left(\\omega \\omega^T \\right) x' = \\gamma x^Tx'$$\n",
    "\n",
    "With non-linear basis\n",
    "\n",
    "- $\\tilde x_i = \\varphi (x_i)$\n",
    "\n",
    "Generally\n",
    "\n",
    "$$ y_i = Z_{x_i} + \\varepsilon_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<center><iframe src=\"https://www.youtube.com/embed/upJ31CIVWZo\" \n",
       "width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen></iframe></center>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<center><iframe src=\"https://www.youtube.com/embed/upJ31CIVWZo\" \n",
    "width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen></iframe></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The key step of GP regresssion__\n",
    "\n",
    "$$ y_i = Z_{x_i} + \\varepsilon_i$$\n",
    "\n",
    "\n",
    "Let $z \\sim N(\\mu, K) \\in \\mathbb{R}^n$, and $\\varepsilon \\sim N(0,\\sigma^2I) \\in \\mathbb{R}^n$ are independent \n",
    "\n",
    "\n",
    "$$y = z + \\varepsilon \\sim N(\\mu, \\underbrace{K + \\sigma^2 I}_{C})$$\n",
    "\n",
    "\n",
    "$a = [1,\\cdots, l]^T$ and $b = [l+1,\\cdots, n]^T$\n",
    "\n",
    "$$y = \\begin{bmatrix} y_a \\\\ y_b\\end{bmatrix}$$\n",
    "\n",
    "$$\\mu = \\begin{bmatrix} \\mu_a \\\\ \\mu_b \\end{bmatrix} \\quad\n",
    "C = \\begin{bmatrix} C_{aa} & C_{ab} \\\\ C_{ba} & C_{bb}\\end{bmatrix} \\\\\n",
    "K = \\begin{bmatrix} K_{aa} & K_{ab} \\\\ K_{ba} & K_{bb} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Conditional distribution $p(y_a \\mid y_b) \\sim N(m,D)$\n",
    "\n",
    "where\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "m &= \\mu_a + C_{ab}C_{bb}^{-1}(y_b - \\mu_b) \\\\\n",
    "D &= C_{aa} - C_{ab}C_{bb}^{-1}C_{ba}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "$$\n",
    "\\begin{align*}\n",
    "m &= \\mu_a + K_{ab}(K_{bb}+\\sigma^2I)^{-1}(y_b - \\mu_b) \\\\\n",
    "D &= (K_{aa} + \\sigma^2 I) - K_{ab}(K_{bb}+\\sigma^2I)^{-1}K_{ba}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<center><iframe src=\"https://www.youtube.com/embed/UH1d2mxwet8\" \n",
       "width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen></iframe></center>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<center><iframe src=\"https://www.youtube.com/embed/UH1d2mxwet8\" \n",
    "width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen></iframe></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<center><iframe src=\"https://www.youtube.com/embed/JdZr74mtZkU\" \n",
       "width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen></iframe></center>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<center><iframe src=\"https://www.youtube.com/embed/JdZr74mtZkU\" \n",
    "width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen></iframe></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function-space view\n",
    "- Squared exponential (~RBF) kernel\n",
    "\n",
    "<img src = \"./image_files/gp2.gif\" width=700>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.3.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
