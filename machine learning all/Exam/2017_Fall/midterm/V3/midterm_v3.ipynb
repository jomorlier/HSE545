{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<center><font size='6'><b>Introduction to Machine Learning</b></font></center>\n",
    "\n",
    "<table style=\"border-style: hidden; border-collapse: collapse;\" width = \"100%\"> \n",
    "    <tr style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "        <td width = 65% style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "\n",
    "        </td>\n",
    "        <td width = 35%>\n",
    "        by Prof. Seungchul Lee<br>iSystems Design Lab<br>http://isystems.unist.ac.kr/<br>UNIST\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem1\n",
    "\n",
    "In this problem, we want to explore a reflection transformation of $\\vec X$, which produces a mirror vector $\\vec Z$ with respect to vector $\\vec V$. See Figure 1.\n",
    "\n",
    "<br>\n",
    "<center><img src=\"./image_files/LA01.png\", width = 250></center>\n",
    "<center>Figure 1</center>\n",
    "\n",
    "(a) Is this reflection transformation linear?\n",
    "\n",
    "(b) If yes, find matrix $M$ such that $Z = MX$ using the concept of a projection.\n",
    "\n",
    "(c) If yes, find matrix $M$ such that $Z = MX$ using the concept of eigen values and vectors. \n",
    "\n",
    "(d) For $V = \\begin{bmatrix} 1 & 1 \\end{bmatrix}^T$, compute $M$ and its eigenvalues/eigenvectors (here, vector $\\vec V$ is a vector with 45 degree angle with x-axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "\n",
    "Let $R = R(\\theta)$ be a rotation matrix with rotational angle of $\\theta$ in $\\mathbb{R}^n$.\n",
    "\n",
    "(a) Prove that $$R^TR=I \\;\\text{in}\\; \\mathbb{R}^n$$\n",
    "    \n",
    "\n",
    "Hint: Euclidian distances of vectors are preserved after a rotational operation. (_i.e._, $\\lVert Rx \\rVert = \\lVert x \\rVert$ for any $x \\in \\mathbb{R}^n$ ) \n",
    "<br><br>\n",
    "\n",
    "(b) Prove that $$R^T(\\theta) = R^{-1}(\\theta) = R(-\\theta)\\;\\text{in}\\;\\mathbb{R}^n$$\n",
    "\n",
    "(c) Show that column vectors in $R$ are orthogonal in $\\mathbb{R}^n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem3\n",
    "\n",
    "Permutation matrices. A square matrix $A$ is called a permutation matrix if it satisfies the following three properties:\n",
    "\n",
    "- all elements of $A$ are either zero or one\n",
    "- each column of $A$ contains exactly one element equal to one\n",
    "- each row of $A$ contains exactly one element equal to one.\n",
    "\n",
    "The matrices,\n",
    "\n",
    "$$A_1 = \\begin{bmatrix}\n",
    "0 & 1 & 0\\\\\n",
    "0 & 0 & 1 \\\\\n",
    "1& 0 & 0 \n",
    "\\end{bmatrix}, \\quad\n",
    "A_2 = \\begin{bmatrix}\n",
    "0 & 1 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 1\\\\\n",
    "0 & 0& 1 & 0 \\\\\n",
    "1 & 0 & 0 & 0\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "are examples of permutation matrices. A less formal definition is the following: a permutation matrix is the identity matrix with its rows reordered.\n",
    "\n",
    "(a) Let $A$ be an $n \\times n$ permutation matrix. Give a simple description of the relation between a $n$-vector $x$ and $f(x) = Ax$ in words.\n",
    "\n",
    "(b) Prove that columns of a permutation matrix is orthogonal.\n",
    "\n",
    "(c) We can also define a second linear function $g(x) = A^Tx$ in terms of the same permutation matrix. What is the relation between $g$ and $f$ ?\n",
    "\n",
    "Hint: Use the result of (b)\n",
    "\n",
    "(d) Describe meaning of columns and rows of a permutation matrix. Then, explain the result of (c)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4\n",
    "\n",
    "The regularized least-squares problem has the form\n",
    "\n",
    "<br>\n",
    "$$ \\min_{\\theta} \\;\\lVert A\\theta -y\\rVert_2^2 + \\lambda \\lVert \\theta \\rVert_2^2$$\n",
    "\n",
    "(a) Show that the solution is given by\n",
    "<br><br>\n",
    "$$ \\hat{\\theta} = \\left( A^T A + \\lambda I_n \\right)^{-1} A^T y $$\n",
    "* Do not use the method of Lagrangian multipliers\n",
    "<br><br>\n",
    "\n",
    "(b) Write down a gradient descent algorithm for the given optimization problem.\n",
    "\n",
    "Hint: Note that $$ \\;\\lVert A\\theta -y\\rVert_2^2 = (A\\theta - y)^T(A\\theta - y)$$\n",
    "\n",
    "Then, you can differentiate the above equation to compute the gradient. \n",
    "Likewise, you can compute the gradient of the regularizer.\n",
    "\n",
    "(c) Based on the result of (b), describe the role of the regularizer term.\n",
    "\n",
    "Hint: Gradient $g$ is computed by $ g = g_{\\text{projection}} + g_{\\text{regularizer}} $\n",
    "\n",
    "(d) Describe the results of (a) and (b) have the same meaning.\n",
    "\n",
    "(e) Find and draw an approximated curve of the given data points in Python using your gradient descent algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5\n",
    "\n",
    "a) Suppose $x_i$'s are features. We might conclude that feature $x_j$ is not useful or redundant when \n",
    "\n",
    "$$ P(y \\mid x_1, ... x_{j-1}, x_j, x_{j+1}, ... x_N) = P(y \\mid x_1, ... x_{j-1}, x_{j+1}, ... x_N).$$ \n",
    "\n",
    "Describe the reason.\n",
    "\n",
    "b) In our class, we learn that meaningful features cna be selected using Lasso. Describe why Lasso selects meaningful features using the above equation.\n",
    "\n",
    "\n",
    "c) Load '`data_input.pkl`' and '`data_target.pkl`'. Using this data set, build the linear classifier using Lasso.\n",
    "\n",
    "d) After applying Lasso, we can select meaningful features. Then, we again apply Lasso to refine our features. Such techniques are called recursive feature elimination. Write the code that recursively apply Lasso to select features and plot the number of features selected at each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 6\n",
    "(a) Figure 2 shows loss functions of each classifier that you have learned in class.\n",
    "\n",
    "<center><img src=\"./image_files/hinge.png\", width = 500></center>\n",
    "<center>Figure 2</center>\n",
    "\n",
    "Here, hinge loss is defined by \n",
    "\n",
    "$$\\text{hinge}(y, t) = \\max(0, 1- t \\cdot y)$$\n",
    "\n",
    "\n",
    "Observe that the hinge loss function does not impose any loss when the inner product between the target $t$ and the prediction $y$ is greater than 1. Actually, support vector machine (SVM) can be coded as the following optimizaiton problem by setting classifier's loss to the hinge loss function: \n",
    "\n",
    "$$ J(\\omega) = \\sum_{i=1}^{m} \\text{hinge} \\left(y^{(i)},\\; f \\left(x^{(i)}; \\omega \\right) \\right) $$\n",
    "\n",
    "where $ x^{(i)} $ is the given input data and $ y^{(i)} $ is the corresponding label.\n",
    "Explain why using the concept of maximum margin.\n",
    "\n",
    "\n",
    "Hint: Think of what the value of $ y^{(i)} \\cdot f\\left(x^{(i)}; \\omega \\right) $ is when the input data $ x^{(i)} $ is located inside the margin or outside the margin.\n",
    "\n",
    "(b) Using Figure 2, describe why support vector machine is robust to outliers.\n",
    "\n",
    "(c) Write the code that optimizing the hinge loss function to build a classifier. Then plot the decision boundary of your classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 7\n",
    "\n",
    "You will use K-means to compress an image by reducing the number of colors it contains.\n",
    "\n",
    "(a) Image Representation\n",
    "\n",
    "The data for this problem contains a 128-pixel by 128-pixel TIFF image named \"bird.tiff.\" It looks like the picture in Figure 3.\n",
    "\n",
    "<br>\n",
    "<img src=\"./image_files/bird.tiff\", width = 250>\n",
    "<center>Figure 3</center>\n",
    "<br>\n",
    "\n",
    "In a straightforward 24-bit color representation of this image, each pixel is represented as three 8-bit numbers (ranging from 0 to 255) that specify red, green and blue (RGB) intensity values. Our bird photo contains thousands of colors, but we'd like to reduce that number to 16. By making this reduction, it would be possible to represent the photo in a more efficient way by storing only the RGB values of the 16 colors present in the image.\n",
    "\n",
    "In this problem, you will use K-means to reduce the color count to $k = 16$. That is, you will compute 16 colors as the cluster centroids and replace each pixel in the image with its nearest cluster centroid color.\n",
    "\n",
    "(b) K-means in Python\n",
    "\n",
    "<a href = \"https://www.dropbox.com/s/tibygtcahpw9hmk/bird.tiff?dl=0\"> download bird.tiff </a>\n",
    "\n",
    "In Matlab, load the image into your program with the following command:\n",
    "\n",
    "```octave \n",
    "im = imread('bird.tiff');\n",
    "A = double(im);\n",
    "imshow(im)\n",
    "```\n",
    "This creates a three-dimensional matrix $A$ whose first two indices identify a pixel position and whose last index represents red, green, or blue. For example, $A(50, 33, 3)$ gives you the blue intensity of the pixel at position $y = 50, x = 33$. (The y-position is given first, but this does not matter so much in our example because the $x$ and $y$ dimensions have the same size).\n",
    "\n",
    "Your task is to compute 16 cluster centroids from this image, with each centroid being a vector of length three that holds a set of RGB values. Here is the K-means algorithm as it applies to this problem:\n",
    "\n",
    "(c) K-means algorithm\n",
    "\n",
    "> 1. For initialization, sample 16 colors randomly from the original picture. There are your \n",
    "$k$ means $\\mu_1, \\mu_2, \\cdots, \\mu_k$.\n",
    "<br><br>\n",
    "> 2. Go through each pixel in the small image and calculate its nearest mean.\n",
    " <br><br>\n",
    " $$c^{(i)} = \\text{arg} min_j \\lVert x^{(i)}-\\mu_j \\rVert^2$$\n",
    " <br><br>\n",
    "> 3. Update the values of the means based on the pixels assigned to them. \n",
    "<br><br>\n",
    "$$\\mu_j = \\frac{\\sum\\limits_i^m 1\\{c^{(i)} = j\\}x^{(i)}}{\\sum\\limits_i^m 1\\{c^{(i)} = j\\}}$$\n",
    "<br><br>\n",
    ">4. Repeat steps 2 and 3 until convergence. This should take between 30 and 100 iterations. You can either run the loop for a preset maximum number of iterations, or you can decide to terminate the loop when the locations of the means are no longer changing by a significant amount.\n",
    "\n",
    "Note: In Step 3, you should update a mean only if there are pixels assigned to it. Otherwise, you will see a divide-by-zero error. For example, it's possible that during initialization, two of the means will be initialized to the same color (_i.e._ black). Depending on your implementation, all of the pixels in the photo that are closest to that color may get assigned to one of the means, leaving the other mean with no assigned pixels.\n",
    "\n",
    "When you have recalculated the image, you can display it. When you are finished, compare your image to the one in the solutions.\n",
    "\n",
    "```octave\n",
    "imshow(unit8(A16));\n",
    "```\n",
    "\n",
    "<img src=\"./image_files/clus02.bmp\", width = 250>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
