{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size = '6'><b>Reinforcement Learning</b></font>\n",
    "\n",
    "<br>\n",
    "- <a href=\"./files/mdps.pdf\" target=\"_blank\">MDP Slides</a> from [Prof. Zico Kolter](http://www.cs.cmu.edu/~15780/) at CMU\n",
    "\n",
    "- <a href=\"./files/mdp09.pdf\" target=\"_blank\">MDP Slides</a>, [DMP tutorial](http://www.autonlab.org/tutorials/mdp.html) from [Prof. Andrew W. Moore](http://www.cs.cmu.edu/~awm/tutorials.html) at CMU\n",
    "\n",
    "- <a href=\"./files/rl.pdf\" target=\"_blank\">RL Slides</a> from [Prof. Zico Kolter](http://www.cs.cmu.edu/~15780/) at CMU\n",
    "\n",
    "\n",
    "<table style=\"border-style: hidden; border-collapse: collapse;\" width = \"90%\"> \n",
    "    <tr style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "        <td width = 60% style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "             \n",
    "        </td>\n",
    "        <td width = 30%>\n",
    "        Collected by Prof. Seungchul Lee<br>\n",
    "        iSystems Design Lab<br>UNIST<br>http://isystems.unist.ac.kr/\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "Table of Contents\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Markov Decision Processes\n",
    "\n",
    "__Decision making under uncertainty__\n",
    "\n",
    "- Markov decision processes (MDP) and their extensions provide an extremely generaly way to think about how we can act optimally under uncertainty.\n",
    "\n",
    "- For many medium-sized problems, we can use the techniques from this lecture to compute an optimal decision policy.\n",
    "\n",
    "__Markov decision processes__\n",
    "\n",
    "- MDP is defined by: states, actions, transition probabilities, and rewards\n",
    "\n",
    "- States encode all information of a system needed to determine how it will evolve when taking acitions, with system governed by the state transition probabilities\n",
    "\n",
    "$$ P(s_{t+1}\\mid s_t, a_t)$$\n",
    "\n",
    "$\\quad \\;\\,$Note that transitions only depend on current state and action, not past states/actions (Markov assumption)\n",
    "\n",
    "- Goal for an agent is to take actions that maximize expected reward.\n",
    "\n",
    "__Graphical model representation of MDP__\n",
    "\n",
    "<img src=\"./image_files/mdp.png\" width = 350>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Formal MDP definition\n",
    "\n",
    "A Markov decision process is defined by:\n",
    "\n",
    "- A set of _states_ $S$ (assumed for now to be discrete)\n",
    "\n",
    "- A set of _actions_ $A$ (also assumed discrete)\n",
    "\n",
    "- _Transition probabilites_ $P$, which defined the probability distribution over next states given the current state and current action\n",
    "\n",
    "$$ P(S_{t+1} \\mid S_t, A_t) $$\n",
    "\n",
    "- Transitions only depend on the _current_ state and action (Markov assumption)\n",
    "\n",
    "- A _reward_ function $R:S \\rightarrow \\mathbb{R}$, mapping states to real numbers (can also define rewards over state/action pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Policies and value functions\n",
    "\n",
    "A _policy_ is a mapping from states to actions $\\pi: S \\rightarrow A$ (can also define stochastic policeis)\n",
    "\n",
    "A _value_ function for a policy, written $V^{\\pi}: S \\rightarrow \\mathbb{R}$ gives the exptected sum of discounted rewards when acting under that policy\n",
    "\n",
    "$$ V^{\\pi}(s) = E\\left[ \\sum_{t=0}^{\\infty} \\gamma^t R(s_t) \\mid s_0 = s, a_t = \\pi(s_t), s_{t+1} \\mid s_t, a_t \\sim P\\right] $$\n",
    "\n",
    "where $\\gamma < 1$ is a _discount factor_ (also formulations for finite horizion, infinite horizon average reward)\n",
    "\n",
    "Can also define value function recursively via the __Bellman equation__\n",
    "\n",
    "\n",
    "$$ V^{\\pi}(s) = R(s) + \\gamma \\sum_{s' \\in S}P\\left( P(s' \\mid s, \\pi(s)) \\right)V^{\\pi}(s') $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='4'><b>Computing the policy value</b></font>\n",
    "\n",
    "Let $\\upsilon^{\\pi} \\in \\mathbb{R}^{\\lvert S \\rvert}$ be a vector of values for each state, $r \\in \\mathbb{R}^{\\lvert S \\rvert}$ be a vector of rewards for each state\n",
    "\n",
    "Let $P^{\\pi} \\in \\mathbb{R}^{\\lvert S \\rvert \\times \\lvert S \\rvert}$ be a matrix containing probabilities for each transition under policy $\\pi$\n",
    "\n",
    "$$ P_{ij}^{\\pi} = P\\left( s_{t+1} = i \\mid s_t = j, a_t = \\pi(s_t)\\right)$$\n",
    "\n",
    "Then Bellman equation can be written in vector form as \n",
    "\n",
    "\\begin{align*}\n",
    "\\upsilon^{\\pi} &= r + \\gamma P^{\\pi} \\upsilon^{\\pi}\\\\\n",
    "(I - \\gamma P^{\\pi})\\upsilon^{\\pi} & = r \\\\\n",
    "\\upsilon^{\\pi} &= (I - \\gamma P^{\\pi})^{-1}r\\\\\n",
    "\\end{align*}\n",
    "\n",
    "_i.e._, computing value for a policy requires solving a linear sytsem, but expensive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='4'><b>Optimal policy and value function</b></font>\n",
    "\n",
    "The _optimal policy_ is the policy that achieves the highest value for every state\n",
    "\n",
    "$$ \\pi^* = \\arg\\max_{\\pi} V^{\\pi}(s)$$\n",
    "\n",
    "and its value function is written $V^* = V^{\\pi^*}$ (but there are an exponential number of policies, so this formulation is not very useful)\n",
    "\n",
    "Instead, we can directly define the optimal value function using the __Bellman optimality equation__\n",
    "\n",
    "$$ V^*(s) = R(s) + \\gamma \\max_{a \\in A} \\sum_{s' \\in S}P(s' \\mid s,a) V^*(s') $$\n",
    "\n",
    "and optimal policy is simply the action that attains this max\n",
    "\n",
    "$$\\pi^*(s) = \\arg\\max_{a} \\sum_{s' \\in S}P(s' \\mid s,a) V^*(s')$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Computing the optimal policy\n",
    "\n",
    "How do we compute the optimal policy? (or equivalently, the optimal value function?)\n",
    "\n",
    "<br>\n",
    "<font size='4'><b>1. Value iteration</b></font>\n",
    "\n",
    "Approach #1: __value iteration__ repeatedly update an estimate of the optimal value function according to Bellman optimality equation\n",
    "\n",
    "1. initialize an estimate for the value function arbitrarily\n",
    "<br>\n",
    "$$ \\hat{V}(s) \\leftarrow 0, \\quad \\forall s \\in S$$\n",
    "<br>\n",
    "2. repeat, update\n",
    "\n",
    "$$ \\hat{V}(s) \\leftarrow R(s) + \\gamma \\max_{a \\in A} \\sum_{s' \\in S}P(s' \\mid s,a) \\hat{V}(s'), \\quad \\forall s \\in S $$\n",
    "\n",
    "<br>\n",
    "<font size='4'><b>2. Policy iteration</b></font>\n",
    "\n",
    "Another approach to computing optimal policy\n",
    "\n",
    "Policy iteration algorithm\n",
    "\n",
    "1. initialize policy $\\hat{\\pi}$ (e.g., randomly)\n",
    "<br>\n",
    "2. Compute value of policy, $V^{\\pi}$ (e.g., via solving linear system, as discussed previously)\n",
    "<br>\n",
    "3. Update $\\pi$ to be greedy policy with respect to $V^{\\pi}$\n",
    "<br>\n",
    "$$\\pi(s) \\leftarrow \\arg\\max_{a} \\sum_{s' \\in S}P(s' \\mid s,a) V^{\\pi}(s')$$\n",
    "<br>\n",
    "4. If policy $\\pi$ changed in last iteration, return to step 2\n",
    "\n",
    "<br>\n",
    "<font size='4'><b>Policy iteration or value iteration?</b></font>\n",
    "\n",
    "- Policy iteration requires fewer iterations that value iteration, but each iteration requires solving a linear system instead of just applying Bellman opertor\n",
    "\n",
    "- In practice, policy iteration is often faster, especially if the transition probabilities are structured (e.g., sparse) to make solution of linear system efficient\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://www.youtube.com/embed/XLEpZzdLxI4\" \n",
       " width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<iframe src=\"https://www.youtube.com/embed/XLEpZzdLxI4\" \n",
    " width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://www.youtube.com/embed/4R6kDYDcq8U\" \n",
       "width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<iframe src=\"https://www.youtube.com/embed/4R6kDYDcq8U\" \n",
    "width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Reinforcement Learning\n",
    "\n",
    "__Agent interaction with environment__\n",
    "\n",
    "<img src=\"./image_files/rl_framework.png\" width = 400>\n",
    "\n",
    "__Markov decision processes__\n",
    "\n",
    "Recall a (discounted) Markov decision process is defined by:\n",
    "\n",
    "$$M = (S,A,P,R)$$\n",
    "\n",
    "- $S$: set of states\n",
    "\n",
    "- $A$: set of actions\n",
    "\n",
    "- $p$: $S \\times S \\rightarrow [0,1]$: transition probability distribution $P(s' \\mid s)$\n",
    "\n",
    "- $R: S \\rightarrow \\mathbb{R}$: reward function, where $R(S)$ is reward for state $s$\n",
    "\n",
    "The RL twist: we do not know $P$ or $R$, or they are too big to enumerate (only have the ability to act in MDP, observe states and rewards)\n",
    "\n",
    "Policy $\\pi: S \\rightarrow A$ is a mapping from states to actions\n",
    "\n",
    "<br>\n",
    "We can determine the value of a policy by solving a linear systems, or via the iteration (similar to a value iteration, but for a fixed policy)\n",
    "\n",
    "$$ \\hat{V}^{\\pi}(s) \\leftarrow R(s) + \\gamma \\sum_{s' \\in S}P(s' \\mid s,\\pi(s)) \\hat{V}^{\\pi}(s'), \\quad \\forall s \\in S $$\n",
    "\n",
    "We can determine the value of _optimal_ policy $V^*$ using a value iteration:\n",
    "\n",
    "$$ \\hat{V}(s) \\leftarrow R(s) + \\gamma \\max_{a \\in A} \\sum_{s' \\in S}P(s' \\mid s,a) \\hat{V}(s'), \\quad \\forall s \\in S $$\n",
    "\n",
    "<br>\n",
    "<font color='red'><b>How can we compute these quantities when $P$ and $R$ are unknown?</b></font>\n",
    "\n",
    "- model-based RL\n",
    "\n",
    "- model-free RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.1. Model-based RL\n",
    "\n",
    "A simple approach: just estimate the MDP from data\n",
    "\n",
    "Agent acts in the work (according to some policy), observes experience\n",
    "\n",
    "$$s_1, r_1, a_1, s_2,r_2,a_2, \\cdots, s_m, r_m, a_m$$\n",
    "\n",
    "We form the empirical estimate of the MDP via the counts\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{P}(s' \\mid s,a) &= \\frac{\\sum_{i=1}^{m-1} \\mathbf{1}\\{s_i = s, a_i = a, s_{i+1} = s'\\} }{\\sum_{i=1}^{m-1} \\mathbf{1}\\{s_i = s, a_i = a\\}}\\\\ \\\\\n",
    "\\hat{R}(s) &= \\frac{ \\sum_{i=1}^{m} \\mathbf{1}\\{s_i = s \\} r_i} {\\sum_{i=1}^{m} \\mathbf{1}\\{s_i = s\\}}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Now solve the MDP $(S,A,\\hat{P}, \\hat{R})$\n",
    "\n",
    "- Will converge to correct MDP (and hence correct value function/policy) given enough samples of each state\n",
    "\n",
    "- How can we ensure we get the \"right\" samples? (a challenging problem for all methods we present here, stay tuned)\n",
    "\n",
    "\n",
    "- Advantages (informally): makes \"efficient\" use of data\n",
    "\n",
    "- Disadvantages: requires we build the actual MDP models, not much help if state space is too large\n",
    "\n",
    "## 2.2. Model-free RL\n",
    "\n",
    "Temporal difference methods (TD, SARSA, Q-learning): directly learn value function $V^{\\pi}$ or $V^*$ (or a slight generalization of value function, that we will see shortly)\n",
    "\n",
    "Direct policy search: directly learn optimal policy $\\pi^*$ (covered in a later lecture)\n",
    "\n",
    "### 2.2.1. Temporal difference (TD) methods\n",
    "\n",
    "Let's consider computing the value function for a fixed policy via the iteration\n",
    "\n",
    "$$ \\hat{V}^{\\pi}(s) \\leftarrow R(s) + \\gamma \\sum_{s' \\in S}P(s' \\mid s,\\pi(s)) \\, \\hat{V}^{\\pi}(s'), \\quad \\forall s \\in S $$\n",
    "\n",
    "Suppose we are in some state $s_t$, receive reward $r_i$, take action $a_t = \\pi(s_t)$ and end up in state $s_{t+1}$\n",
    "\n",
    "We cannot update $\\hat{V}^{\\pi}$ for all $s$, but can we update for $s_t$?\n",
    "\n",
    "$$ \\hat{V}^{\\pi}(s_t) \\leftarrow r_t + \\gamma \\sum_{s' \\in S}P(s' \\mid s,a_t) \\hat{V}^{\\pi}(s') $$\n",
    "\n",
    "...No, because we still cannot compute this sum\n",
    "\n",
    "But, $s_{t+1}$ is a sample from the distribution $P(s' \\mid s_t,a_t)$, so we could perform the update\n",
    "\n",
    "$$ \\hat{V}^{\\pi}(s_t) \\leftarrow r_t + \\gamma \\hat{V}^{\\pi}(s_{t+1}) $$\n",
    "\n",
    "- It is too \"harsh\" assignment if we assume that $s_{t+1}$ is the only possible next state; \n",
    "\n",
    "- instead \"smooth\" the update using some $\\alpha < 1$\n",
    "\n",
    "$$ \\hat{V}^{\\pi}(s_t) \\leftarrow (1-\\alpha) \\left( \\hat{V}^{\\pi}(s_{t}) \\right) + \\alpha \\left( r_t + \\gamma \\hat{V}^{\\pi}(s_{t+1}) \\right)$$\n",
    "\n",
    "This is the _<font color='red'><b>temporal difference (TD)</b></font>_ algorithm.\n",
    "\n",
    "TD lets us learn the value function of a policy $\\pi$ directly, without ever constructing the MDP.\n",
    "\n",
    "But is this really that helpful?\n",
    "\n",
    "- Consider trying to execute greedy policy w.r.t. estimated $\\hat{V}^{\\pi}$\n",
    "\n",
    "- We need a model anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.2.2. SARSA and Q-learning\n",
    "\n",
    "$Q$ function (for MDPs in general) are like value functions but defined over __state-action pairs__\n",
    "\n",
    "\\begin{align*}\n",
    "Q^{\\pi}(s,a) &= R(s) + \\sum_{s' \\in S} P(s' \\mid s,a) Q^{\\pi}(s',\\pi(s')) \\\\\n",
    "Q^*(s,a) &= R(s) + \\sum_{s' \\in S} P(s' \\mid s,a) \\max_{a'}Q^*(s',a') \\\\\n",
    "&= R(s) + \\sum_{s' \\in S} P(s' \\mid s,a) V^*(s') \n",
    "\\end{align*}\n",
    "\n",
    "_i.e._, $Q$ function is a value of starting state $s$, taking action $a$, and then acting according to $\\pi$ (or optimally for $Q^*$)\n",
    "\n",
    "<br>\n",
    "We can easily construct analogues of value iteration or policy evaluation to construct $Q$ functions directly given an MDP.\n",
    "\n",
    "$Q$ function leads to new TD-like methods.\n",
    "\n",
    "As with TD, observe state $s$, reward $r$, take action $a$ (but not necessarily $a = \\pi(s)$), observe next sate $s'$\n",
    "\n",
    "- SARSA: estimate $Q^{\\pi}(s,a)$\n",
    "\n",
    "$$ \\hat{Q}^{\\pi}(s,a) \\leftarrow (1-\\alpha) \\left( \\hat{Q}^{\\pi}(s,a) \\right) + \\alpha \\left( r_t + \\gamma \\hat{Q}^{\\pi}(s',\\pi(s') \\right)$$\n",
    "\n",
    "- $Q$-learning: estimate $Q^*(s,a)$\n",
    "\n",
    "$$ \\hat{Q}^*(s,a) \\leftarrow (1-\\alpha) \\left( \\hat{Q}^*(s,a) \\right) + \\alpha \\left( r_t + \\gamma \\max_{a'}\\hat{Q}^*(s',a') \\right)$$\n",
    "\n",
    "Again, these algorithms converge to true $Q^{\\pi}, Q^*$ if all state-action pairs are seen frequently enough\n",
    "\n",
    "<br>\n",
    "The advantage of this approach is that we can now select actions without a model of MDP\n",
    "\n",
    "- SARSA, greedy policy w.r.t. $Q^{\\pi}(s,a)$\n",
    "\n",
    "$$ \\pi'(s) = \\max_{a} \\hat{Q}^{\\pi}(s,a)$$\n",
    "\n",
    "- $Q$-learning, optimal policy\n",
    "\n",
    "$$\\pi^*(s) = \\max_{a} \\hat{Q}^*(s,a)$$\n",
    "\n",
    "So with $Q$-learning, for instance, we can learn optimal policy without model MDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://www.youtube.com/embed/un-FhSC0HfY\" \n",
       "width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<iframe src=\"https://www.youtube.com/embed/un-FhSC0HfY\" \n",
    "width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='5'><b>David Silver's Lecture</b></font>\n",
    "\n",
    "- UCL homepage for slides (http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html)\n",
    "\n",
    "- DeepMind for RL videos (https://www.youtube.com/watch?v=2pWv7GOvuf0)\n",
    "\n",
    "- An Introduction to Reinforcement Learning, Sutton and Barto <a href=\"./files/SuttonBook.pdf\" target=\"_blank\">pdf</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
