{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size = '6'><b>Recurrent Neural Networks</b></font>\n",
    "\n",
    "- <a href=\"./files/771A_lec24_slides.pdf\" target=\"_blank\">Slides</a> by Piyush Rai\n",
    "\n",
    "<table style=\"border-style: hidden; border-collapse: collapse;\" width = \"90%\"> \n",
    "    <tr style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "        <td width = 60% style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "             \n",
    "        </td>\n",
    "        <td width = 30%>\n",
    "        Collected by Prof. Seungchul Lee<br>\n",
    "        iSystems<br>http://isystems.unist.ac.kr/<br>\n",
    "        UNIST\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Table of Contents\n",
    "<div id=\"toc\"></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Limitation of Feedforward Neural Nets\n",
    "\n",
    "- FFNN cannot take into account the sequential structure in the data\n",
    "\n",
    "- For a sequence of observation $x_1, \\cdots, x_T$, their corresponding hidden units (states) $h_1, \\cdots, h_T$ are assumed independent of each other\n",
    "\n",
    "<br>\n",
    "<img src=\"./image_files/sequence_layer.jpg\" width = 400>\n",
    "<br>\n",
    "\n",
    "- Not ideal for sequential data, e.g., sentence/paragraph/document (sequence of words), video (sequence of frames), etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Recurrent Neural Nets (RNN)\n",
    "\n",
    "- Hidden state at each step depends on the hidden state of the previous\n",
    "\n",
    "<br>\n",
    "<img src=\"./image_files/sequence_layer_dependv2.jpg\" width = 360>\n",
    "<br>\n",
    "\n",
    "- Each hidden state is typically defined as\n",
    "\n",
    "$$ h_t = f \\left( W x_t + U h_{t-1}\\right) $$\n",
    "\n",
    "$\\quad \\;$where $U$ is like a transition matrix and $f$ is some nonlinear function (e.g., $\\tanh$).\n",
    "\n",
    "- Now $h_t$ acts as a memory which helps us remember what happened up to step $t$\n",
    "\n",
    "- Note: Unlike sequence data models such as HMM where each state is discrete, RNN states are continuous-valued. In that sense, RNNs are similar to linear Guassian models like Kalman Filters which have continuous states.\n",
    "\n",
    "- RNNs can also be extended to have more than one hidden layer.\n",
    "\n",
    "\n",
    "- A more \"micro\" view of RNN (transition matrix $U$ connects the hidden states across observations, propagating information along the sequence)\n",
    "\n",
    "<table style=\"border-style: hidden; border-collapse: collapse;\" width = \"96%\"> \n",
    "    <tr style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "        <td width = 48% style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "<img src=\"./image_files/micro_view_rnn.jpg\" width = 400>\n",
    "        </td>\n",
    "        <td width = 48%>\n",
    "<img src=\"./image_files/recurrence_gif.gif\" width = 400>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "<br>\n",
    "\n",
    "- RNN: Applications\n",
    "<br>\n",
    "<img src=\"./image_files/rnn_application.jpg\" width = 700>\n",
    "<br>\n",
    "    - Input, output, or both, can be sequences (possibly of different lengths)\n",
    "    - Different inputs (and different outputs) need not be of  the same length\n",
    "    - Regardless of the length of the input sequence, RNN will learn a fixed size embedding for the input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Training RNN\n",
    "\n",
    "- Trained using Backpropagation Through Time \n",
    "    - Forward  propagate from step 1 to end, and then backward propagate from end to 1\n",
    "   \n",
    "- Think of the time-dimension as another hidden layer and then it is just like standard backpropagation for feedforward neural nets.\n",
    "\n",
    "<img src=\"./image_files/backprop_through_time.gif\" width = 550>\n",
    "\n",
    "- Black: Prediction, Yellow: Error, Orange: Gradients\n",
    "\n",
    "## 1.2. RNN: Vanishing/Exploding Gradients Problem\n",
    "<br>\n",
    "<img src=\"./image_files/vanishing_gradient.jpg\" width = 550>\n",
    "<br>\n",
    "\n",
    "- Sensitivity of hidden states and outputs on a given input becomes weaker as we move away from it along the sequence (weak memory)\n",
    "\n",
    "- New inputs \"overwrite\" the activations of previous hidden states\n",
    "\n",
    "- Repeated multiplications can cause the gradients to vanish or explode\n",
    "\n",
    "## 1.3. Capturing Long-Range Dependencies\n",
    "\n",
    "- Idea: Augment the hidden states with gates (with parameters to be learned)\n",
    "\n",
    "- These gates can help us remember and forget information \"selectively\"\n",
    "\n",
    "<img src=\"./image_files/gates_lstm.png\" width = 550>\n",
    "\n",
    "- The hidden states have 3 types of gates\n",
    "    - input (bottom), forget (left), output (top)\n",
    "\n",
    "- Open gate denoted by 'o', closed gate denoted by '-'\n",
    "\n",
    "- LSTM: Long Short-Term Memory is one such idea\n",
    "\n",
    "## 1.4. LSTM\n",
    "\n",
    "- Essentially an RNN, except that the hidden states are computed differently\n",
    "\n",
    "- Recall that RNN computs the hidden states as $h_t = f \\left( W x_t + U h_{t-1}\\right)$\n",
    "\n",
    "- For RNN: State update is multiplicative (weak memory and gradient issues)\n",
    "\n",
    "- In contrast, LSTM maintains a \"context\" $C_t$ and computes hidden states\n",
    "\n",
    "<img src=\"./image_files/LSTM.png\" width = 450>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Neural Nets for Unsupervised Learning\n",
    "\n",
    "## 2.1. Autoencoder\n",
    "\n",
    "- A neural net for unsupervised feature extraction\n",
    "\n",
    "- Basic principle: Learns an encoding of the inputs so as to recover the original input from the encodings as well as possible\n",
    "\n",
    "<table style=\"border-style: hidden; border-collapse: collapse;\" width = \"90%\"> \n",
    "    <tr style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "        <td width = 50% style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "<img src=\"./image_files/autoencoder1_V2.png\" width = 500>\n",
    "        </td>\n",
    "        <td width = 40%>\n",
    "<img src=\"./image_files/sDA.png\" width = 270>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "- Also used to initialize deep learning models (layer-by-layer pre-training)\n",
    "\n",
    "## 2.2. Autoencoder: an Example\n",
    "\n",
    "- Real-value inputs, binary-valued encodings\n",
    "\n",
    "<br>\n",
    "<img src=\"./image_files/autoencoder2_V2.png\" width = 500>\n",
    "<br>\n",
    "\n",
    "- Sigmoid encoder (parameter matrix $W$), linear decoder (parameter matrix $D$), learned via:\n",
    "\n",
    "$$\\arg\\min_{D,W} E(D,W) = \\sum_{n=1}^{N} \\rVert Dz_n - x_n\\lVert^2 = \\sum_{n=1}^{N} \\rVert D \\,\\sigma(Wx_n) - x_n\\lVert^2$$\n",
    "\n",
    "- If encoder is also linear, then autoencoder is equivalent to PCA\n",
    "\n",
    "## 2.3. Denoising: Autoencoders\n",
    "\n",
    "- Idea: introduce stochastic corruption to the input: \n",
    "    - Hide some features\n",
    "    - Add Gaussian noise\n",
    "\n",
    "<br>\n",
    "<img src=\"./image_files/autoencoder3.png\" width = 500>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
