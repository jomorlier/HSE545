{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<font size = '6'><b>Learning from Imbalanced Data</b></font>\n",
    "\n",
    "- <a href=\"./reference_files/771A_lec25_slides.pdf\" target=\"_blank\">Slides</a> by Piyush Pai\n",
    "\n",
    "<table style=\"border-style: hidden; border-collapse: collapse;\" width = \"90%\"> \n",
    "    <tr style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "        <td width = 60% style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "             \n",
    "        </td>\n",
    "        <td width = 30%>\n",
    "        Collected by Prof. Seungchul Lee<br>\n",
    "        iSystems<br>http://isystems.unist.ac.kr/<br>\n",
    "        UNIST\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Table of Contents\n",
    "<div id=\"toc\"></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imbalanced Data\n",
    "- Consider binary classification\n",
    "\n",
    "- Often the classes are highly imbalanced\n",
    "\n",
    "<img src='./image_files/imbalanced.png' width = 150>\n",
    "\n",
    "- Should I feel happy if the classifier gets 99.997% classification accuracy on test data?\n",
    "\n",
    "## 1.1. True Definition of Imbalance Data?\n",
    "\n",
    "- Debatable ...\n",
    "\n",
    "- Scenario 1: 100,000 negative and 1,000 positive examples\n",
    "\n",
    "- Scenario 2: 10,000 negative and 10 positive examples\n",
    "\n",
    "- Scenario 3: 1,000 negative and 1 positive examples\n",
    "\n",
    "- Usually, imbalance is characterized by absolute rather than relative rarity \n",
    "    - Finding needles in a haystack...\n",
    "\n",
    "## 1.2. Minimizing Loss\n",
    "\n",
    "- Any model to minimize the loss, e.g.,\n",
    "\n",
    "$$\n",
    "\\text{Classification}: \\hat\\omega = \\arg \\min_{\\omega} \\sum_{n=1}^{N} \\ell \\left(y_n,\\omega^T x_n \\right)\n",
    "$$\n",
    "\n",
    "$\\quad \\;$ ... will usually get a high accuracy\n",
    "\n",
    "- However, it will be highly biased towards predicting the majority class\n",
    "    - Thus accuary alone cannot be trusted as the evaluation measure if we care more about predicting minority class (say positive) correctly\n",
    "    \n",
    "## 1.3. Better Evaluation Measures\n",
    "\n",
    "- Precision: What fraction of positive predictions is truely positive\n",
    "\n",
    "$$ P = \\frac{\\# \\text{ example correctly predicted as positive}}{\\# \\text{ examples predicted as positive}}$$\n",
    "\n",
    "- Recall: What fraction of total positives are predicted as positives\n",
    "\n",
    "$$ R = \\frac{\\# \\text{ example correctly predicted as positive}}{\\# \\text{ total positive examples in the test set}}$$\n",
    "\n",
    "<br>\n",
    "<img src='./image_files/PandR.png' width = 400>\n",
    "<br>\n",
    "\n",
    "- Often there is a trade-off between precision and recall. Also there can be combined to yield other measures such as F1 score, AUC score, etc.\n",
    "\n",
    "## 1.4. Dealing with Class Imbalance\n",
    "\n",
    "- Modifying the training data (the class distiribution)\n",
    "    - Undersampling the majority class\n",
    "    - Oversampling the minority class\n",
    "    - Reweighting the examples\n",
    "\n",
    "\n",
    "- Modifying the learning model\n",
    "    - Use loss functions customized to handle class imbalance\n",
    "\n",
    "\n",
    "- Reweighting can be also seen as a way to modify the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Modifying the Training Data\n",
    "\n",
    "## 2.1. Undersampling\n",
    "Create a new training data set by:\n",
    "- including all $k$ \"positive\" examples\n",
    "- randomly picking $k$ \"negative\" examples\n",
    "\n",
    "<img src='./image_files/undersampling.png' width = 400>\n",
    "\n",
    "Throws away a lot of data/information, but efficient to train\n",
    "\n",
    "## 2.2. Oversampling\n",
    "Create a new training data set by:\n",
    "- including all $m$ \"negative\" examples\n",
    "- includ $m$ \"positive\" examples:\n",
    "    - repeat each example a fixed number of times, or\n",
    "    - sample with replacement\n",
    "\n",
    "<img src='./image_files/oversampling.png' width = 400>\n",
    "\n",
    "- From the loss function's perspective, the repeated examples simply constribute multiple times to the loss function\n",
    "\n",
    "- Oversampling ususally tends to outperform undersampling because we are using more data to train the model\n",
    "\n",
    "- Some oversampling methods (SMOTE) are based on creating synthetic examples from the minority class\n",
    "\n",
    "## 2.3. Reweighting Examples\n",
    "Add costs/weights to the training set\n",
    "\n",
    "- \"negative\" examples get weight 1\n",
    "\n",
    "- \"positive\" examples get a much larger weight\n",
    "\n",
    "Change learning algorithm to optimize weighted training error\n",
    "\n",
    "<img src='./image_files/reweighting.png' width = 300>\n",
    "\n",
    "- Similar effect as oversampling but is more efficient (because there is no multiplicity of examples)\n",
    "\n",
    "- Also requires a classfier that can learn with weighted examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Modifying the Loss Function\n",
    "\n",
    "## 3.1. Loss Functions Customized for Imbalanced Data\n",
    "\n",
    "- Traditional loss functions have the form $\\sum_{n=1}^{N} \\ell \\left( y_n, f(x_n)\\right)$\n",
    "\n",
    "- Such loss functions look at positive and negative examples individually, so the majority class tends to overwhelm the minority class\n",
    "\n",
    "- Reweighting the loss function differently for different classes can be one way to handle class imbalance, e.g., $\\sum_{n=1}^{N} C_{y_n} \\ell \\left( y_n, f(x_n)\\right)$\n",
    "\n",
    "- Alternatively, we can loss functions that look at pairs of examples (a positive example $x_n^+$ and a negative example $x_m^-$). For example:\n",
    "\n",
    "$$ \\ell \\left( f(x_n^+), f(x_m^-)\\right) = \n",
    "\\begin{cases}\n",
    "0, & \\text{if }\\; f(x_n^+) > f(x_m^{-})\\\\\n",
    "1, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- These are called \"pairwise\" loss functions\n",
    "\n",
    "- Why is it a good loss function for imbalanced data?\n",
    "\n",
    "## 3.2. Pairwise Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.3.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
