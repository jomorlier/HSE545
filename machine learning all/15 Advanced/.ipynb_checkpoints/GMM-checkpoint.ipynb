{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<br><br>\n",
    "<font size = '6'><b>Gaussian Mixture Model</b></font>\n",
    "\n",
    "- <a href=\"./reference_files/13.mixture-models.pdf\" target=\"_blank\">Slides</a> by David Rosenberg\n",
    "\n",
    "<table style=\"border-style: hidden; border-collapse: collapse;\" width = \"90%\"> \n",
    "    <tr style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "        <td width = 60% style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "             \n",
    "        </td>\n",
    "        <td width = 30%>\n",
    "        Collected by Prof. Seungchul Lee<br>\n",
    "        iSystems<br>http://isystems.unist.ac.kr/<br>\n",
    "        UNIST\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Table of Contents\n",
    "<div id=\"toc\"></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Probabilistic Model for Clustering\n",
    "\n",
    "Let's consider a generative model for the data\n",
    "\n",
    "Suppose\n",
    "- there are $k$ clusters\n",
    "- we have a probability density for each cluster\n",
    "\n",
    "Generate a point as follows\n",
    "\n",
    "$\\;\\;$1) Choose a random cluster $z \\in \\{1,2,\\cdots,k\\}$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&Z \\sim \\left( \\pi_1, \\cdots, \\pi_k\\right) \\\\\n",
    "&p(z_i = 1) = \\pi_i \\;, \\quad\n",
    "\\sum_{i=1}^{k} \\pi_i =1\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "$\\;\\;$2) Choose a point from the distribution for cluster $Z$\n",
    "\n",
    "$$\\left(X \\mid Z = z \\right) \\sim p(x \\mid z)$$\n",
    "\n",
    "# Gaussian Mixture Model\n",
    "\n",
    "For example, $k = 3$\n",
    "\n",
    "$\\;\\;$1) Select $Z \\in \\{1,2,3\\} \\sim \\left(\\frac{1}{3},\\frac{1}{3},\\frac{1}{3} \\right)$\n",
    "\n",
    "$\\;\\;$2) Sample from $\\left(X \\mid Z = z \\right) \\sim N(x \\mid \\mu_k, \\Sigma_k)$\n",
    "\n",
    "\n",
    "<img src = \"./image_files/GMM.png\" width=500>\n",
    "\n",
    "Example: generating data from two Gaussians\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mu_1 &= \\begin{bmatrix} 3\\\\3 \\end{bmatrix}, & \\Sigma_1 &= \\begin{bmatrix}1 & 0 \\\\ 0 & 2 \\end{bmatrix} \\\\\n",
    "\\mu_2 &= \\begin{bmatrix} 1\\\\-3 \\end{bmatrix}, & \\Sigma_2 &= \\begin{bmatrix}2 & 0 \\\\ 0 & 1 \\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\\begin{bmatrix} \\pi_1 &\\pi_2 \\end{bmatrix} = \\begin{bmatrix} 0.7 & 0.3\\end{bmatrix}$$\n",
    "\n",
    "<img src = \"./image_files/twoGMM.jpg\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Latent Variable Model\n",
    "\n",
    "- Back in reality, we observe $X$, not $(X,Z)$\n",
    "\n",
    "- Cluster assignemnt $Z$ is called a hidden variable\n",
    "\n",
    "- A latent variable model is a probability model for which certain variables are never observed.\n",
    "\n",
    "Model-based clustering\n",
    "\n",
    "- we observe $X = x$\n",
    "\n",
    "- The conditional distribution is a soft assignment to clusters\n",
    "\n",
    "$$p(z \\mid X = x) = \\frac{p(x,z)}{p(x)}$$\n",
    "\n",
    "- A hard assignment is \n",
    "\n",
    "$$z^* = \\arg \\min_{z \\in (1,\\cdots,k)} \\mathbb{P}(Z = z \\mid X = x)$$\n",
    "\n",
    "Estimating/Learning the Gaussian Mixture Model (GMM)\n",
    "\n",
    "- What does it mean to \"have\" or \"know\" the GMM? It means we know the following parameters:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Cluster probabilities} && \\pi &= (\\pi_1,\\cdots,\\pi_k) \\\\\n",
    "\\text{Cluster means} && \\mu &= (\\mu_1,\\cdots,\\mu_k)\\\\\n",
    "\\text{Cluster covariance matrices} && \\Sigma &= (\\Sigma_1,\\cdots,\\Sigma_k)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- We have a probability model: let's find the MLE\n",
    "    - Suppose we have data $D = \\{x_1,\\cdots,x_m\\}$\n",
    "    - We need the model likelihood for $D$\n",
    "    \n",
    "- Since we only observe $X$, we need the marginal distribution:\n",
    "\n",
    "$$p(x) = \\sum_{z=1}^{k}p(x,z)=\\sum_{z=1}^{k}\\pi_z \\,N(x \\mid \\mu_z, \\Sigma_z)$$\n",
    "\n",
    "- The model likelihood for $D = \\{x_1,\\cdots,x_m\\}$ is \n",
    "\n",
    "$$L(\\pi,\\mu,\\Sigma) = \\prod_{i=1}^{m} p(x_i) = \\prod_{i=1}^{m} \\sum_{z=1}^{k}\\pi_z \\,N(x \\mid \\mu_z, \\Sigma_z)$$\n",
    "\n",
    "- As usual, we will take our objective function to be the log of this:\n",
    "\n",
    "$$J(\\pi,\\mu,\\Sigma) = \\sum_{i=1}^{m} \\log \\left\\{ \\sum_{z=1}^{k}\\pi_z \\,N(x \\mid \\mu_z, \\Sigma_z) \\right\\}$$\n",
    "\n",
    "- The log-likelihood for a single Gaussian:\n",
    "\n",
    "$$\\sum_{i=1}^{m} \\log  \\,N(x \\mid \\mu, \\Sigma) = -\\frac{md}{2} \\log(2\\pi) - \\frac{m}{2} \\log \\lvert \\Sigma\\rvert -\\frac{1}{2} \\sum_{i=1}^{m} (x_i-\\mu)^{T}\\Sigma^{-1}(x_i-\\mu)$$\n",
    "\n",
    "- For a single Gaussian, the log cancels the $\\exp$ in the Gaussian density $\\implies$ things simplify a lot.\n",
    "\n",
    "- For the GMM, the sum inside the log prevents this cancellation $\\implies$ No closed form expression for MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## MLE for Gaussian Model\n",
    "\n",
    "- Let's start by considering MLE for the _single_ Gaussian model\n",
    "\n",
    "- For data $D = \\{x_1,\\cdots,x_m\\}$, the log likelihood is given by\n",
    "\n",
    "$$\\sum_{i=1}^{m} \\log  \\,N(x \\mid \\mu, \\Sigma) = -\\frac{md}{2} \\log(2\\pi) - \\frac{m}{2} \\log \\lvert \\Sigma\\rvert -\\frac{1}{2} \\sum_{i=1}^{m} (x_i-\\mu)^{T}\\Sigma^{-1}(x_i-\\mu)$$\n",
    "\n",
    "- With some calculus, we find that the MLE parameters are\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mu_{\\text{MLE}} &= \\frac{1}{m} \\sum_{i=1}^{m} x_i\\\\ \n",
    "\\Sigma_{\\text{MLE}} &= \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu_{\\text{MLE}})(x_i - \\mu_{\\text{MLE}})^T\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- For GMM, if we know the cluster assignment $z_i$ for each $x_i$, we could compute the MLEs for each cluster\n",
    "\n",
    "## Cluster Responsibilities \n",
    "\n",
    "- Denote the probability that observed value $x_i$ comes from cluster $j$ by\n",
    "\n",
    "$$\\gamma_{i}^{j} = \\mathbb{P}(Z = j \\mid X = x_i)$$\n",
    "\n",
    "- The responsibility that cluster $j$ takes for obervation $x_i$\n",
    "\n",
    "- Computationally\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\gamma_{i}^{j} &= \\mathbb{P}(Z = j \\mid X = x_i)\\\\ \\\\\n",
    "& = \\frac{p(Z=j, X=x_i)}{p(x)}\\\\ \\\\\n",
    "& = \\frac{\\pi_j N(x_i \\mid \\mu_j, \\Sigma_j)}{\\sum_{c=1}^{k} \\pi_c N(x_i \\mid \\mu_c, \\Sigma_c)}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- The vector $\\left(\\gamma_{i}^{1},\\cdots,\\gamma_{i}^{k}\\right)$ is exactly the soft assignment for $x_i$\n",
    "\n",
    "## EM Algorithm for GMM\n",
    "\n",
    "1) Initialize parameters $\\mu, \\Sigma, \\pi$\n",
    "\n",
    "2) \"E step\": Evaluate the responsibilities using current parameters\n",
    "\n",
    "$$\\gamma_{i}^{j} = \\frac{\\pi_j N(x_i \\mid \\mu_j, \\Sigma_j)}{\\sum_{c=1}^{k} \\pi_c N(x_i \\mid \\mu_c, \\Sigma_c)}$$\n",
    "\n",
    "$\\;\\;\\,$for $i=1,\\cdots,m$ and $j=1,\\cdots,k$\n",
    "\n",
    "3) \"M step\": Re-estimate the parameters using responsibilites:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mu_{\\text{c}}^{\\text{new}} &= \\frac{1}{m_c} \\sum_{i=1}^{m} \\gamma_{i}^{c}x_i\\\\ \n",
    "\\Sigma_{\\text{c}}^{\\text{new}} &= \\frac{1}{m_c} \\sum_{i=1}^{m} \\gamma_{i}^{c}\\,(x_i - \\mu_{\\text{MLE}})(x_i - \\mu_{\\text{MLE}})^T\\\\\n",
    "\\pi_{c}^{\\text{new}} &= \\frac{m_c}{m} = \\frac{\\sum_{i=1}^{m}\\gamma_{i}^{c}}{m}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "4) Repeat from Step 2, until log-likelihood converges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<center><iframe src=\"https://www.youtube.com/embed/PejHsxneli8\"\n",
       "width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen></iframe></center>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<center><iframe src=\"https://www.youtube.com/embed/PejHsxneli8\"\n",
    "width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen></iframe></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
