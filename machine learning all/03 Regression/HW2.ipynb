{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<center><font size='5'><b>Machine Learning for Mechanical Engineering</b></font><br><br><font size='5'><b>Homework 2</b></font><br><br><font size='3'><b>Due Wednesday, 03/28/2018, 9:30 AM</b></font></center>\n",
    "\n",
    "<table style=\"border-style: hidden; border-collapse: collapse;\" width = \"95%\"> \n",
    "    <tr style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "        <td width = 75% style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "\n",
    "        </td>\n",
    "        <td width = 20%>\n",
    "        by Seungchul Lee<br>industrial AI Lab<br>POSTECH\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(a)__ \n",
    "Find the solution using the derivatives of the following objective function:\n",
    "\n",
    "$$\\min_{x} \\, (2x_1 - 1)^2 + (-x_1 + x_2)^2 + (2x_2 +1)^2$$\n",
    "\n",
    "__(b)__\n",
    "Formulate the corresponding least-squares problem (_i.e._, find $A$ and $b$), and solve it using cvxpy.\n",
    " \n",
    "$$\\min_{x} \\, \\lVert Ax - b\\rVert_2$$\n",
    "\n",
    "__(c)__\n",
    "How many suboptimal points exist? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Suppose you receive the binary signal. The signal is corrupted with noises while transmitting through channels. We want to estimate original signal through the $L_1$ optimization. The mathematical problem statement is given: \n",
    "\n",
    "$$\n",
    "\\begin{array}{Icr}\\begin{align*}\n",
    "y = x + \\omega\\\\\n",
    "x \\in \\{ 0, 1 \\}\\\\\n",
    "\\omega \\text{ is noise}\n",
    "\\end{align*}\\end{array}\n",
    "\\quad \\Longrightarrow \n",
    "\\begin{array}{I} \\quad \n",
    "\\text{Recover the original signal } x \\text{ from the corrupted signal } y\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Note: this problem will be revisited after a midterm. We will solve this problem using the probability theory later.\n",
    "\n",
    "### Step 1. Data generation\n",
    "\n",
    "First, let’s generate the original signal as shown in the below figure. This can be simply done using `ones` and `zeros` commands in Python. \n",
    "\n",
    "<img src=\"./image_files/problem06.png\", width = 400>\n",
    "\n",
    "Next, corrupt the original signal with Gaussian noise. This can be done using `randn` command in Python. One of realizations of the corrupted signal will be given: \n",
    "\n",
    "<img src=\"./image_files/problem07.png\", width = 400>\n",
    "\n",
    "### Step 2. $L_1$ optimization\n",
    "\n",
    "Note that the signal is sparse (in a sense of frequency domain). Therefore we can apply the $L_1$ optimization. We will optimize the $L_2$ cost function with the $L_1$ constraints. Here’s a rough CVX code : \n",
    "\n",
    "```python\n",
    "import cvxpy as cvx\n",
    "\n",
    "x_con = cvx.Variable(200)\n",
    "objective = cvx.Minimize(cvx.norm(x_con[1:200] - x_con[0:199],1))\n",
    "constraints = [cvx.norm(x_con - x_corrupt, 2) <= beta]\n",
    "prob = cvx.Problem(objective, constraints)\n",
    "result = prob.solve()\n",
    "```\n",
    "\n",
    "Plot the reconstructed signal. In addition, by changing the beta, explain how beta affects in an optimization process. One of reconstructed signal with a proper value of beta is shown below:\n",
    "\n",
    "<img src=\"./image_files/problem08.png\", width = 400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw many optimization problems in a machine learning context and witnessed that $L_1$ norm reguarlization provides not only small, but also sparse decision variables. In this problem we will use a linear programming to solve $L_1$ norm optimization problems instead of simply using cvx.\n",
    "\n",
    "__(a)__ \n",
    "Show that \n",
    "\n",
    "$$ \\lvert y_i \\rvert = \\max \\left(-y_i,y_i \\right) $$\n",
    "\n",
    "__(b)__ \n",
    "Show that, by introducing slack variable $t_i$, the absolute value optimization problem can be coverted to\n",
    "\n",
    "<table style=\"border-style: hidden; border-collapse: collapse;\" width = \"60%\"> \n",
    "    <tr style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "        <td width = 30% style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "        $$ \\min_{y_i} \\,\\lvert y_i \\rvert = \\min_{y_i} \\left\\{\\max \\left(-y_i,y_i \\right) \\right\\} $$ \n",
    "        \n",
    "        </td>\n",
    "        <td width = 5% style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "             $$ \\iff $$\n",
    "             \n",
    "        </td>\n",
    "        <td width = 20%>\n",
    "        $$\n",
    "        \\begin{align*}\n",
    "        \\min_{t_i} \\quad & t_i  \\\\\n",
    "        \\text{subject to} \\quad \n",
    "        & y_i \\leq t_i \\\\\n",
    "        & -y_i \\leq t_i \n",
    "        \\end{align*}\n",
    "        $$ \n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "__(c)__ \n",
    "Suppose $A\\theta-b = r$ and $t = \\begin{bmatrix} t_1 & t_2 & \\cdots & t_m \\end{bmatrix}^T $, then show that $L_1$ norm fitting/approximation problem, _i.e._, sum of (absolute) residuals can be coverted to\n",
    "\n",
    "\n",
    "<table style=\"border-style: hidden; border-collapse: collapse;\" width = \"60%\"> \n",
    "    <tr style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "        <td width = 30% style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "        $$ \\min_{\\theta} \\,\\lVert A\\theta-b \\rVert_{1} $$\n",
    "        </td>\n",
    "        <td width = 5% style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "             $$ \\iff $$\n",
    "             \n",
    "        </td>\n",
    "        <td width = 20%>\n",
    "        $$\n",
    "        \\begin{align*}\n",
    "        \\min_{t, \\theta} \\quad & t_1 + \\cdots + t_m \\\\\n",
    "        \\text{subject to} \\quad \n",
    "        & A\\theta - b \\leq t \\\\\n",
    "        & -(A\\theta - b) \\leq t \n",
    "        \\end{align*}\n",
    "        $$\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    " \n",
    "__(d)__ \n",
    "Change the LP in matrix form where $\\mathbb{1} = \\begin{bmatrix} 1& \\cdots & 1\\end{bmatrix}^T$ and $\\mathbb{0} = \\begin{bmatrix} 0& \\cdots & 0\\end{bmatrix}^T$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\min_{t, \\theta} \\quad & \\begin{bmatrix} \\mathbb{0}^T & \\mathbb{1}^T \\end{bmatrix} \\begin{bmatrix} \\theta \\\\ t \\end{bmatrix} \\\\\n",
    "\\text{subject to} \\quad \n",
    "& \\begin{bmatrix} A & -I \\\\ -A & -I \\end{bmatrix}\\begin{bmatrix} \\theta \\\\ t \\end{bmatrix} \n",
    "\\leq \\begin{bmatrix} b \\\\ -b \\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "__(e)__\n",
    "Use cvxpy in Python to find and plot a linear regression for data points with the above formulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.array([0.1, 0.4, 0.7, 1.2, 1.3, 1.7, 2.2, 2.8, 3.0, 4.0, 4.3, 4.4, 4.9])\n",
    "y = np.array([0.5, 0.9, 1.1, 1.5, 1.5, 2.0, 2.2, 2.8, 2.7, 3.0, 3.5, 3.7, 3.9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regularized least-squares problem has the form\n",
    "\n",
    "$$ \\min_{\\theta} \\;\\lVert A\\theta -y\\rVert_2^2 + \\lambda \\lVert \\theta \\rVert_2^2$$\n",
    "\n",
    "__(a)__ \n",
    "Show that the solution is given by (Do not use the method of Lagrangian multipliers)\n",
    "\n",
    "$$ \\hat{\\theta} = \\left( A^T A + \\lambda I_n \\right)^{-1} A^T y $$\n",
    "\n",
    "\n",
    "__(b)__\n",
    "Find and draw an approximated curve of the given data points in Python. Plot the result.\n",
    "- Use RBF\n",
    "- Pick a proper value of $\\lambda$ on your own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.linspace(-4.5, 4.5, 10)\n",
    "y = np.array([0.9819, 0.7973, 1.9737, 0.1838, 1.3180, -0.8361, -0.6591, -2.4701, -2.8122, -6.2512])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
