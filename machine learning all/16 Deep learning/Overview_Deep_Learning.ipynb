{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<font size = '6'><b>Neural Networks and Deep Learning: A Quick Overview</b></font>\n",
    "\n",
    "- <a href=\"./reference_files/deep_learning by Zico.pdf\" target=\"_blank\">Slides</a> from Prof. [Zico Kolter](http://www.zicokolter.com/) at CMU\n",
    "\n",
    "- Lectures from Prof. [Seungjin Choi](http://mlg.postech.ac.kr/) at POSTECH\n",
    "\n",
    "<table style=\"border-style: hidden; border-collapse: collapse;\" width = \"90%\"> \n",
    "    <tr style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "        <td width = 60% style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "             \n",
    "        </td>\n",
    "        <td width = 30%>\n",
    "        Collected by Prof. Seungchul Lee<br>\n",
    "        iSystems Design Lab<br>UNIST<br>http://isystems.unist.ac.kr/\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "Table of Contents\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Recall supervised learning setup\n",
    "\n",
    "- Input features $x^{(i)} \\in \\mathbb{R}^n$\n",
    "\n",
    "- Ouput $y^{(i)}$\n",
    "\n",
    "- Model parameters $\\theta \\in \\mathbb{R}^k$\n",
    "\n",
    "- Hypothesis function $h_{\\theta}: \\mathbb{R}^n \\rightarrow y$\n",
    "\n",
    "- Loss function $\\ell: y \\times y \\rightarrow \\mathbb{R}_+$\n",
    "\n",
    "<br>\n",
    "- Machine learning optimization problem\n",
    "\n",
    "$$ \\min_{\\theta} \\sum_{i=1}^{m}\\ell\\left( h_{\\theta}\\left(x^{(i)}\\right),y^{(i)}\\right)$$\n",
    "\n",
    "$\\quad \\;$(possibly plus some additional regularization)\n",
    "\n",
    "<br>\n",
    "We mainly considered the _linear_ hypothesis class\n",
    "\n",
    "$$h_{\\theta}\\left( x^{(i)}\\right) = \\theta^T \\phi (x^{(i)})$$\n",
    "\n",
    "for some set of _non-linear_ feature $\\phi: \\mathbb{R}^n \\rightarrow \\mathbb{R}^k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Challenges with linear models\n",
    "\n",
    "- Linear models crucially depend on choosing \"good\" features\n",
    "\n",
    "\n",
    "- Some \"stardard\" choices: polynomial features, radial basis functions, random features (srprisingly effective)\n",
    "\n",
    "\n",
    "- But, many specialized domains required highly engineered special features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Neural networks\n",
    "\n",
    "Neural networks are a simply a machine learning algorithm with a more complex hypothesis class, directly incorporating non-linearity (in the parameters)\n",
    "\n",
    "__Example__: neural network with one hidden layer\n",
    "\n",
    "$$h_{\\theta}(x) = \\Theta^{(2)} f\\left( \\Theta^{(1)}x\\right)$$\n",
    "\n",
    "where $\\Theta^{(1)} \\in \\mathbb{R}^{k \\times n}, \\Theta^{(2)} \\in \\mathbb{R}^{1 \\times k}$ and $f$ is some non-linear function applied elementwise to a vector (common choice is \"tanh\" function $\\tanh(x) = \\frac{1 - e^{-2x}}{1+e^{-2x}}$)\n",
    "\n",
    "<img src=\"./image_files/tanh.png\" width = 300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Architectures are often shown graphically\n",
    "\n",
    "<img src=\"./image_files/nn_structure.png\" width = 300>\n",
    "\n",
    "Middle layer $a$ is referred to as the hidden layer, there is nothing in the data that prescribes what values these should take, left up to the algorithm to decide\n",
    "\n",
    "Viewed another way: neural networks are like linear classifiers where the features themselves are also learned\n",
    "\n",
    "__Pros__\n",
    "\n",
    "- No need to manually engineer good features, just let the machine learning algorithm handle this part\n",
    "\n",
    "- It turns out that a 3-layer network is a universal function approximator, any non-linear function can be represented with a 3-layer network with a large enough hidden layer\n",
    "\n",
    "__Cons__\n",
    "\n",
    "- Minimizing loss on training data is no longer a convex optimization problem in parameters $\\theta$\n",
    "\n",
    "- Still need to engineer a good architecture (more on this shortly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Deep learning\n",
    "\n",
    "\"Deep\" neural networks typically refer to networks with multiple hidden layers\n",
    "\n",
    "<img src=\"./image_files/deep_structure.png\" width = 450>\n",
    "\n",
    "Note: original term \"deep learning\" referred to any machine learning architecture with multiple layers, including several probabilistic models, etc, but most work these days focuses on neural networks\n",
    "\n",
    "Motivation from neurobiology: brain appears to use multiple levels of interconnected neurons to process information (but careful,neurons in brain are not just non-linear functions)\n",
    "\n",
    "In practive: works better for many domains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Training Neural Networks\n",
    "\n",
    "## 2.1. Optimizing neural network parameters\n",
    "\n",
    "How do we optimize the parameters for the machine learning loss minimization problem with a neural network\n",
    "\n",
    "$$ \\min_{\\theta} \\sum_{i=1}^{m}\\ell\\left( h_{\\theta}\\left(x^{(i)}\\right),y^{(i)}\\right)$$\n",
    "\n",
    "now is this problem non-convex?\n",
    "\n",
    "Just do exactly what we did before:  initialize with random weights and run stochastic gradient descent\n",
    "\n",
    "Now have the possibility of local optima, and function can be harder to optimize, but we will not worry about all that because the resulting model still often perform better than linear models\n",
    "\n",
    "Stochastic gradient descent, repeat:\n",
    "\n",
    "- Select some example $i$\n",
    "\n",
    "$$ \\theta:= \\theta - \\alpha \\nabla_{\\theta} \\left( h_{\\theta} \\left(x^{(i)}\\right),y^{(i)}\\right)$$\n",
    "\n",
    "So how do we compute the gradient with respect to all the parameters in a neural network (_i.e._, all weights $\\Theta^{(1)}, \\Theta^{(2)}, \\cdots$) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Backpropagation\n",
    "\n",
    "Backpropagation is a method for computing all the necessary gradients using one \"forward pass\" (just computing all the values at layers), and one \"backward pass\" (computing gradients backwards in the network)\n",
    "\n",
    "The equations sometimes look complex, it is just an application of the chain rule of calculus\n",
    "\n",
    "First, some notation that will make things a bit easier:\n",
    "\n",
    "- Activations: $a^{(i)}$ valuesat hidden layer $i$ (with convention that $a^{(1)} = x$)\n",
    "\n",
    "- Linear activation: $z^{(i)} = \\Theta^{(i)}a^{(i)}$, so $a^{(i+1)} = f(z^{(i)})$\n",
    "\n",
    "Let's treat everything as scalars for now, and consider a network with two hidden layers:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial \\Theta^{(1)}}\\ell \\left( h_{\\theta}(x),y \\right) & = \\frac{\\partial}{\\partial \\Theta^{(1)}}\\ell \\left( \\Theta^{(3)}f(\\Theta^{(2)}f(\\Theta^{(1)}x))   ,y \\right)\\\\\n",
    "& = \\ell'\\left( z^{(3)},y\\right) \\Theta^{(3)} \\frac{\\partial}{\\partial \\Theta^{(1)}} f(\\Theta^{(2)}f(\\Theta^{(1)}x))\\\\\n",
    "& = \\ell'\\left( z^{(3)},y\\right) \\Theta^{(3)} f'\\left(z^{(2)} \\right)\\Theta^{(2)}\\frac{\\partial}{\\partial \\Theta^{(1)}}f(\\Theta^{(1)}x)\\\\\n",
    "& = \\ell'\\left( z^{(3)},y\\right) \\Theta^{(3)} f'\\left(z^{(2)} \\right)\\Theta^{(2)} f'\\left( z^{(1)}\\right) a^{(1)}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "By the same procedure\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\Theta^{(2)}}\\ell \\left( h_{\\theta}(x),y \\right) = \\ell'\\left( z^{(3)},y\\right) \\Theta^{(3)} f'\\left(z^{(2)} \\right) a^{(2)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to compute gradients with respect to all the parameters $\\Theta^{(1)},\\cdots,\\Theta^{(L)}$, we can \"reuse\" parts of this computation\n",
    "\n",
    "Let\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "g^{(L)} & = \\ell'\\left( z^{(L)}\\right) \\Theta^{(L)}\\\\\n",
    "g^{(i)} & = g^{(i+1)} f'\\left(z^{(i)} \\right)\\Theta^{(i)}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "then\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial \\Theta^{(i)}}\\ell \\left( h_{\\theta}(x),y \\right) = g^{(i+1)} f'\\left(z^{(i)} \\right) a^{(i)}\n",
    "$$\n",
    "\n",
    "It takes just slightly more advanced calculus, but it turns out the general matrix/vector case is exactly the same, just being carful with the ordering/size of matrix multiplication\n",
    "\n",
    "The full backpropagation algorithm:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "g^{(L)} & = {\\Theta^{(L)}}^T \\ell'\\left( z^{(L)}\\right) \\\\\n",
    "g^{(i)} & = {\\Theta^{(i)}}^T g^{(i+1)} f'\\left(z^{(i)} \\right)\\\\\n",
    "\\nabla_{\\Theta^{(i)}}\\ell \\left( h_{\\theta}(x),y \\right) &= \\left( g^{(i+1)} \\cdot f'\\left(z^{(i)} \\right) \\right) {a^{(i)}}^T\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\cdot$ denotes elementwise multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradients can get somewhat tedious to derive by hand, especially for the more complex models \n",
    "\n",
    "__Fortunately, a lot of this work has already been done for us__\n",
    "\n",
    "Tools like Theano, Torch, Caffe, TensorFlow all let us specify the network structure and then automatically compute all gradients (and use GPUs to do so)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://www.youtube.com/embed/uXt8qF2Zzfo\" \n",
       " width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<iframe src=\"https://www.youtube.com/embed/uXt8qF2Zzfo\" \n",
    " width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. What is changed since the 80s?\n",
    "\n",
    "All these algorithms (and most of the extensions in later slides), were developed in the 80s or 90s\n",
    "\n",
    "So why are these just becoming more popular in the last few years?\n",
    "\n",
    "- more data\n",
    "\n",
    "- faster computers\n",
    "\n",
    "- (some) better optimization techniques\n",
    "\n",
    "__Unsupervised pre-training (Hinton et al., 2006)__: \"Pre-train\" the network have the hidden layers recreate their input, one layer at a time, in an unsupervised fashion\n",
    "\n",
    "- This paper was partly responsible for re-igniting the interest in deep neural networks, but the general feeling now is that it does not help much\n",
    "\n",
    "__Dropout (Hinton et al., 2012)__: During training and computation of gradients, randomly set about half the hidden units to zero \n",
    "\n",
    "- Acts like regularization, prevents the parameters for overfitting to particular exampels\n",
    "\n",
    "__Different non-linear fuctions (Nair and Hinton, 2010)__: Use non-linearity $f(x) = \\max\\{0,x\\}$ instead of $f(x) = \\tanh(x)$\n",
    "\n",
    "## 3.2. Again, why successful?\n",
    "\n",
    "- Pre-training: Restricted Boltzmann machine (RBM), Autoencoder, nonnegative matrix factorization (NMF)\n",
    "\n",
    "- Training: Dropout\n",
    "\n",
    "- Rectified linear units: No vanishing gradient, sparse activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://www.youtube.com/embed/VrMHA3yX_QI\" \n",
       "width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<iframe src=\"https://www.youtube.com/embed/VrMHA3yX_QI\" \n",
    "width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Advanced Models and Architectures in Deep Learning\n",
    "- Convolutional neural networks\n",
    "\n",
    "- Recurrent neural networks\n",
    "\n",
    "- Deep reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Convolutional neural networks\n",
    "\n",
    "One of the biggest successes for neural networks has come in computer vision, using convolution neural networks.\n",
    "\n",
    "In traditional neural networks, images are treated as unstructured vectors $x \\in \\mathbb{R}^{W\\cdot H}$, and we learn arbitrary transformation _i.e._, $f(\\Theta x)$\n",
    "\n",
    "But this does not seem like a good model, slightly shifting/scaling image winds up with a very different input vector, so we need to learn all these invariances in our parameters.\n",
    "\n",
    "Basic idea of convolutional neural networks: parameters are elements of a (set of) convolutional filters applied to the image\n",
    "\n",
    "$$ a^{(i+1)} = f\\left( a^{(i)} \\ast \\Theta^{(i)}\\right)$$\n",
    "\n",
    "The function $f$ also does downsampling \"max-pooling\" to produce lower dimensional images and translation invariance at later layers.\n",
    "\n",
    "__ImageNet Large Scale Visual Recognition Challenge (ILSVRC)__\n",
    "\n",
    "- Lenet-5 (LeCun et al., 1998) architecture, 1% error on MNIST classification (compare to 10% for linear classifier)\n",
    "\n",
    "<img src=\"./image_files/Lenet.png\" width = 700>\n",
    "\n",
    "- \"AlexNet\" (Krizhevsky et al., 2012), work ImageNet 2012 competition with a Top-5 error rate of 15.3% (next best system with highly engineered features based upon SIFT got 26.1% error)\n",
    "\n",
    "<img src=\"./image_files/AlexNet.png\" width = 700>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://www.youtube.com/embed/bL1Zymz1b7g\" \n",
       "width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<iframe src=\"https://www.youtube.com/embed/bL1Zymz1b7g\" \n",
    "width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\;\\;$<a href=\"./reference_files/deep_learning_tutorial_2015.pdf\" target=\"_blank\">Slides available</a> (click to open in a new page)\n",
    "\n",
    "- By Phillip Isola\n",
    "\n",
    "- Tutorial Series in Computational topics for BCS \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Recurrent neural networks\n",
    "\n",
    "Framework for dealing with _sequence_ data: hidden units for elements in sequence are fed into hidden units for subsequent elements\n",
    "\n",
    "- difference equation\n",
    "\n",
    "- time series analysis\n",
    "\n",
    "- hidden Markov model (HMM)\n",
    "\n",
    "<img src=\"./image_files/RNN.png\" width = 500>\n",
    "<center>Figure from Karpathy, 2015 (http://karpathy.github.io/2015/05/21/rnn-effectiveness/)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Deep reinforcement learning\n",
    "\n",
    "Instead of maintaining a separate Q-value for each state/action pair, we can use a deep neural network (or any other class of functions) to represent the Q function\n",
    "\n",
    "Q-Learning update rule becomes\n",
    "\n",
    "$$\\theta := \\theta - \\alpha \\left( R+\\gamma \\max_{a'}Q(s',a';\\theta) -Q(s,a;\\theta)\\right) \\nabla_{\\theta} Q(s,a;\\theta) $$\n",
    "\n",
    "where $\\theta$ are the parameters (_i.e._, network weights) that specify our reprentation of the Q function\n",
    "\n",
    "Google DeepMind paper shows deep learning to play Atari video games: Pong, Breakout, Space Invaders, Seaquest, Beam Rider (Mihn et al., 2013)\n",
    "\n",
    "- Markov decision process (MDP)\n",
    "\n",
    "- Bellman equations\n",
    "\n",
    "- Reinforcement learning\n",
    "\n",
    "- Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
