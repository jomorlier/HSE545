{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size = '6'><b>Neural Networks</b></font>\n",
    "\n",
    "- Ryan Harris\n",
    "    - https://www.youtube.com/playlist?list=PL29C61214F2146796\n",
    "    - https://www.youtube.com/playlist?list=PLRyu4ecIE9tibdzuhJr94uQeKnOFkkbq6\n",
    "    - <a href=\"./files/BackPropagation.pdf\" target=\"_blank\">Backpropagation Slides</a> \n",
    "    \n",
    "- Gene Kogan   \n",
    "    - http://ml4a.github.io/classes/neural-aesthetic/\n",
    "    - https://github.com/ml4a/ml4a-guides/blob/master/notebooks/simple_neural_networks.ipynb\n",
    "\n",
    "<table style=\"border-style: hidden; border-collapse: collapse;\" width = \"90%\"> \n",
    "    <tr style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "        <td width = 60% style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "             \n",
    "        </td>\n",
    "        <td width = 30%>\n",
    "        Collected by Prof. Seungchul Lee<br>\n",
    "        iSystems Design Lab.<br>\n",
    "        http://isystems.unist.ac.kr/<br>\n",
    "        UNIST\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Table of Contents\n",
    "<div id=\"toc\"></div>\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Structure of Neural Networks\n",
    "\n",
    "__The neuron__\n",
    "\n",
    "- The sigmoid equation is what is typically used as a transfer function between neurons. It is similar to the step fuction, but is continuous and differentiable.\n",
    "\n",
    "$$ \\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "- One useful property of this transfer function is the simplicity of computing its derivative.\n",
    "\n",
    "$$\\frac{d}{dx}\\sigma(x) = \\sigma' = \\sigma(x) (1-\\sigma(x))$$\n",
    "\n",
    "__Single input neuron__\n",
    "\n",
    "<img src=\"./image_files/single_neuron.png\" width = 300>\n",
    "\n",
    "$$ O = \\sigma(\\xi \\omega + \\theta) $$\n",
    "\n",
    "__Multiple input neuron__\n",
    "\n",
    "<img src=\"./image_files/multiple_neuron.png\" width = 300>\n",
    "\n",
    "$$ O = \\sigma(\\xi_1 \\omega_1 + \\xi_2 \\omega_2 + \\xi_3 \\omega_3 +\\theta) $$\n",
    "\n",
    "__A neural network__\n",
    "\n",
    "<img src=\"./image_files/nn_03.png\" width = 300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Learning: Backpropagation Algorithm\n",
    "\n",
    "__Notation__\n",
    "\n",
    "- $x_j^\\ell$: Input to node $j$ of layer $\\ell$\n",
    "\n",
    "- $W_{ij}^\\ell$: Weight from layer $\\ell - 1$ node $i$ to layer $\\ell$ node $j$\n",
    "\n",
    "- $\\sigma(x) = \\frac{1}{1+e^{-x}}$: Sigmoid transfer function\n",
    "\n",
    "- $\\theta_j^{\\ell}$: Bias of node $j$ of layer $\\ell$\n",
    "\n",
    "- $O_j^{\\ell}$: Output of node $j$ in layer $\\ell$\n",
    "\n",
    "- $t_j$: Target value of node $j$ of the output layer\n",
    "\n",
    "<br>\n",
    "<font size='4'><b>The error calculation</b></font>\n",
    "\n",
    "Given a set of training data points $t_k$ and output layer output $O_k$ we can write the error as\n",
    "\n",
    "$$ E = \\frac{1}{2} \\sum_{k \\in K} (O_k - t_k)^2$$\n",
    "\n",
    "We want to calculate $\\frac{\\partial E}{\\partial W_{jk}^{\\ell}}$, the rate of change of the error with respect to the given connective weight, so we can minimize it.\n",
    "\n",
    "Now we consider two cases: the node is an output node, or it is in a hidden layer\n",
    "\n",
    "__1) Output layer node__\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial E}{\\partial W_{jk}} &= \\frac{\\partial}{\\partial W_{jk}} \\frac{1}{2} (O_k - t_k)^2 = (O_k - t_k)\\frac{\\partial}{\\partial W_{jk}} O_k = (O_k - t_k)\\frac{\\partial}{\\partial W_{jk}} \\sigma(x_k)\\\\\n",
    "&= (O_k - t_k) \\sigma(x_k) (1-\\sigma(x_k)) \\frac{\\partial}{\\partial W_{jk}} x_k \\\\\n",
    "&= (O_k - t_k) O_k (1 - O_k) O_j\n",
    "\\end{align*}\n",
    "\n",
    "$\\quad$For notation purposes, I will define $\\delta_k$ to be the expression $(O_k - t_k) O_k (1 - O_k)$, so we can rewrite the equation above as\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial W_{jk}} = O_j \\delta_k $$\n",
    "\n",
    "__2) Hidden layer node__\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial E}{\\partial W_{ij}} &= \\frac{\\partial}{\\partial W_{ij}} \\frac{1}{2} \\sum_{k \\in K} (O_k - t_k)^2 = \\sum_{k \\in K} (O_k - t_k)\\frac{\\partial}{\\partial W_{ij}} O_k = \\sum_{k \\in K} (O_k - t_k)\\frac{\\partial}{\\partial W_{ij}} \\sigma(x_k)\\\\\n",
    "&= \\sum_{k \\in K} (O_k - t_k) \\sigma(x_k) (1-\\sigma(x_k)) \\frac{\\partial}{\\partial W_{ij}} x_k \\\\\n",
    "&= \\sum_{k \\in K} (O_k - t_k) O_k (1 - O_k) \\frac{\\partial x_k}{\\partial O_j}\\cdot \\frac{\\partial O_j}{\\partial W_{ij}} = \\sum_{k \\in K} (O_k - t_k) O_k (1 - O_k) W_{jk}\\cdot \\frac{\\partial O_j}{\\partial W_{ij}}\\\\\n",
    "&= \\frac{\\partial O_j}{\\partial W_{ij}} \\cdot \\sum_{k \\in K} (O_k - t_k) O_k (1 - O_k) W_{jk}\\\\\n",
    "&= O_j (1-O_j)\\frac{\\partial x_j}{\\partial W_{ij}} \\cdot \\sum_{k \\in K} (O_k - t_k) O_k (1 - O_k) W_{jk}\\\\\n",
    "&= O_j (1-O_j)O_i \\cdot \\sum_{k \\in K} (O_k - t_k) O_k (1 - O_k) W_{jk}\\\\\n",
    "&= O_i O_j (1-O_j) \\sum_{k \\in K} \\delta_k W_{jk}\n",
    "\\end{align*}\n",
    "\n",
    "$\\quad$Similar to before we will now define all terms besides $O_i$ to be $\\delta_j = O_j (1-O_j) \\sum_{k \\in K} \\delta_k W_{jk}$, so we have\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial W_{ij}} = O_i \\delta_j$$\n",
    "\n",
    "\n",
    "__How weights affect errors__\n",
    "\n",
    "- For an output layer node $k \\in K$\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial W_{jk}} = O_j \\delta_k $$\n",
    "\n",
    "$\\quad \\;\\,$where $$\\delta_k = (O_k - t_k) O_k (1 - O_k)$$\n",
    "\n",
    "- For a hidden layer node $j \\in J$\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial W_{ij}} = O_i \\delta_j$$\n",
    "\n",
    "$\\quad \\;\\,$where $$\\delta_j = O_j (1-O_j) \\sum_{k \\in K} \\delta_k W_{jk}$$\n",
    "\n",
    "__What about the bias?__\n",
    "\n",
    "If we incorporate the bias term $\\theta$ into the equation you will find that\n",
    "\n",
    "$$ \\frac{\\partial O}{\\partial \\theta} = 1$$\n",
    "\n",
    "This is why we view the bias term as output from a node which is always one. This holds for any layer $\\ell$, a substitution into the previous equations gives us that\n",
    "\n",
    "$$ \\frac{\\partial E}{\\partial \\theta} = \\delta_{\\ell}$$\n",
    "\n",
    "<br>\n",
    "<font size='4'><b>The backpropagation algorithm using gradient descent</b></font>\n",
    "\n",
    "1. Run the network forward with your input data to get the netwrok output\n",
    "\n",
    "2. For each output node compute\n",
    "$$\\delta_k = (O_k - t_k) O_k (1 - O_k)$$\n",
    "3. For eatch hidden node calculate\n",
    "$$\\delta_j = O_j (1-O_j) \\sum_{k \\in K} \\delta_k W_{jk}$$\n",
    "4. Update the weights and biases as follows<br>\n",
    "Given\n",
    "$$\\begin{align*}\n",
    "\\Delta W &= -\\eta \\delta_{\\ell} O_{\\ell -1}\\\\\n",
    "\\Delta \\theta &= -\\eta \\delta_{\\ell}\n",
    "\\end{align*}$$\n",
    "apply\n",
    "$$\\begin{align*}\n",
    "W &\\leftarrow W + \\Delta W \\\\\n",
    "\\theta &\\leftarrow \\theta + \\Delta \\theta\n",
    "\\end{align*}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://www.youtube.com/embed/aVId8KMsdUU?list=PL29C61214F2146796\" \n",
       "width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<iframe src=\"https://www.youtube.com/embed/aVId8KMsdUU?list=PL29C61214F2146796\" \n",
    "width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://www.youtube.com/embed/zpykfC4VnpM?list=PL29C61214F2146796\" \n",
       "width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<iframe src=\"https://www.youtube.com/embed/zpykfC4VnpM?list=PL29C61214F2146796\" \n",
    "width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Implementation in python\n",
    "\n",
    "Try to be very explicit about what parts are \"up in the air\" (i.e. modifiable) so you get a sense of where you can experiment with new neural networks.\n",
    "\n",
    "- [ml4a chapter on neural networks](http://ml4a.github.io/ml4a/neural_networks/). \n",
    "\n",
    "- Michael Nielsen's [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/), \n",
    "\n",
    "- Goodfellow, Bengio, and Courville's [Deep Learning](http://www.deeplearningbook.org/) book\n",
    "\n",
    "- Yoav Goldberg's \"[A Primer on Neural Network Models for Natural Language Processing](http://arxiv.org/abs/1510.00726)\". \n",
    "\n",
    "- [Notes](http://frnsys.com/ai_notes/machine_learning/neural_nets.html) on neural networks include a lot more details and additional resources as well.\n",
    "\n",
    "For the other neural network guides we will mostly rely on the excellent [Keras](http://keras.io/) library, which makes it very easy to build neural networks and can take advantage of [Theano](http://deeplearning.net/software/theano/) or [TensorFlow](https://www.tensorflow.org/)'s optimizations and speed. However, to demonstrate the basics of neural networks, we'll use `numpy` so we can see exactly what's happening every step of the way.\n",
    "\n",
    "Each unit (neuron) has a vector of weights - these are the parameters that the network learns.\n",
    "\n",
    "The inner operations of the basic unit is straightforward. We collapse the weight vector $w$ and input vector $v$ into a scalar by taking their dot product. Often a _bias_ term $b$ is added to this dot product; this bias is also learned. then we pass this dot product through an _activation function_ $f$, which also returns a scalar. Activation functions are typically nonlinear so that neural networks can learn nonlinear functions. I'll mention a few common activation functions in a bit, but for now let's see what a basic unit is doing:\n",
    "\n",
    "```python\n",
    "def unit(inputs, weights, b):\n",
    "    return activation_function(np.dot(inputs, weights) + b)\n",
    "```\n",
    "\n",
    "Note that the output units often do not have an activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. A basic neural network with `numpy`\n",
    "\n",
    "First we'll import `numpy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With machine learning we are trying to find a hidden function that describes data that we have. Here we are going to cheat a little and define the function ourselves and then use that to generate data. Then we'll try to \"reverse engineer\" our data and see if we can recover our original function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  4.]\n",
      " [ 29.]\n",
      " [  9.]]\n"
     ]
    }
   ],
   "source": [
    "def unknown_function(X):\n",
    "    coeff = np.array([[2., -1., 5.]])\n",
    "    return np.dot(X, coeff.T)\n",
    "\n",
    "X = np.array([\n",
    "    [4.,9.,1.],\n",
    "    [2.,5.,6.],\n",
    "    [1.,8.,3.]\n",
    "])\n",
    "\n",
    "t = unknown_function(X) # target\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to set up our simple neural network. It will have just one hidden layer with two units (which we will refer to as unit 1 and unit 2).\n",
    "\n",
    "<img src=\"./image_files/simple_nn_structure.png\" width = 250>\n",
    "\n",
    "First we have to define the weights (i.e. parameters) of our network.\n",
    "\n",
    "- We have three inputs each going into two units, then one bias value for each unit, so we have eight parameters for the hidden layer.\n",
    "\n",
    "- Then we have the output of those two hidden layer units going to the output layer, which has only one unit - this gives us two more parameters, plus one bias value.\n",
    "\n",
    "- So in total, we have eleven parameters.\n",
    "\n",
    "Let's set them to arbitrary values for now (random initialization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initial hidden layer weights\n",
    "hidden_layer_weights = np.array([\n",
    "    [0.5, 0.5, 0.5],    # unit 1\n",
    "    [0.1, 0.1, 0.1]     # unit 2\n",
    "])\n",
    "hidden_layer_biases = np.array([1. ,1.])\n",
    "\n",
    "# initial output layer weights\n",
    "output_weights = np.array([[1., 1.]])\n",
    "output_biases = np.array([1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use $\\tanh$ activations for our hidden units, so let's define that real quick:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def activation(X):\n",
    "    return np.tanh(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\tanh$ activations are quite common, but you may also encounter sigmoid activations and, more recently, ReLU activations (which output 0 when $x \\leq 0$ and output $x$ otherwise). These activation functions have different benefits: \n",
    "- ReLUs in particular are robust against training difficulties that come when dealing with deeper networks.\n",
    "\n",
    "To make things clearer later on, we'll also define the linear function that combines a unit's input with its weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear(input, weights, biases):\n",
    "    return np.dot(input, weights.T) + biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do a forward pass with our inputs $X$ to see what the predicted outputs are.\n",
    "\n",
    "## 3.2. Forward pass\n",
    "\n",
    "First, we'll pass the input through the hidden layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden output\n",
      "[[ 0.99999977  0.98367486]\n",
      " [ 0.99999939  0.9800964 ]\n",
      " [ 0.99999834  0.97574313]]\n"
     ]
    }
   ],
   "source": [
    "hidden_linout = linear(X, hidden_layer_weights, hidden_layer_biases)\n",
    "hidden_output = activation(hidden_linout)\n",
    "\n",
    "print('hidden output')\n",
    "print(hidden_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(We're keeping the neuron unit's intermediary value, `hidden_linout` for use in backpropagation.)\n",
    "\n",
    "Then we'll take the hidden layer's output and pass it through the output layer to get our predicted outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted\n",
      "[[ 2.98367463]\n",
      " [ 2.98009578]\n",
      " [ 2.97574147]]\n"
     ]
    }
   ],
   "source": [
    "output_linout = linear(hidden_output, output_weights, output_biases)\n",
    "output_output = output_linout # no activation function on output layer\n",
    "\n",
    "predicted = output_output\n",
    "print('predicted')\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the mean squared error of our predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean squared error\n",
      "238.120007837\n"
     ]
    }
   ],
   "source": [
    "mse = np.mean((t - predicted)**2)\n",
    "print('mean squared error')\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take this error and backpropagate it through the network. This will tell us how to update our weights.\n",
    "\n",
    "## 3.3. Backpropagation\n",
    "\n",
    "Since backpropagation is essentially a chain of derivatives (that is used for gradient descent), we'll need the derivative of our activation function, so let's define that first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def activation_deriv(X):\n",
    "    return 1 - np.tanh(X)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we want to set a learning rate - this is a value from 0 to 1 which affects how large we tweak our parameters by for each training iteration.\n",
    "\n",
    "- You don't want to set this to be too large or else training will never converge (your parameters might get really big and you'll start seeing a lot of `nan` values).\n",
    "\n",
    "- You don't want to set this to be too small either, otherwise training will be very slow. There are more sophisticated forms of gradient descent that deal with this, but those are beyond the scope of this guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll propagate the error through the output layer (I won't go through the derivation of each step but they are straightforward to work out if you know a bit about derivatives):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# derivative of mean squared error\n",
    "error = predicted - t\n",
    "\n",
    "# delta for the output layer (no activation on output layer)\n",
    "delta_output = error\n",
    "\n",
    "# output layer updates\n",
    "output_weights_update = delta_output.T.dot(hidden_output)\n",
    "output_biases_update = delta_output.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then through the hidden layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# push back the delta to the hidden layer\n",
    "delta_hidden = delta_output*output_weights*activation_deriv(hidden_linout)\n",
    "\n",
    "# hidden layer updates\n",
    "hidden_weights_update = delta_hidden.T.dot(X)\n",
    "hidden_biases_update = delta_hidden.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can apply the updates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_weights -= output_weights_update*learning_rate\n",
    "output_biases -= output_biases_update*learning_rate\n",
    "\n",
    "hidden_layer_weights -= hidden_weights_update*learning_rate\n",
    "hidden_layer_biases -= hidden_biases_update*learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's one training iteration! In reality, you would do this many, many times - feedforward, backpropagate, update weights, then rinse and repeat. That's the basics of a neural network - at least, the \"vanilla\" kind. There are other more sophisticated kinds (recurrent and convolutional neural networks are two of the most common) that are covered in other guides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.98367463]\n",
      " [ 2.98009578]\n",
      " [ 2.97574147]]\n"
     ]
    }
   ],
   "source": [
    "print predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Neural Networks in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  4.]\n",
      " [ 29.]\n",
      " [  9.]]\n",
      "[[  5.54772718]\n",
      " [ 26.56860045]\n",
      " [  9.73852491]]\n"
     ]
    }
   ],
   "source": [
    "# hidden layer weights\n",
    "hidden_layer_weights = np.array([\n",
    "    [0.5, 0.5, 0.5],    # unit 1\n",
    "    [0.1, 0.1, 0.1]     # unit 2\n",
    "])\n",
    "hidden_layer_biases = np.array([1. ,1.])\n",
    "\n",
    "# output layer weights\n",
    "output_weights = np.array([[1., 1.]])\n",
    "output_biases = np.array([1.])\n",
    "\n",
    "for i in range(1000):\n",
    "    \n",
    "    hidden_linout = linear(X, hidden_layer_weights, hidden_layer_biases)\n",
    "    hidden_output = activation(hidden_linout)\n",
    "    \n",
    "    output_linout = linear(hidden_output, output_weights, output_biases)\n",
    "    output_output = output_linout # no activation function on output layer\n",
    "\n",
    "    predicted = output_output\n",
    "\n",
    "    # derivative of mean squared error\n",
    "    error = predicted - t\n",
    "\n",
    "    # delta for the output layer (no activation on output layer)\n",
    "    delta_output = error\n",
    "\n",
    "    # output layer updates\n",
    "    output_weights_update = delta_output.T.dot(hidden_output)\n",
    "    output_biases_update = delta_output.sum(axis = 0)\n",
    "\n",
    "    # push back the delta to the hidden layer\n",
    "    delta_hidden = delta_output*output_weights*activation_deriv(hidden_linout)\n",
    "\n",
    "    # hidden layer updates\n",
    "    hidden_weights_update = delta_hidden.T.dot(X)\n",
    "    hidden_biases_update = delta_hidden.sum(axis = 0)\n",
    "\n",
    "    output_weights -= output_weights_update*learning_rate\n",
    "    output_biases -= output_biases_update*learning_rate\n",
    "\n",
    "    hidden_layer_weights -= hidden_weights_update*learning_rate\n",
    "    hidden_layer_biases -= hidden_biases_update*learning_rate\n",
    "\n",
    "print t    \n",
    "print predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
