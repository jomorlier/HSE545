{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\"><b>Parameter Estimation in Probabilistic Model</b></font>\n",
    "\n",
    "Table of Contents\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1. Generative model\n",
    "\n",
    "$$\n",
    "P\\left(y \\mid X,\\omega,\\sigma^2 \\right) = N\\left(\\omega^TX,\\sigma^2 \\right)$$\n",
    "\n",
    "<br>\n",
    "<img src=\"./image_files/ML001.png\" width = 250>\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"./image_files/ML002.png\" width = 250>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "Estimate pramters $\\theta\\left(\\omega,\\sigma^2\\right)$ such that maximize likelihood given a generative model\n",
    "- Given observed data $$ D=\\{(x_1,y_1),(x_2,y_2),\\cdots,(x_m,y_m)\\}$$\n",
    "\n",
    "\n",
    "- Generative model structure \n",
    "\n",
    "$$ \n",
    "\\begin{align*}\n",
    "y_i &= \\hat{y}_i + \\varepsilon \\\\\n",
    "    &=\\omega^T x_i+\\varepsilon,\\quad \\varepsilon \\sim N\\left(0,\\sigma^2\\right)\n",
    "\\end{align*}    \n",
    "$$\n",
    "    \n",
    "\n",
    "- Find parameters $\\omega$ and $\\sigma$ that maximize the likelihood over the observed data or\n",
    "\n",
    "\n",
    "- Likelihood:\n",
    "\n",
    "\n",
    "$$\\begin{align*} L(\\omega,\\sigma)\n",
    "& = P\\left(y_1,y_2,\\cdots,y_m \\mid x_1,x_2,\\cdots,x_m; \\; \\underbrace{\\omega, \\sigma}_{\\theta}\\right)\\\\\n",
    "& = \\prod\\limits_{i=1}^{m} P\\left(y_i \\mid x_i; \\; \\omega,\\sigma\\right)\\\\\n",
    "& = \\frac{1}{(2\\pi\\sigma^2)^\\frac{m}{2}}\\exp\\left(-\\frac{1}{2\\sigma^2}\\sum\\limits_{i=1}^m(y_i-\\omega^T x_i)^2\\right)\n",
    "\\end{align*}$$\n",
    "\n",
    "\n",
    "- Perhaps the simplest (but widely used) parameter estimation method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.1. Given $m$ data points, drawn from an exponential distribution \n",
    "\n",
    "- Exponential distribution from [Fundamentals of statistics](http://www.statlect.com/exponential_distribution_maximum_likelihood.htm)\n",
    "\n",
    "<br>\n",
    "$$f(y)=\\frac{1}{a}\\exp\\left(-\\frac{1}{a}y \\right)\\text{: generative model}$$\n",
    "<br><br>\n",
    "$$\\begin{align*} L\n",
    "& = P\\left(y_1,y_2,\\cdots,y_m \\mid a \\right)\\\\\n",
    "& = \\prod\\limits_{i=1}^m\\frac{1}{a} \\exp\\left(-\\frac{1}{a}y_i \\right)\\\\\n",
    "& = \\frac{1}{a^m} \\exp\\left(-\\frac{1}{a}\\sum\\limits_{i=1}^m y_i \\right)\\\\ \\\\\n",
    "\\text{Log-likelihood} \\; \\ell &=\\text{log}L=-m\\text{log}a-\\frac{1}{a}\\sum\\limits_{i=1}^m y_i\n",
    "\\end{align*} $$\n",
    "\n",
    "- Find $a$ that maximizes $\\ell$\n",
    "$$\\frac{d\\ell}{da}=-\\frac{m}{a}+\\frac{1}{a^2}\\sum\\limits_{i=1}^m y_i = 0 \\\\\n",
    "\\therefore \\;\\; a_{ML}=\\frac{1}{m}\\sum\\limits_{i=1}^m y_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-d999ba08bcbf>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-d999ba08bcbf>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    x = exprnd(700,1,m);  % mu = 700\u001b[0m\n\u001b[1;37m                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "% exponential random variable\n",
    "m = 50;\n",
    "x = exprnd(700,1,m);  % mu = 700\n",
    "\n",
    "% MLE of \n",
    "1/m*sum(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.2. Given $m$ data points, drawn from a Gaussian distribution\n",
    "\n",
    "$$\n",
    "\\begin{align*} \n",
    "P\\left(y=y_i \\mid \\mu,\\sigma^2\\right)\n",
    "& =\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp \\left(-\\frac{1}{2\\sigma^2}(y_i-\\mu)^2 \\right) \\text{: generative model}\\\\\\\\\n",
    "L = P \\left(y_1,y_2,\\cdots,y_m \\mid \\mu,\\sigma^2 \\right)\n",
    "& =\\prod\\limits_{i=1}^m\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{1}{2\\sigma^2}\\left(y_i-\\mu\\right)^2\\right)\\\\\n",
    "& = \\frac{1}{(2\\pi)^\\frac{m}{2}\\sigma^m}\\exp\\left(-\\frac{1}{2\\sigma^2}\\sum\\limits_{i=1}^m(y_i-\\mu)^2 \\right)\\\\\\\\\n",
    "\\ell &=-\\frac{m}{2} \\text{log}{2\\pi}-m \\text{log}{\\sigma}-\\frac{1}{2\\sigma^2}\\sum\\limits_{i=1}^m(y_i-\\mu)^2\n",
    "\\end{align*}$$\n",
    "\n",
    "- To maximize, $\\frac{\\partial \\ell}{\\partial \\mu}=0, \\frac{\\partial \\ell}{\\partial \\sigma}=0$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\ell}{\\partial \\mu} &=\\frac{1}{\\sigma^2}\\sum\\limits_{i=1}^m(y_i-\\mu)=0 \\;\\; \\implies \\mu_{ML}=\\frac{1}\n",
    "{m}\\sum\\limits_{i=1}^m y_i\\\\\n",
    "\\frac{\\partial \\ell}{\\partial \\sigma} &= -\\frac{m}{\\sigma}+\\frac{1}{\\sigma^3}\\sum\\limits_{i=1}^m(y_i-\\mu)^2=0 \\;\\; \\implies \n",
    "\\sigma_{ML}^2=\\frac{1}{m}\\sum\\limits_{i=1}^m(y_i-\\mu)^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- BIG Lesson\n",
    "    - We often compute a mean and variance to represent data statistics\n",
    "    - We kind of assume that a data set is Gaussian distributed\n",
    "    - Good news: sample mean is Gaussian distributed by the central limit theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Numerical Simulation__\n",
    "\n",
    "- Compute likelihood function\n",
    "    - maximize the likelihood function \n",
    "    - adjust the mean and variance of the Gaussian to maximize its product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-179b327a48b0>, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-179b327a48b0>\"\u001b[1;36m, line \u001b[1;32m14\u001b[0m\n\u001b[1;33m    muhat = [-5 0 5 mu_ml];\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "%plot -s 560,600\n",
    "% MLE of Gaussian distribution \n",
    "% mu\n",
    "\n",
    "m = 20;\n",
    "mu = 0;\n",
    "sigma = 5;\n",
    "\n",
    "x = normrnd(mu,sigma,1,m);\n",
    "xp = linspace(-20,20,100);\n",
    "y0 = zeros(1,m);\n",
    "\n",
    "mu_ml = mean(x);\n",
    "muhat = [-5 0 5 mu_ml];\n",
    "\n",
    "for i = 1:4   \n",
    "    yp = normpdf(xp,muhat(i),sigma); % to plot pdf\n",
    "    y = normpdf(x,muhat(i),sigma);\n",
    "    g = sum(log(y));\n",
    "\n",
    "    subplot(4,1,i)\n",
    "    plot(xp,yp,'r');  hold on\n",
    "    plot(x,y0,'k+');\n",
    "    plot(x,y,'bo','markerfacecolor','b','markersize',4)\n",
    "    plot([x;x],[y0;y],'k--');  hold off\n",
    "\n",
    "    htitle = title(['$\\hat{\\mu}$ = ',num2str(muhat(i))]);\n",
    "    set(htitle,'Interpreter','Latex');\n",
    "    set(htitle,'fontsize',10)\n",
    "\n",
    "    ht = text(-15,0.06,num2str(g));\n",
    "    set(ht,'fontsize',10)\n",
    "    xlim([-20 20])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%plot -s 560,420\n",
    "\n",
    "% mean is unknown in this example\n",
    "% variance is known in this example\n",
    "\n",
    "m = 10;\n",
    "mu = 0;\n",
    "sigma = 5;\n",
    "\n",
    "x = normrnd(mu,sigma,1,m);\n",
    "\n",
    "%\n",
    "muhat = -10:0.5:10;\n",
    "G = [];\n",
    "\n",
    "for i = 1:length(muhat)    \n",
    "    y = normpdf(x,muhat(i),sigma);    \n",
    "    g = sum(log(y));\n",
    "    G = [G g];\n",
    "end\n",
    "\n",
    "plot(muhat,G,'.') \n",
    "\n",
    "xlim([-10.2 10.2])\n",
    "xlabel('$\\hat{\\mu}$','fontsize',10,'Interpreter','Latex')\n",
    "htitle = title('$log (\\prod{N(x|\\mu,\\sigma^2)})$');\n",
    "set(htitle,'Interpreter','Latex');\n",
    "set(htitle,'fontsize',10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare to a result from formula\n",
    "\n",
    "$$\\mu_{ML}=\\frac{1}{m}\\sum_{i = 1}^{m}x_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%plot -s 560,420\n",
    "% mean is known in this example\n",
    "% variance is unknown in this example\n",
    "\n",
    "m = 100;\n",
    "mu = 0;\n",
    "sigma = 3;\n",
    "\n",
    "x = normrnd(mu,sigma,1,m);  % samples\n",
    "\n",
    "sigmahat = 1:0.05:10;\n",
    "G = [];\n",
    "\n",
    "for i = 1:length(sigmahat)\n",
    "    y = normpdf(x,mu,sigmahat(i));      % likelihood    \n",
    "    g = sum(log(y));\n",
    "    G = [G g];\n",
    "end\n",
    "\n",
    "plot(sigmahat,G,'.')\n",
    "xlim([0 10.2])\n",
    "xlabel('$\\hat{\\sigma}$','fontsize',10,'Interpreter','Latex')\n",
    "htitle = title('$log (\\prod{N(x|\\mu,\\sigma^2)})$');\n",
    "set(htitle,'Interpreter','Latex');\n",
    "set(htitle,'fontsize',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqrt(var(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Linear Regression: A Probablistic View\n",
    "\n",
    "- Probabilistic Machine Learning\n",
    "    - I personally believe this is a more fundamental way of looking at ML\n",
    "\n",
    "\n",
    "- Linear regression model with (Gaussian) normal erros \n",
    "\n",
    "$$ \n",
    "\\begin{align*}\n",
    "y &=\\omega^T x + \\varepsilon, \\;\\; \\varepsilon \\sim N(0,\\sigma^2)\\\\\n",
    "y - \\omega^T x &= \\varepsilon \\sim N\\left(0,\\sigma^2\\right)\\\\ \\\\\n",
    "P \\left(y_i \\mid x_i;\\omega,\\sigma^2\\right) & =\\frac{1}{\\sqrt{2\\pi}\\sigma}\n",
    "\\exp \\left(-\\frac{1}{2\\sigma^2}\\left(y_i-\\omega^Tx_i\\right)^2 \\right) \\text{: generative model}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\\begin{align*} \n",
    "L = P\\left(y_1,y_2,\\cdots,y_m \\mid \\mu,\\sigma^2\\right)\n",
    "& =\\prod\\limits_{i=1}^m P\\left(y_i \\mid x_i; \\; \\omega,\\sigma^2\\right) \\\\\n",
    "& =\\frac{1}{\\left(\\sqrt{2\\pi}\\right)^m}\\frac{1}{\\sigma^m} \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum\\limits_{i=1}^m\\left(y_i-\\omega^T x_i\\right)^2\\right)=\\text{likelihood} \\\\\\\\\n",
    "\\ell &= -\\frac{m}{2}\\log{2\\pi}-m\\log{\\sigma}-\\frac{1}{2\\sigma^2}\\sum\\limits_{i=1}^m\\left(y_i-\\omega^T x_i\\right)^2\\\\\\\\\n",
    "\\frac{d\\ell}{d\\omega} &=-2X^TY+2X^TX\\omega=0\\;\n",
    "\\implies \\; \\omega_{ML}=\\left(X^TX\\right)^{-1}X^TY \n",
    "\\quad \\text{(look familiar ?)}\\\\\n",
    "\\frac{d\\ell}{d\\sigma}\n",
    "& =-\\frac{m}{\\sigma}+\\frac{1}{\\sigma^3}\\sum\\limits_{i=1}^m\\left(y_i-\\omega^T x_i\\right)^2=0  \n",
    "\\implies \\; \\sigma^2_{ML}=\\frac{1}{m}\\sum\\limits_{i=1}^m\\left(y_i-\\omega^T x_i\\right)^2\n",
    "\\end{align*}$$\n",
    "\n",
    "\n",
    "\n",
    "- BIG Lession\n",
    "    - same as the least squared optimization\n",
    "    \n",
    "$$\\begin{align*} \n",
    "\\text{loss function}\n",
    "& =\\sum\\limits_{i=1}^m\\left(y_i-\\omega^T x_i\\right)^2 \\\\\n",
    "& =\\|Y-X\\omega\\|^2_2 \\\\\n",
    "& =\\left(Y-X\\omega\\right)^T\\left(Y-X\\omega\\right)\\\\\n",
    "& =Y^TY-\\omega^TX^TY-Y^TX\\omega+\\omega^TX^TX\\omega\\\\ \\\\\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Numerical simulation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = 100;            % # of data points\n",
    "\n",
    "a = 1;              % slope\n",
    "x = 3 + 2*rand(m,1);\n",
    "e = 0.1*randn(m,1);\n",
    "\n",
    "y = a*x + e;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% compute theta(1) and theta(2) which are coefficients of y = theta(1)*x +\n",
    "% theta(2)\n",
    "A = [x ones(m,1)];\n",
    "theta = inv(A'*A)*A'*y;\n",
    "\n",
    "% to plot the fitted line\n",
    "xp = linspace(min(x),max(x),m)';\n",
    "yp = theta(1)*xp + theta(2);\n",
    "\n",
    "plot(x,y,'b.',xp,yp,'r');\n",
    "axis equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%plot -s 700,500\n",
    "\n",
    "yhat0 = theta(1)*x + theta(2);\n",
    "error0 = yhat0-y;\n",
    "\n",
    "yhat1 = 1.2*x - 1;\n",
    "error1 = yhat1-y;\n",
    "\n",
    "yhat2 = 1.3*x - 1;\n",
    "error2 = yhat2-y;\n",
    "\n",
    "subplot(2,3,1), plot(x,y,'b.',x,yhat0,'r'); \n",
    "subplot(2,3,2), plot(x,y,'b.',x,yhat1,'r');\n",
    "subplot(2,3,3), plot(x,y,'b.',x,yhat2,'r');\n",
    "subplot(2,3,4), hist(error0,21),    xlabel('\\epsilon')\n",
    "subplot(2,3,5), hist(error1,21),    xlabel('\\epsilon')\n",
    "subplot(2,3,6), hist(error2,21),    xlabel('\\epsilon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%plot -s 800,300\n",
    "a01 = error0(2:end);\n",
    "a02 = error0(1:end-1);\n",
    "\n",
    "a11 = error1(2:end);\n",
    "a12 = error1(1:end-1);\n",
    "\n",
    "a21 = error2(2:end);\n",
    "a22 = error2(1:end-1);\n",
    "\n",
    "subplot(1,3,1)\n",
    "plot(a01,a02,'.'),    axis equal,   axis([-0.7 0.7 -0.7 0.7]),   grid on\n",
    "xlabel('\\epsilon_i'),   ylabel('\\epsilon_{i-1}')\n",
    "\n",
    "subplot(1,3,2)\n",
    "plot(a11,a12,'.'),    axis equal,   axis([-0.7 0.7 -0.7 0.7]),   grid on\n",
    "xlabel('\\epsilon_i'),   ylabel('\\epsilon_{i-1}')\n",
    "\n",
    "subplot(1,3,3)\n",
    "plot(a21,a22,'.'),    axis equal,   axis([-0.7 0.7 -0.7 0.7]),   grid on\n",
    "xlabel('\\epsilon_i'),   ylabel('\\epsilon_{i-1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.4. Data Fusion with Uncertainties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- [Learning Theory (Reza Shadmehr, Johns Hopkins University)](http://www.shadmehrlab.org/Courses/learningtheory.html)\n",
    "    - [youtube](https://www.youtube.com/watch?v=52jlBrAcw9Q)\n",
    "\n",
    "<table style=\"border-style: hidden; border-collapse: collapse;\" width = \"70%\"> \n",
    "    <tr style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "        <td width = 35% style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "<img src=\"./image_files/ML003.png\" width=150>             \n",
    "        </td>\n",
    "        <td width = 35%>\n",
    "$$\\begin{align*} y_a &= x + \\varepsilon_a, \\; \\varepsilon_a \\sim N\\left(0,\\sigma^2_a\\right) \\\\\n",
    "y_b &= x + \\varepsilon_b, \\; \\varepsilon_b \\sim N\\left(0,\\sigma^2_b\\right) \\end{align*}$$\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "- in a matrix form\n",
    "\n",
    "$$ y = \\begin{bmatrix} y_a \\\\ y_b\\end{bmatrix} = Cx+\\varepsilon = \\begin{bmatrix}1\\\\1\\end{bmatrix}x+\\begin{bmatrix}\\varepsilon_a\\\\ \\varepsilon_b \\end{bmatrix} \\quad \\quad \\varepsilon \\sim N\\left(0,R\\right),\\;\\; R=\\begin{bmatrix} \\sigma^2_a & 0\\\\ 0 & \\sigma^2_b\\end{bmatrix}$$\n",
    "\n",
    "<br>\n",
    "$$\\begin{align*}P\\left(y \\mid x\\right) & \\sim N\\left(Cx,R\\right)\\\\\n",
    "                            &= \\frac{1}{\\sqrt{\\left(2\\pi\\right)^2\\vert R \\vert}}\\exp\\left(-\\frac{1}{2}\\left(y-Cx\\right)^TR^{-1}\\left(y-Cx\\right)\\right)\\end{align*}$$\n",
    "<br>\n",
    "\n",
    "- Find $\\,\\hat{x}_{ML}$\n",
    "\n",
    "$$\\ell = -\\log{2\\pi}-\\frac{1}{2}\\log{\\vert R \\vert}-\\frac{1}{2} \\underbrace{\\left(y-Cx\\right)^TR^{-1}\\left(y-Cx\\right)}$$\n",
    "<br><br>\n",
    "$$ \n",
    "\\begin{align*}\n",
    "\\left(y-Cx\\right)^TR^{-1}\\left(y-Cx\\right) &= y^TR^{-1}y-y^TR^{-1}Cx-x^TC^TR^{-1}y+x^TC^TR^{-1}Cx \\\\ \\\\\n",
    "\\implies \\frac{d\\ell}{dx} & =0=-2C^TR^{-1}y + 2C^TR^{-1}Cx\\\\\n",
    "\\therefore \\;\\; x_{ML} &=\\left(C^TR^{-1}C\\right)^{-1}C^TR^{-1}y \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    " \n",
    "- $ \\left(C^TR^{-1}C\\right)^{-1}C^TR^{-1} $\n",
    "\n",
    "$$ \n",
    "\\begin{align*}\n",
    "\\left(C^TR^{-1}C\\right) &= \\begin{bmatrix}1 & 1\\end{bmatrix}\\begin{bmatrix}\\frac{1}{\\sigma^2_a} & 0 \\\\ 0 & \\frac{1}{\\sigma^2_b}\\end{bmatrix}\\begin{bmatrix}1\\\\ 1\\end{bmatrix} = \\frac{1}{\\sigma^2_a}+\\frac{1}{\\sigma^2_b}\\\\ \n",
    "C^TR^{-1} &= \\begin{bmatrix}1 & 1\\end{bmatrix}\\begin{bmatrix}\\frac{1}{\\sigma^2_a} & 0 \\\\ 0&\\frac{1}{\\sigma^2_b}\\end{bmatrix}\n",
    "= \\begin{bmatrix}\\frac{1}{\\sigma^2_a} &\\frac{1}{\\sigma^2_b} \\end{bmatrix}\\\\ \\\\\n",
    "\\end{align*} $$\n",
    "\n",
    "$$\\begin{align*} \n",
    "\\implies \\; \\hat{x}_{ML} &= \\left(C^TR^{-1}C\\right)^{-1}C^TR^{-1}y = \\left(\\frac{1}{\\sigma^2_a}+\\frac{1}{\\sigma^2_b}\\right)^{-1} \\begin{bmatrix}\\frac{1}{\\sigma^2_a} &\\frac{1}{\\sigma^2_b} \\end{bmatrix} \\begin{bmatrix}y_a\\\\y_b\\end{bmatrix} = \\frac{\\frac{1}{\\sigma^2_a}y_a+\\frac{1}{\\sigma^2_b}y_b}{\\frac{1}{\\sigma^2_a}+\\frac{1}{\\sigma^2_b}}\\\\ \\\\\n",
    "\\text{var}\\left(\\hat{x}_{ML}\\right) &= \\left(C^TR^{-1}C \\right)^{-1}C^TR^{-1} \\cdot \\text{var}(y) \\cdot \\left( \\left(C^TR^{-1}C)^{-1}C^TR^{-1} \\right) \\right)^T\\\\\n",
    "& = \\left(C^TR^{-1}C \\right)^{-1}C^TR^{-1} \\cdot R \\cdot \\left( \\left(C^TR^{-1}C)^{-1}C^TR^{-1} \\right) \\right)^T\\\\\n",
    "&= \\left(C^TR^{-1}C \\right)^{-1}C^T \\cdot \\left(R^{-1} \\right)^TC \\left(\\left(C^TR^{-1}C \\right)^{-1} \\right)^T \\\\\n",
    "&=\\underbrace{\\left(C^TR^{-1}C\\right)^{-1}} \\underbrace{C^TR^{-1}C}\\left(\\left(C^TR^{-1}C\\right)^{-1}\\right)^T = \\left(C^TR^{-1}C\\right)^{-1}\\\\\n",
    "&=\\frac{1}{\\frac{1}{\\sigma^2_a}+\\frac{1}{\\sigma^2_b}} \\leq \\;\\; \\sigma^2_a,\\;\\sigma^2_b \\end{align*}$$\n",
    "\n",
    "- summary\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{x}_{ML} &= \\frac{\\frac{1}{\\sigma^2_a}y_a+\\frac{1}{\\sigma^2_b}y_b}{\\frac{1}{\\sigma^2_a}+\\frac{1}{\\sigma^2_b}}\\\\\n",
    "\\text{var}\\left(\\hat{x}_{ML}\\right) &= \\frac{1}{\\frac{1}{\\sigma^2_a}+\\frac{1}{\\sigma^2_b}} \\leq \\;\\; \\sigma^2_a,\\;\\sigma^2_b \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- BIG Lesson: \n",
    "    - two sensors are better than one sensor $\\implies$ less uncertainties\n",
    "    - accuracy or uncertainty information is also important in sensors\n",
    "\n",
    "<br>\n",
    "<img src=\"./image_files/ML004.png\", width = 700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Example of two rulers__\n",
    "- 1D example\n",
    "- how brain works on human measurements from both _haptic_ and _visual_ channels\n",
    "\n",
    "<img src=\"./image_files/length_estimation.png\", width = 300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%plot -s 560,520\n",
    "x = 5;      % true state (length in this example)\n",
    "a = 1;      % sigma of a\n",
    "b = 2;      % sigma of b\n",
    "\n",
    "YA = [];\n",
    "YB = [];\n",
    "XML = [];\n",
    "\n",
    "for i = 1:1000\n",
    "    ya = x + normrnd(0,a);\n",
    "    yb = x + normrnd(0,b);\n",
    "    xml = (1/a^2*ya + 1/b^2*yb)/(1/a^2+1/b^2);\n",
    "%     xml = (ya + yb)/2;      % not a good choice\n",
    "    YA = [YA ya];\n",
    "    YB = [YB yb];\n",
    "    XML = [XML xml];\n",
    "end\n",
    "\n",
    "subplot(3,1,1),  histogram(YA,31),   xlim([0 10]),  title('y_a','fontsize',10)\n",
    "subplot(3,1,2),  histogram(YB,31),   xlim([0 10]),  title('y_b','fontsize',10)\n",
    "subplot(3,1,3),  histogram(XML,31),  xlim([0 10]),  ylim([0 100]), title('x_{ML}','fontsize',10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Example of two GPSs__\n",
    "- 2D example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%plot -s 560,420\n",
    "x = [5;10]; % true position\n",
    "mu = [0;0];\n",
    "Ra = [9 1;\n",
    "      1 1];\n",
    "Rb = [1 1;\n",
    "      1 9];\n",
    "\n",
    "YA = [];\n",
    "YB = [];\n",
    "XML = [];\n",
    "\n",
    "for i = 1:1000\n",
    "    ya = x + mvnrnd(mu,Ra)';\n",
    "    yb = x + mvnrnd(mu,Rb)';\n",
    "    xml = inv(inv(Ra)+inv(Rb))*(inv(Ra)*ya + inv(Rb)*yb);\n",
    "%     xml = (ya + yb)/2;\n",
    "    YA = [YA ya];\n",
    "    YB = [YB yb];\n",
    "    XML = [XML xml];\n",
    "end\n",
    "\n",
    "plot(YA(1,:),YA(2,:),'b.'),  hold on\n",
    "plot(YB(1,:),YB(2,:),'r.'),\n",
    "plot(XML(1,:),XML(2,:),'k.'),   hold off\n",
    "axis equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Maximum-a-Posterior Estimation (MAP)\n",
    "- Choose $\\theta$ that maximizes the posterior probability of $\\theta$ (_i.e._ probability in the light of the observed data)\n",
    "\n",
    "- Posterior probability of $\\theta$ is given by the Bayes Rule\n",
    "$$P(\\theta \\mid D) = \\frac{P(D \\mid \\theta)P(\\theta)}{P(D)}$$\n",
    "    - $P(\\theta)$: Prior probability of $\\theta$ (without having seen any data)\n",
    "    - $P(D \\mid \\theta)$: Likelihood\n",
    "    - $P(D)$: Probability of the data (independent of $\\theta$)\n",
    "    \n",
    "$$ P(D) = \\int P(\\theta)P(D \\mid \\theta) d\\theta $$    \n",
    "\n",
    "- The Bayes Rule lets us update our belief about $\\theta$ in the light of observed data\n",
    "\n",
    "- While doing MAP, we usually maximize the <font color='blue'>log of the posterior probability</font>\n",
    "\n",
    "$$\\begin{align*} \\theta_{MAP} = \\underset{\\theta}{\\mathrm{argmax}}\\;\\;P(\\theta \\mid D) &= \\underset{\\theta}{\\mathrm{argmax}}\\;\\;\\frac{P(D \\mid \\theta)P(\\theta)}{P(D)}\\\\\n",
    "& =\\underset{\\theta}{\\mathrm{argmax}} \\;\\; P(D \\mid \\theta)P(\\theta)\\\\\n",
    "& = \\underset{\\theta}{\\mathrm{argmax}} \\;\\; \\log P(D \\mid \\theta)P(\\theta)\\\\\n",
    "& = \\underset{\\theta}{\\mathrm{argmax}} \\;\\; \\left\\{\\log{P \\left(D \\mid \\theta \\right)} + \\log{P(\\theta) }\\right\\}\\end{align*}$$\n",
    "\n",
    "- for multiple observations $D = \\{d_1,d_2,\\cdots,d_m\\}$\n",
    "\n",
    "$$ \\theta_{MAP} = \\underset{\\theta}{\\mathrm{argmax}} \\;\\; \\left\\{\\sum_{i=1}^{m}\\log{P \\left(d_i \\mid \\theta \\right)} + \\log{P(\\theta) }\\right\\} $$\n",
    "\n",
    "- same as MLE except the extra log-prior-distribution term\n",
    "\n",
    "- MAP allows incorporating our <font color='blue'>prior knowledge</font> about $\\theta$ in its estimation\n",
    "\n",
    "<br>\n",
    "<table width = \"60%\"> \n",
    "    <tr>\n",
    "        <td width = 30% >\n",
    "$$\\theta_{MAP} = \\underset{\\theta}{\\mathrm{argmax}}\\;\\;P(\\theta \\mid D)$$\n",
    "        </td>\n",
    "        <td width = 30%>\n",
    "$$ \\theta_{MLE} = \\underset{\\theta}{\\mathrm{argmax}}\\;\\;P(D \\mid \\theta) $$\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. MAP for mean of a univariate Gaussian, $N(\\theta,\\sigma^2)$\n",
    "\n",
    "Suppose that $\\theta$ is a random variable with $\\theta \\sim N(\\mu,1^2)$, but a prior knowledge (unknown $\\theta$ and known $\\mu, \\; \\sigma^2$)\n",
    "\n",
    "Observations $ D=\\{x_1,x_2,\\cdots,x_m\\}: \\text{conditionally independent given}\\; \\theta$\n",
    "\n",
    "$$ x_i \\sim N(\\theta,\\sigma^2) $$\n",
    "\n",
    "- Joint Probability\n",
    "$$ P(x_1,x_2,\\cdots,x_m \\mid \\theta) = \\prod\\limits_{i=1}^m P(x_i \\mid \\theta) $$\n",
    "\n",
    "- MAP: choose $\\theta_{MAP}$\n",
    "\n",
    "$$\\begin{align*} \\theta_{MAP} &= \\underset{\\theta}{\\mathrm{argmax}}\\;\\;P(\\theta \\mid D)=\\frac{P(D \\mid \\theta)P(\\theta)}{P(D)}\\\\\n",
    "& = \\underset{\\theta}{\\mathrm{argmax}} \\;\\; P(D \\mid \\theta)P(\\theta)\\\\\n",
    "& = \\underset{\\theta}{\\mathrm{argmax}} \\;\\; \\left\\{\\log{P \\left(D \\mid \\theta \\right)} + \\log{P(\\theta) }\\right\\}\\end{align*}$$\n",
    "\n",
    "\n",
    "$$\\begin{align*} \n",
    "\\frac{\\partial}{\\partial \\theta} \\left(\\log{P \\left(D \\mid \\theta \\right)} \\right)\n",
    "& =\\; \\cdots \\; = \\frac{1}{\\sigma^2} \\left(\\sum\\limits_{i=1}^m x_i-m\\theta \\right)\\quad (\\text{we did in MLE})\\\\\n",
    "\\frac{\\partial}{\\partial \\theta} \\left(\\log{P \\left(\\theta \\right)} \\right)\n",
    "&=\\frac{\\partial}{\\partial\\theta} \\left(\\log \\left(\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}(\\theta-\\mu)^2} \\right) \\right)\\\\\n",
    "& \\; \\vdots \\\\\n",
    "&=\\frac{\\partial}{\\partial\\theta} \\left(-\\frac{1}{2}\\log{2\\pi}-\\frac{1}{2} \\left(\\theta - \\mu \\right)^2 \\right)\\\\\n",
    "& =\\mu-\\theta \\\\ \\\\\n",
    "\\implies  \\frac{\\partial}{\\partial \\theta} \\left(\\log{P \\left(D \\mid \\theta \\right)} \\right) +  \\frac{\\partial}{\\partial \\theta} \\left(\\log{P \\left(\\theta \\right)} \\right)  & =  \\frac{1}{\\sigma^2}\\left(\\sum\\limits_{i=1}^m x_i - m\\theta^* \\right) + \\mu - \\theta^* = 0 \\\\\n",
    "& = \\frac{1}{\\sigma^2}\\sum\\limits_{i=1}^m x_i + \\mu - \\left(\\frac{m}{\\sigma^2}+1\\right)\\theta^* = 0 \\\\\n",
    "\\theta^* &= \\frac{\\frac{1}{\\sigma^2}\\sum\\limits_{i=1}^m x_i + \\mu}{\\frac{m}{\\sigma^2}+1} = \\frac{\\frac{m}{\\sigma^2}\\cdot\\frac{1}{m}\\sum\\limits_{i=1}^m x_i+1\\cdot\\mu}{\\frac{m}{\\sigma^2}+1}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "$$\\therefore \\;\\theta_{MAP} = \\frac{\\frac{m}{\\sigma^2}}{\\frac{m}{\\sigma^2}+1}\\bar{x}+\\frac{1}{\\frac{m}{\\sigma^2}+1}\\mu \\;\\;\\;\\text{: look familiar ?}$$\n",
    "\n",
    "\n",
    "- ML interpretation: \n",
    "\n",
    "$$\\begin{align*}\n",
    "&\\begin{cases}\n",
    "\\mu = \\text{prior mean}\\\\\n",
    "\\bar{x} = \\text{sample mean}\n",
    "\\end{cases} \\\\\n",
    "&\\begin{cases}\n",
    "\\mu = 1\\text{st} \\;\\;\\text{observation} \\; \\sim N\\left(0,1^2\\right)\\\\\n",
    "\\bar{x} = 2\\text{nd}\\;\\; \\text{observation} \\; \\sim N\\left(0,\\left(\\frac{\\sigma}{\\sqrt{m}}\\right)^2\\right)\n",
    "\\end{cases} \\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "- BIG Lesson: a prior acts as a data\n",
    "\n",
    "<img src=\"./image_files/map.png\" width = 350>\n",
    "\n",
    "__Note:__ prior knowledge\n",
    "- Education\n",
    "- Get older\n",
    "- School ranking\n",
    "\n",
    "Example) Experiment in class\n",
    "- Which one do you think is heavier?\n",
    "    - with eyes closed\n",
    "    - with visual inspection\n",
    "    - with haptic (touch) inspection\n",
    "\n",
    "<img src=\"./image_files/heavier.png\" width = 300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. MAP Matlab code\n",
    "\n",
    "- for mean of a univariate Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% theta\n",
    "mu = 5;\n",
    "theta = normrnd(mu,1)\n",
    "\n",
    "sigma = 2;\n",
    "x = normrnd(theta,sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% MAP\n",
    "\n",
    "m = 4;\n",
    "X = mvnrnd(theta,sigma,m);\n",
    "\n",
    "xbar = mean(X);\n",
    "theta_MAP = m/(m+sigma^2)*xbar + sigma^2/(m+sigma^2)*mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%plot -s 650,420\n",
    "% theta\n",
    "mu = 5;\n",
    "theta = normrnd(mu,1)\n",
    "\n",
    "sigma = 2;\n",
    "m = 1000;\n",
    "\n",
    "X = normrnd(theta,sigma,m,1);\n",
    "histogram(X,31), xlim([-5 15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 30;\n",
    "XMAP = [];\n",
    "for k = 1:n\n",
    "    xbar = mean(X(1:k));\n",
    "    xmap = k/(k + sigma^2)*xbar + sigma^2/(k + sigma^2)*mu;\n",
    "    XMAP = [XMAP xmap];\n",
    "end\n",
    "%X(1:n)'\n",
    "%XMAP\n",
    "\n",
    "k = 0;\n",
    "xmap = k/(k + sigma^2)*xbar + sigma^2/(k + sigma^2)*mu;\n",
    "XMAP = [xmap XMAP];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 30;\n",
    "XMLE = [];\n",
    "for k = 1:n\n",
    "    xmle = mean(X(1:k));    \n",
    "    XMLE = [XMLE xmle];\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(1:n,XMLE,'b',0:n,XMAP,'r')\n",
    "legend('MLE','MAP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Object Tracking in Computer Vision \n",
    "\n",
    "- Optional\n",
    "- Lecture: Introduction to Computer Vision by Prof. Aaron Bobick at Georgia Tech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<iframe width=\"560\" height=\"315\" \n",
    "src=\"https://www.youtube.com/embed/rf3DKqWajWY\" frameborder=\"0\" allowfullscreen>\n",
    "</iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<iframe width=\"560\" height=\"315\" \n",
    "src=\"https://www.youtube.com/embed/5yUjYCkm2jI\" frameborder=\"0\" allowfullscreen>\n",
    "</iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__iSystems tracking demo__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<iframe width=\"420\" height=\"315\" \n",
    "src=\"https://www.youtube.com/embed/_-niSgzEhSk\" frameborder=\"0\" allowfullscreen>\n",
    "</iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Robotics: Map-based Probabilistic Localization\n",
    "\n",
    "- Optional\n",
    "- Lecture: Autonomous Mobile Robots by Prof. Roland Siegwart at ETH Zurich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<iframe width=\"560\" height=\"315\" \n",
    "src=\"https://www.youtube.com/embed/y47jvldda-k\" \n",
    "frameborder=\"0\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Kernel Density Estimation\n",
    "- _non-parametric_ estimate of density\n",
    "- Lecture: Learning Theory (Reza Shadmehr, Johns Hopkins University) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<iframe width=\"560\" height=\"315\" \n",
    "src=\"https://www.youtube.com/embed/a357NoXy4Nk\" frameborder=\"0\" allowfullscreen>\n",
    "</iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%plot -s 560,420\n",
    "m = 20;\n",
    "mu = 0;\n",
    "sigma = 5;\n",
    "\n",
    "x = normrnd(mu,sigma,1,m);\n",
    "xp = linspace(-20,20,100);\n",
    "y0 = zeros(1,m);\n",
    "\n",
    "X = [];\n",
    "\n",
    "for i = 1:m\n",
    "    X(i,:) = normpdf(xp,x(i),sigma);\n",
    "end\n",
    "\n",
    "Xnorm = sum(X,1)/m;\n",
    "\n",
    "plot(x,y0,'kx','markerfacecolor','k'), hold on\n",
    "plot(xp,X,'b--')\n",
    "plot(xp,Xnorm,'r','linewidth',3), hold off\n",
    "ylim([-0.01 0.09])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
