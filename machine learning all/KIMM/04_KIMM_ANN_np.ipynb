{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<font size = '6'><b>Neural Networks</b></font>\n",
    "\n",
    "<table style=\"border-style: hidden; border-collapse: collapse;\" width = \"100%\"> \n",
    "    <tr style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "        <td width = 60% style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "             \n",
    "        </td>\n",
    "        <td width = 35%>\n",
    "        Collected by Prof. Seungchul Lee<br>\n",
    "        iSystems Design Lab.<br>\n",
    "        http://isystems.unist.ac.kr/<br>\n",
    "        UNIST\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Table of Contents\n",
    "<div id=\"toc\"></div>\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Neural Networks\n",
    "\n",
    "## 1.1. Recall supervised learning setup\n",
    "\n",
    "- Input features $x^{(i)} \\in \\mathbb{R}^n$\n",
    "\n",
    "- Ouput $y^{(i)}$\n",
    "\n",
    "- Model parameters $\\theta \\in \\mathbb{R}^k$\n",
    "\n",
    "- Hypothesis function $h_{\\theta}: \\mathbb{R}^n \\rightarrow y$\n",
    "\n",
    "- Loss function $\\ell: y \\times y \\rightarrow \\mathbb{R}_+$\n",
    "\n",
    "<br>\n",
    "- Machine learning optimization problem\n",
    "\n",
    "$$ \\min_{\\theta} \\sum_{i=1}^{m}\\ell\\left( h_{\\theta}\\left(x^{(i)}\\right),y^{(i)}\\right)$$\n",
    "\n",
    "$\\quad \\;$(possibly plus some additional regularization)\n",
    "\n",
    "- But, many specialized domains required highly engineered special features\n",
    "\n",
    "## 1.2. Neural networks\n",
    "\n",
    "Neural networks are a simply a machine learning algorithm with a more complex hypothesis class, directly incorporating non-linearity (in the parameters)\n",
    "\n",
    "<br>\n",
    "__Example__: neural network with one hidden layer\n",
    "\n",
    "$$h_{\\theta}(x) = \\Theta^{(2)} f\\left( \\Theta^{(1)}x\\right)$$\n",
    "\n",
    "where $\\Theta^{(1)} \\in \\mathbb{R}^{k \\times n}, \\Theta^{(2)} \\in \\mathbb{R}^{1 \\times k}$ and $f$ is some non-linear function applied elementwise to a vector\n",
    "\n",
    "<table style=\"border-style: hidden; border-collapse: collapse;\" width = \"96%\"> \n",
    "    <tr style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "        <td width = 28% style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "            <img src=\"./image_files/sigmoid_function.png\" width = 250>\n",
    "            $$ g(x) = \\frac{1}{1+e^{-x}}$$\n",
    "        </td>\n",
    "        <td width = 28% style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "            <img src=\"./image_files/tanh_function.png\" width = 250>\n",
    "            <br>\n",
    "            $$ g(x) = \\tanh (x)$$\n",
    "        </td>\n",
    "        <td width = 28% style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "            <img src=\"./image_files/relu_function.png\" width = 250>\n",
    "            <br>\n",
    "            $$ g(x) = \\max (0, x)$$\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Architectures are often shown graphically\n",
    "\n",
    "<img src=\"./image_files/nn_structure.png\" width = 270>\n",
    "\n",
    "- Middle layer $a$ is referred to as the hidden layer, there is nothing in the data that prescribes what values these should take, left up to the algorithm to decide\n",
    "\n",
    "- __Viewed another way: neural networks are like classifiers where the features themselves are also learned__\n",
    "\n",
    "\n",
    "- __Pros__\n",
    "    - No need to manually engineer good features, just let the neural networks handle this part\n",
    "\n",
    "- __Cons__\n",
    "\n",
    "    - Minimizing loss on training data is no longer a convex optimization problem in parameters $\\theta$\n",
    "\n",
    "    - Still need to engineer a good architecture \n",
    "\n",
    "## 1.3. Deep learning\n",
    "\n",
    "\"Deep\" neural networks typically refer to networks with multiple hidden layers\n",
    "\n",
    "<img src=\"./image_files/deep_structure.png\" width = 450>\n",
    "\n",
    "## 1.4. Machine learning and Neural networks (or Deep learning)\n",
    "\n",
    "__Machine Learning__\n",
    "- Hand-crafted features\n",
    "- Depends on expertise\n",
    "    \n",
    "__Deep Learning__\n",
    "- Automatic feature extraction\n",
    "- Depends on network structure\n",
    "\n",
    "<img src=\"./image_files/ML_DL.png\" width = 600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Structure of Neural Networks\n",
    "\n",
    "__The neuron__\n",
    "\n",
    "- The sigmoid equation is what is typically used as a transfer function between neurons. It is similar to the step fuction, but is continuous and differentiable.\n",
    "\n",
    "$$ \\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "- One useful property of this transfer function is the simplicity of computing its derivative.\n",
    "\n",
    "$$\\frac{d}{dx}\\sigma(x) = \\sigma' = \\sigma(x) (1-\\sigma(x))$$\n",
    "\n",
    "__Single input neuron__\n",
    "\n",
    "<img src=\"./image_files/single_neuron.png\" width = 300>\n",
    "\n",
    "$$ O = \\sigma(\\xi \\omega + \\theta) $$\n",
    "\n",
    "__Multiple input neuron__\n",
    "\n",
    "<img src=\"./image_files/multiple_neuron.png\" width = 300>\n",
    "\n",
    "$$ O = \\sigma(\\xi_1 \\omega_1 + \\xi_2 \\omega_2 + \\xi_3 \\omega_3 +\\theta) $$\n",
    "\n",
    "__A neural network__\n",
    "\n",
    "<img src=\"./image_files/nn_03.png\" width = 300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Learning: Backpropagation Algorithm (Optional)\n",
    "\n",
    "__Notation__\n",
    "\n",
    "- $x_j^\\ell$: Input to node $j$ of layer $\\ell$\n",
    "\n",
    "- $W_{ij}^\\ell$: Weight from layer $\\ell - 1$ node $i$ to layer $\\ell$ node $j$\n",
    "\n",
    "- $\\sigma(x) = \\frac{1}{1+e^{-x}}$: Sigmoid transfer function\n",
    "\n",
    "- $\\theta_j^{\\ell}$: Bias of node $j$ of layer $\\ell$\n",
    "\n",
    "- $O_j^{\\ell}$: Output of node $j$ in layer $\\ell$\n",
    "\n",
    "- $t_j$: Target value of node $j$ of the output layer\n",
    "\n",
    "<br>\n",
    "<font size='4'><b>The error calculation</b></font>\n",
    "\n",
    "Given a set of training data points $t_k$ and output layer output $O_k$ we can write the error as\n",
    "\n",
    "$$ E = \\frac{1}{2} \\sum_{k \\in K} (O_k - t_k)^2$$\n",
    "\n",
    "\n",
    "- Forward propagation \n",
    "    - the initial information propagates up to the hidden units at each layer and finally produces output\n",
    "- Backpropagation\n",
    "    - allows the information from the cost to flow backwards through the network in order to compute the gradients\n",
    "\n",
    "<img src=\"./image_files/animate_backpropa.gif\" width = 450>\n",
    "\n",
    "\n",
    "We want to calculate $\\frac{\\partial E}{\\partial W_{jk}^{\\ell}}$, the rate of change of the error with respect to the given connective weight, so we can minimize it.\n",
    "\n",
    "Now we consider two cases: the node is an output node, or it is in a hidden layer\n",
    "\n",
    "__1) Output layer node__\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial E}{\\partial W_{jk}} &= \\frac{\\partial}{\\partial W_{jk}} \\frac{1}{2} (O_k - t_k)^2 = (O_k - t_k)\\frac{\\partial}{\\partial W_{jk}} O_k = (O_k - t_k)\\frac{\\partial}{\\partial W_{jk}} \\sigma(x_k)\\\\\n",
    "&= (O_k - t_k) \\sigma(x_k) (1-\\sigma(x_k)) \\frac{\\partial}{\\partial W_{jk}} x_k \\\\\n",
    "&= (O_k - t_k) O_k (1 - O_k) O_j\n",
    "\\end{align*}\n",
    "\n",
    "$\\quad$For notation purposes, I will define $\\delta_k$ to be the expression $(O_k - t_k) O_k (1 - O_k)$, so we can rewrite the equation above as\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial W_{jk}} = O_j \\delta_k $$\n",
    "\n",
    "__2) Hidden layer node__\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial E}{\\partial W_{ij}} &= \\frac{\\partial}{\\partial W_{ij}} \\frac{1}{2} \\sum_{k \\in K} (O_k - t_k)^2 = \\sum_{k \\in K} (O_k - t_k)\\frac{\\partial}{\\partial W_{ij}} O_k = \\sum_{k \\in K} (O_k - t_k)\\frac{\\partial}{\\partial W_{ij}} \\sigma(x_k)\\\\\n",
    "&= \\sum_{k \\in K} (O_k - t_k) \\sigma(x_k) (1-\\sigma(x_k)) \\frac{\\partial}{\\partial W_{ij}} x_k \\\\\n",
    "&= \\sum_{k \\in K} (O_k - t_k) O_k (1 - O_k) \\frac{\\partial x_k}{\\partial O_j}\\cdot \\frac{\\partial O_j}{\\partial W_{ij}} = \\sum_{k \\in K} (O_k - t_k) O_k (1 - O_k) W_{jk}\\cdot \\frac{\\partial O_j}{\\partial W_{ij}}\\\\\n",
    "&= \\frac{\\partial O_j}{\\partial W_{ij}} \\cdot \\sum_{k \\in K} (O_k - t_k) O_k (1 - O_k) W_{jk}\\\\\n",
    "&= O_j (1-O_j)\\frac{\\partial x_j}{\\partial W_{ij}} \\cdot \\sum_{k \\in K} (O_k - t_k) O_k (1 - O_k) W_{jk}\\\\\n",
    "&= O_j (1-O_j)O_i \\cdot \\sum_{k \\in K} (O_k - t_k) O_k (1 - O_k) W_{jk}\\\\\n",
    "&= O_i O_j (1-O_j) \\sum_{k \\in K} \\delta_k W_{jk}\n",
    "\\end{align*}\n",
    "\n",
    "$\\quad$Similar to before we will now define all terms besides $O_i$ to be $\\delta_j = O_j (1-O_j) \\sum_{k \\in K} \\delta_k W_{jk}$, so we have\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial W_{ij}} = O_i \\delta_j$$\n",
    "\n",
    "\n",
    "__How weights affect errors__\n",
    "\n",
    "- For an output layer node $k \\in K$\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial W_{jk}} = O_j \\delta_k $$\n",
    "\n",
    "$\\quad \\;\\,$where $$\\delta_k = (O_k - t_k) O_k (1 - O_k)$$\n",
    "\n",
    "- For a hidden layer node $j \\in J$\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial W_{ij}} = O_i \\delta_j$$\n",
    "\n",
    "$\\quad \\;\\,$where $$\\delta_j = O_j (1-O_j) \\sum_{k \\in K} \\delta_k W_{jk}$$\n",
    "\n",
    "__What about the bias?__\n",
    "\n",
    "If we incorporate the bias term $\\theta$ into the equation you will find that\n",
    "\n",
    "$$ \\frac{\\partial O}{\\partial \\theta} = 1$$\n",
    "\n",
    "This is why we view the bias term as output from a node which is always one. This holds for any layer $\\ell$, a substitution into the previous equations gives us that\n",
    "\n",
    "$$ \\frac{\\partial E}{\\partial \\theta} = \\delta_{\\ell}$$\n",
    "\n",
    "__(Stochastic) Gradient Descent__\n",
    "\n",
    "- Negative gradients points directly downhill of cost function\n",
    "- We can decrease cost by moving in the direction of the negative gradient ($\\eta$ is a learning rate)\n",
    "\n",
    "$$ W:= W - \\eta \\nabla_{W} \\left( h_{W} \\left(x^{(i)}\\right),y^{(i)}\\right)$$\n",
    "\n",
    "<br>\n",
    "<table style=\"border-style: hidden; border-collapse: collapse;\" width = \"96%\"> \n",
    "    <tr style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "        <td width = 48% style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "            <img src=\"./image_files/GradientDescent.png\" width = 450>\n",
    "        </td>\n",
    "        <td width = 48% style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "            <img src=\"./image_files/GradientDescent_3d.png\" width = 450>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "<br>\n",
    "<font size='4'><b>The backpropagation algorithm using gradient descent</b></font>\n",
    "\n",
    "1. Run the network forward with your input data to get the netwrok output\n",
    "\n",
    "2. For each output node compute\n",
    "$$\\delta_k = (O_k - t_k) O_k (1 - O_k)$$\n",
    "3. For eatch hidden node calculate\n",
    "$$\\delta_j = O_j (1-O_j) \\sum_{k \\in K} \\delta_k W_{jk}$$\n",
    "4. Update the weights and biases as follows<br>\n",
    "Given\n",
    "$$\\begin{align*}\n",
    "\\Delta W &= -\\eta \\delta_{\\ell} O_{\\ell -1}\\\\\n",
    "\\Delta \\theta &= -\\eta \\delta_{\\ell}\n",
    "\\end{align*}$$\n",
    "apply\n",
    "$$\\begin{align*}\n",
    "W &\\leftarrow W + \\Delta W \\\\\n",
    "\\theta &\\leftarrow \\theta + \\Delta \\theta\n",
    "\\end{align*}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Implementation in Python and numpy (Optional)\n",
    "\n",
    "For the other neural network guides we will mostly rely on the excellent [TensorFlow](https://www.tensorflow.org/)'s optimizations and speed. However, to demonstrate the basics of neural networks, we'll use `numpy` so we can see exactly what's happening every step of the way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. A basic neural network with `numpy`\n",
    "\n",
    "First we'll import `numpy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With machine learning we are trying to find a hidden function that describes data that we have. Here we are going to cheat a little and define the function ourselves and then use that to generate data. Then we'll try to \"reverse engineer\" our data and see if we can recover our original function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  4.]\n",
      " [ 29.]\n",
      " [  9.]]\n"
     ]
    }
   ],
   "source": [
    "def unknown_function(X):\n",
    "    coeff = np.array([[2., -1., 5.]])\n",
    "    return np.dot(X, coeff.T)\n",
    "\n",
    "X = np.array([\n",
    "    [4.,9.,1.],\n",
    "    [2.,5.,6.],\n",
    "    [1.,8.,3.]\n",
    "])\n",
    "\n",
    "t = unknown_function(X) # target\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to set up our simple neural network. It will have just one hidden layer with two units (which we will refer to as unit 1 and unit 2).\n",
    "\n",
    "<img src=\"./image_files/simple_nn_structure.png\" width = 250>\n",
    "\n",
    "First we have to define the weights (i.e. parameters) of our network.\n",
    "\n",
    "- We have three inputs each going into two units, then one bias value for each unit, so we have eight parameters for the hidden layer.\n",
    "\n",
    "- Then we have the output of those two hidden layer units going to the output layer, which has only one unit - this gives us two more parameters, plus one bias value.\n",
    "\n",
    "- So in total, we have eleven parameters.\n",
    "\n",
    "Let's set them to arbitrary values for now (random initialization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initial hidden layer weights\n",
    "hidden_layer_weights = np.array([\n",
    "    [0.5, 0.5, 0.5],    # unit 1\n",
    "    [0.1, 0.1, 0.1]     # unit 2\n",
    "])\n",
    "hidden_layer_biases = np.array([1. ,1.])\n",
    "\n",
    "# initial output layer weights\n",
    "output_weights = np.array([[1., 1.]])\n",
    "output_biases = np.array([1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use $\\tanh$ activations for our hidden units, so let's define that real quick:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def activation(X):\n",
    "    return np.tanh(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\tanh$ activations are quite common, but you may also encounter sigmoid activations and, more recently, ReLU activations (which output 0 when $x \\leq 0$ and output $x$ otherwise). These activation functions have different benefits: \n",
    "- ReLUs in particular are robust against training difficulties that come when dealing with deeper networks.\n",
    "\n",
    "To make things clearer later on, we'll also define the linear function that combines a unit's input with its weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear(input, weights, biases):\n",
    "    return np.dot(input, weights.T) + biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do a forward pass with our inputs $X$ to see what the predicted outputs are.\n",
    "\n",
    "## 4.2. Forward pass\n",
    "\n",
    "First, we'll pass the input through the hidden layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden output\n",
      "[[ 0.99999977  0.98367486]\n",
      " [ 0.99999939  0.9800964 ]\n",
      " [ 0.99999834  0.97574313]]\n"
     ]
    }
   ],
   "source": [
    "hidden_linout = linear(X, hidden_layer_weights, hidden_layer_biases)\n",
    "hidden_output = activation(hidden_linout)\n",
    "\n",
    "print('hidden output')\n",
    "print(hidden_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(We're keeping the neuron unit's intermediary value, `hidden_linout` for use in backpropagation.)\n",
    "\n",
    "Then we'll take the hidden layer's output and pass it through the output layer to get our predicted outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted\n",
      "[[ 2.98367463]\n",
      " [ 2.98009578]\n",
      " [ 2.97574147]]\n"
     ]
    }
   ],
   "source": [
    "output_linout = linear(hidden_output, output_weights, output_biases)\n",
    "output_output = output_linout # no activation function on output layer\n",
    "\n",
    "predicted = output_output\n",
    "print('predicted')\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the mean squared error of our predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean squared error\n",
      "119.060003918\n"
     ]
    }
   ],
   "source": [
    "mse = np.mean((t - predicted)**2)/2\n",
    "print('mean squared error')\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take this error and backpropagate it through the network. This will tell us how to update our weights.\n",
    "\n",
    "## 4.3. Backpropagation\n",
    "\n",
    "Since backpropagation is essentially a chain of derivatives (that is used for gradient descent), we'll need the derivative of our activation function, so let's define that first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def activation_deriv(X):\n",
    "    return 1 - np.tanh(X)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we want to set a learning rate - this is a value from 0 to 1 which affects how large we tweak our parameters by for each training iteration.\n",
    "\n",
    "- You don't want to set this to be too large or else training will never converge (your parameters might get really big and you'll start seeing a lot of `nan` values).\n",
    "\n",
    "- You don't want to set this to be too small either, otherwise training will be very slow. There are more sophisticated forms of gradient descent that deal with this, but those are beyond the scope of this guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll propagate the error through the output layer (I won't go through the derivation of each step but they are straightforward to work out if you know a bit about derivatives):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# derivative of mean squared error\n",
    "error = predicted - t\n",
    "\n",
    "# delta for the output layer (no activation on output layer)\n",
    "delta_output = error\n",
    "\n",
    "# output layer updates\n",
    "output_weights_update = delta_output.T.dot(hidden_output)\n",
    "output_biases_update = delta_output.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then through the hidden layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# push back the delta to the hidden layer\n",
    "delta_hidden = delta_output*output_weights*activation_deriv(hidden_linout)\n",
    "\n",
    "# hidden layer updates\n",
    "hidden_weights_update = delta_hidden.T.dot(X)\n",
    "hidden_biases_update = delta_hidden.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can apply the updates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_weights -= output_weights_update*learning_rate\n",
    "output_biases -= output_biases_update*learning_rate\n",
    "\n",
    "hidden_layer_weights -= hidden_weights_update*learning_rate\n",
    "hidden_layer_biases -= hidden_biases_update*learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's one training iteration! In reality, you would do this many, many times - feedforward, backpropagate, update weights, then rinse and repeat. That's the basics of a neural network - at least, the \"vanilla\" kind. There are other more sophisticated kinds (recurrent and convolutional neural networks are two of the most common) that are covered in other guides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.98367463]\n",
      " [ 2.98009578]\n",
      " [ 2.97574147]]\n"
     ]
    }
   ],
   "source": [
    "print(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Neural networks in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  4.]\n",
      " [ 29.]\n",
      " [  9.]]\n",
      "[[  5.53872208]\n",
      " [ 26.5638112 ]\n",
      " [  7.78347079]]\n"
     ]
    }
   ],
   "source": [
    "# hidden layer weights\n",
    "hidden_layer_weights = np.array([\n",
    "    [0.5, 0.5, 0.5],    # unit 1\n",
    "    [0.1, 0.1, 0.1]     # unit 2\n",
    "])\n",
    "hidden_layer_biases = np.array([1. ,1.])\n",
    "\n",
    "# output layer weights\n",
    "output_weights = np.array([[1., 1.]])\n",
    "output_biases = np.array([1.])\n",
    "\n",
    "for i in range(1000):\n",
    "    \n",
    "    hidden_linout = linear(X, hidden_layer_weights, hidden_layer_biases)\n",
    "    hidden_output = activation(hidden_linout)\n",
    "    \n",
    "    output_linout = linear(hidden_output, output_weights, output_biases)\n",
    "    output_output = output_linout # no activation function on output layer\n",
    "\n",
    "    predicted = output_output\n",
    "\n",
    "    # derivative of mean squared error\n",
    "    error = predicted - t\n",
    "\n",
    "    # delta for the output layer (no activation on output layer)\n",
    "    delta_output = error\n",
    "\n",
    "    # output layer updates\n",
    "    output_weights_update = delta_output.T.dot(hidden_output)\n",
    "    output_biases_update = delta_output.sum(axis = 0)\n",
    "\n",
    "    # push back the delta to the hidden layer\n",
    "    delta_hidden = delta_output*output_weights*activation_deriv(hidden_linout)\n",
    "\n",
    "    # hidden layer updates\n",
    "    hidden_weights_update = delta_hidden.T.dot(X)\n",
    "    hidden_biases_update = delta_hidden.sum(axis = 0)\n",
    "\n",
    "    output_weights -= output_weights_update*learning_rate\n",
    "    output_biases -= output_biases_update*learning_rate\n",
    "\n",
    "    hidden_layer_weights -= hidden_weights_update*learning_rate\n",
    "    hidden_layer_biases -= hidden_biases_update*learning_rate\n",
    "\n",
    "print(t)\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.3.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
