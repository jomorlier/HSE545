{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='6'><b>Bayesian Methods</b></font><br><br>\n",
    "\n",
    "<table style=\"border-style: hidden; border-collapse: collapse;\" width = \"80%\"> \n",
    "    <tr style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "        <td width = 50% style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "\n",
    "        </td>\n",
    "        <td width = 30%>\n",
    "        David Rosenberg<br>New York University<br>April 22, 2015\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Table of Contents\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import *\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequentist of \"Classical\" Statistics\n",
    "\n",
    "- Probability model with parameter $\\theta \\in \\Theta$\n",
    "$$\\{ p(y;\\theta \\mid \\theta \\in \\Theta \\}, $$\n",
    "where $p(y;\\theta)$ is either a PDF or a PMF.\n",
    "\n",
    "- Assume that $p(y;\\theta)$ governs the world we are observing.\n",
    "- In **frequentist statistics**, the **parameter** $\\theta$ is a\n",
    "    - **fixed constant** (i.e. not random) and is\n",
    "    - **unknown** to us.\n",
    "- If we knew $\\theta$, there would be no need for statistics.\n",
    "- Instead of $\\theta$, we have a **sample** $\\mathcal{D} = \\{ y1, \\cdots ,y_n \\}$ i.i.d. $p(y;\\theta).$\n",
    "- Statistics is about how to use $\\mathcal{D}$ in place of $theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point Estimation\n",
    "\n",
    "- One type of statistical problem is **point estimation**.\n",
    "- A **statistic** $s = s(\\mathcal{D})$ is any function of the data.\n",
    "- A statistic $\\hat \\theta = \\hat \\theta (\\mathcal{D})$ is a **point estimator** if $\\hat \\theta \\approx \\theta$.\n",
    "- Desirable statistical properties of point estimators:\n",
    "    - **Consistency:** As data size $n \\rightarrow \\infty$, we get $\\hat \\theta \\rightarrow \\theta$.\n",
    "    - **Efficiency:** (Roughly speaking) For large $n, \\hat \\theta$ achieves accuracy at least as good as any other estimator.\n",
    "    - e.g. **maximum likelihood estimation** is consistent and efficient under reasonable conditions.\n",
    "- In frequentist statistics, you can make up any estimator you want.\n",
    "    - Justify its use by showing it has desirable properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Statistics\n",
    "\n",
    "- Major viewpoint change In **Baysian statistics:**\n",
    "    - parameter $\\theta \\in \\Theta$ is a **random variable**.\n",
    "- New ingredient: the prior distribution:\n",
    "    - a distribution on parameter space $\\Theta$.\n",
    "    - Reflects our belief about $\\theta$.\n",
    "    - Must be chosen before seeing any data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Bayesian Method\n",
    "\n",
    "- Define the modelL\n",
    "    - Choose a distribution $p(\\theta)$, called the **prior distribution**.\n",
    "    - Choose a probability model or \"**likelihood model**\", now written as:\n",
    "$$\\{ p(y \\mid \\theta) \\mid \\theta \\in \\Theta \\}.$$\n",
    "- After observing $\\mathcal{D}$, compute the **posterior distribution** $p(\\theta \\mid\\mathcal{D})$.\n",
    "- Decide the **action** based on $p(\\theta \\mid \\mathcal{D})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Posterior Distribution\n",
    "\n",
    "- By Bayes rule, can write the posterior distribution as\n",
    "$$p(\\theta \\mid \\mathcal{D}) = \\frac{p(\\mathcal{D} \\mid \\theta) \\ p(\\theta)}{p(\\mathcal{D})}$$\n",
    "- **likelihood:** $p(\\theta)$\n",
    "- **prior:** $p(\\theta)$\n",
    "- **marginal likelihood:** $p(\\mathcal{D})$.\n",
    "- Note: $p(\\mathcal{D})$ is just a normalizaion constant for $p(\\theta \\mid \\mathcal{D})$. Can write\n",
    "$$p(\\theta \\mid \\mathcal{D}) \\sim p(\\mathcal{D} \\mid \\theta) \\ p(\\theta).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap and Interpretation\n",
    "- Prior represents belief about $\\theta$ before observing data $\\mathcal{D}$.\n",
    "- Posterior represents the rationally “updated” beliefs after seeing $\\mathcal{D}$.\n",
    "- All inferences and action-taking are based on the posterior distribution.\n",
    "- In the Bayesian approach,\n",
    "    - No issue of “choosing a procedure” or justifying an estimator.\n",
    "    - Only choices are the **prior** and the **likelihood model**.\n",
    "    - For decision making, need a **loss function**.\n",
    "    - Everything after that is **computation**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Coin Flipping\n",
    "- Suppose we have a coin, possibly biased\n",
    "$$\\mathbb{P}(Heads \\mid \\theta) = \\theta.$$\n",
    "- **Parameter space** $\\theta \\in \\Theta = [0, 1]$\n",
    "- **Prior distribution:** $\\theta \\sim Beta(\\theta \\mid 2, 2).$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEdCAYAAADjFntmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBRERqaSiICIilf4fglwY5KdZJCIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x169d8eea7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def Beta(theta, a, b):\n",
    "    val = gamma(a+b)/(gamma(a)*gamma(b)) * theta**(a-1) * (1-theta)**(b-1)\n",
    "    return val\n",
    "\n",
    "plt.plot(np.arange(0, 1, 0.01),\n",
    "         [Beta(theta, 2, 2) for theta in np.arange(0, 1, 0.01)], 'r')\n",
    "plt.grid(True)\n",
    "plt.axis('tight')\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel(r'$p(\\theta)$')\n",
    "plt.title(r'$Prior: Beta(\\mu \\mid 2,2)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next, we gather some data $\\mathcal{D} = \\{H,H,T,T,T,T,T,H,\\cdots,T\\} :$\n",
    "- Heads: 75 $\\quad$ Tails: 60\n",
    "    - $\\hat \\theta_{MLE} = \\frac{75}{75+60} \\approx 0.556$\n",
    "- **Posterior distribution:** $\\theta \\mid \\mathcal{D} \\sim Beta(\\theta \\mid 77, 62):$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEdCAYAAADtk8dMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIjVAxV5EpAao2IuI1AAVexGRGvD/tDK3yQIboSMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x169d8b443c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(0, 1, 0.01),\n",
    "         [Beta(theta, 77, 62) for theta in np.arange(0, 1, 0.01)], 'r')\n",
    "plt.grid(True)\n",
    "plt.axis('tight')\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel(r'$p(\\theta)$')\n",
    "plt.title(r'$Posterior: Beta(\\mu \\mid 77,62)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What to do with the Posterior Distribution?\n",
    "\n",
    "- Look at it.\n",
    "- xtract a point estimate of $\\theta$ (e.g. mean or mode of posterior).\n",
    "- Extract \"credible set\" for $\\theta$ (a Bayesian confidence interval).\n",
    "    - e.g. Interval $[a,b]$ is a 95% **credible set** if\n",
    "$$\\mathbb{P} (\\theta \\in [a,b] \\mid D) \\geqslant 0.95$$\n",
    "- The most “Bayesian” approach is **Bayesian decision theory:**\n",
    "    - Choose a loss function.\n",
    "    - Find action minimizing \"posterior risk\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Decision Theory\n",
    "\n",
    "- Ingredients:\n",
    "    - **Action space** $\\mathcal{A}$\n",
    "    - **Parameter space** $\\Theta$\n",
    "    - **Loss function:** $\\mathcal{l}:\\mathcal{A}\\times \\Theta \\rightarrow \\mathbf{R}$\n",
    "    - **Prior:** Distribution $p(\\theta)$ on $\\Theta$.\n",
    "- The **posterior risk** of an action $a \\in \\mathcal{A}$ is\n",
    "    $$\\begin{array}{ll} r(a) & := & \\mathbb{E}[l(\\theta, a) \\mid \\mathcal{D}] \\\\ & = & \\int l(\\theta, a) \\ p(\\theta \\mid \\mathcal{D}) \\ d\\theta \\end{array} $$\n",
    "    - It's the **expected loss under the posterior**.\n",
    "- A **Bayes action** $a^*$ is an action that minimizes posterior risk:\n",
    "$$r(a^*) = \\min_{a \\in \\mathcal{A}} r(a) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Point Estimation\n",
    "\n",
    "- General Setup:\n",
    "    - Data $\\mathcal{D}$ generated by $p(y \\mid \\theta)$, for unknown $\\theta \\in \\Theta$.\n",
    "    - Want to produce a **point estimate** for $\\theta$.\n",
    "- Choose the following:\n",
    "    - **Loss** $l(\\hat \\theta, \\theta) = \\left( \\theta - \\hat \\theta \\right)^2$\n",
    "    - **Prior** $p(\\theta)$ on $\\Theta$.\n",
    "- Find **action** $\\hat \\theta \\in \\Theta$ that minimizes **posterior risk:**\n",
    "$$\\begin{array}{ll} r(\\hat \\theta) & = & \\mathbb{E} \\left[ \\left( \\theta - \\hat \\theta \\right]^2 \\mid \\mathcal{D} \\right] \\\\ & = & \\int \\left(\\theta - \\hat \\theta \\right)^2 \\ p(\\theta \\mid \\mathcal{D}) \\ d\\theta \\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Point Estimation: Square Loss\n",
    "\n",
    "- Find **action** $\\hat \\in \\Theta$ that minimizes **posterior risk**\n",
    "$$r(\\hat \\theta) = \\int \\left( \\theta - \\hat \\theta \\right)^2 \\ p(\\theta \\mid \\mathcal{D} \\ d\\theta$$\n",
    "- Differentiate:\n",
    "$$\\begin{array}{ll} {dr(\\hat \\theta) \\over d\\hat \\theta} & = & - \\int 2\\left( \\theta - \\hat \\theta \\right) \\ p(\\theta \\mid \\mathcal{D}) \\ d\\theta \\\\ & = & -2 \\int \\theta p(\\theta \\mid \\mathcal{D}) \\ d\\theta + 2\\hat \\theta \\int p(\\theta \\mid \\mathcal{D}) \\ d\\theta \\\\ & = & -2 \\int \\theta p(\\theta \\mid \\mathcal{D}) \\ d\\theta \\end{array} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Point Estimation: Square Loss\n",
    "\n",
    "- Derivative of posterior risk is\n",
    "$${dr(\\hat \\theta) \\over d\\hat \\theta} = -2\\int \\theta \\ p(\\theta \\mid \\mathcal{D}) \\ d\\theta + 2\\hat \\theta.$$\n",
    "- First order condition ${dr(\\hat \\theta) \\over d\\hat \\theta} = 0$ gives\n",
    "$$\\begin{array}{ll} \\hat \\theta & = & \\int \\theta \\ p(\\theta \\mid \\mathcal{D}) \\ d\\theta \\\\ & = & \\mathbb{E} [\\theta \\mid \\mathcal{D}] \\end{array} $$\n",
    "- **Bayes action** for **square loss** is the posterior mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Point Estimation: Absolute Loss\n",
    "\n",
    "- **Loss:** $l(\\theta,\\hat \\theta) = \\left| \\ \\theta - \\hat \\theta \\ \\right|$\n",
    "- **Bayes action** for **absolute loss** is the **posterior median.**\n",
    "    - That is, the median of the distribution $p(\\theta \\mid \\mathcal{D})$.\n",
    "    - Show with approach similar to what was used in Homework #1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Point Estimation: Zero-One Loss\n",
    "\n",
    "- Suppose $\\Theta$ is discrete (e.g. $\\Theta = \\{ english, french \\}$)\n",
    "- **Zero-one loss:** $l(\\theta, \\hat \\theta) = \\mathbf{1}(\\theta \\ne \\hat \\theta )$\n",
    "- **Posterior risk:**\n",
    "$$\\begin{array}{ll} r(\\hat \\theta) & = & \\mathbb{E} \\left[ \\mathbf{1} (\\theta \\ne \\hat \\theta ) \\mid \\mathcal{D} \\right] \\\\ & = & \\mathbb{P} \\left( \\theta \\ne \\hat \\theta \\mid \\mathcal{D} \\right ) \\\\ & = & \\mathbf{1} - \\mathbb{P} \\left( \\theta = \\hat \\theta \\mid \\mathcal{D} \\right) \\\\ & = & \\mathbf{1} - p(\\hat \\theta \\mid \\mathcal{D}) \\end{array} $$\n",
    "- **Bayes action** is\n",
    "$$ \\hat \\theta = \\arg \\max_{\\theta \\in \\Theta} \\ p(\\theta \\mid \\mathcal{D} )$$\n",
    "- This $\\hat \\theta$ is called the **maximum a posteriori (MAP)** estimate.\n",
    "- The MAP estimate is the mode of the posterior distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Point Estimation: Custom Loss Function\n",
    "\n",
    "- Suppose $\\Theta$ is discrete (e.g. $\\Theta = \\{english, french\\}$)\n",
    "- **Loss function $l(\\hat \\theta, \\theta)$:**\n",
    "$$\\begin{array}{ll} l(french, english) & = & 10 \\\\ l(english, french) & = & 1 \\\\ l(english, english) & = & 0 \\\\ l(french, french) & = & 0 \\end{array}$$\n",
    "- **Posterior risk:**\n",
    "$$\\begin{array}{ll} r(french) & = & 10p(english \\mid \\mathcal{D}) + 0p(french \\mid \\mathcal{D}) \\\\ r(english) & = & 1p(french \\mid \\mathcal{D}) + 0p(english \\mid \\mathcal{D}) \\end{array}$$\n",
    "- **Bayes action** is french iff $r(french) > r(english)$, i.e. when\n",
    "$${p(english \\mid \\mathcal{D}) \\over p(french \\mid \\mathcal{D})} > {1 \\over 10}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Conditional Models\n",
    "\n",
    "- Input space $\\mathcal{X} = \\mathbf{R}^d \\quad$ Output space $\\mathcal{Y} = \\mathbf{R}$\n",
    "- **Conditional probability model**, or **likelihood model:**\n",
    "$$\\{ p(y \\mid  x, \\theta) \\mid \\theta \\in \\Theta \\}$$\n",
    "    - Conditional here refers to the conditioning on the input $x$.\n",
    "    - Means that $x$'s are known and not governed by our probability model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Regression Model\n",
    "\n",
    "- Input space $\\mathcal{X} = \\mathbf{R}^d \\quad$ Output space $\\mathcal{Y} = \\mathbf{R}$\n",
    "- **Conditional probability model**, or **likelihood model:**\n",
    "$$y \\mid x, \\theta \\sim \\mathcal{N} (\\theta^T x, \\sigma^2),$$\n",
    "for some known $\\sigma^2 > 0$\n",
    "- **Parameter space** $\\Theta = \\mathbf{R}^d$\n",
    "- **Data:** $mathcal{D} = \\{ (x_1, y_1), \\cdots, (x_n, y_n)\\}$\n",
    "    - Write $y = (y_1, \\cdots, y_n)$ and $x = (x_1, \\cdots, x_n)$.\n",
    "    - Assume $y_i$'s are **conditionally independent**, given $x$ and $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Gaussian Likelihood\n",
    "\n",
    "- The ** likelihood** of $\\theta \\in \\Theta$ for the data $\\mathcal{D}$ is\n",
    "$$\\begin{array}{ll} p(y \\mid x, \\theta) & = & \\prod_{i=1}^n p(y_i \\mid x_i, \\theta) \\quad \\text{by conditional independence.} \\\\ & = & \\prod_{i=1}^n \\left[ {1 \\over \\sigma \\sqrt{2\\pi}} \\exp{\\left( -{(y_i - \\theta^T x_i)^2 \\over 2\\sigma^2}\\right) } \\right] \\end{array}$$\n",
    "- Recall from the GLM lecture$^1$ that the **MLE** is\n",
    "$$\\begin{array}{ll} \\theta_{MLE}^* & = & \\arg \\max_{\\theta \\in \\mathbf{R}^d} p(y \\mid x, \\theta) \\\\ & = & \\arg \\min_{\\theta \\in \\mathbf{R}^d} \\sum_{i=1}^n (y_i - \\theta^T x_i)^2 \\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Priors and Posteriors\n",
    "\n",
    "- Choose a Gaussian prior distribution p(θ) on Θ:\n",
    "$$\\Theta \\sim \\mathcal{N}(0,\\Sigma_0)$$\n",
    "for some **covariance matrix** $\\Sigma_0 \\succ 0$ (i.e. $\\Sigma_0$ is spd).\n",
    "- **Posterior distribution**\n",
    "$$\\begin{array}{ll} p(\\theta \\mid \\mathcal{D}) & = & p(\\theta \\mid x, y) \\\\\n",
    "& = & p(y \\mid x, \\theta) \\ p(\\theta) / p(y) \\\\\n",
    "& \\propto & \\prod_{i=1}^n \\left[{1 \\over \\sigma \\sqrt{2\\pi}} \\exp{\\left( -{(y_i - \\theta^T x_i)^2 \\over 2 \\sigma^2}\\right)} \\right] \\ \\text{(likelihood)} \\\\\n",
    "& & \\ \\times \\left\\vert 2\\pi\\Sigma_0 \\right\\vert^{-1/2} \\exp{\\left( -{1 \\over 2} \\theta^T \\Sigma_0^{-1} \\theta \\right)} \\ \\text{(prior)}\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example in 1-Dimension\n",
    "\n",
    "- Input space $\\mathcal{X} = [-1, 1] \\quad$ Output space $\\mathcal{Y} = \\mathbf{R}$\n",
    "- Basic Gaussian regression model:\n",
    "$$y = \\omega_0 + \\omega_1 x + \\epsilon,$$\n",
    "where $\\epsilon \\sim \\mathcal{N}(0,0.2^2)$.\n",
    "- Written another way, the **likelihood model** is\n",
    "$$y \\mid x, \\theta = (\\omega_0, \\omega_1) \\sim \\mathcal{N}(\\omega_0 + \\omega_1 x, 0.2^2).$$\n",
    "- **Prior distribution:** $\\theta = (\\omega_0, \\omega_1) \\sim \\mathcal{N}(0, {1 \\over 2}I)$\n",
    "<img src = \"./image_files/prior_posterior1.png\", width = 300>\n",
    "- On right, plots of $y = \\omega_0 + \\omega_1 x$ for random $(\\omega_0, \\omega_1) \\sim p(\\theta) = \\mathcal{N}(0, {1 \\over 2}I).$\n",
    "- Consider $y$ and $x$ related as $y = \\omega_0 + \\omega_1 x + \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0, 0.2^2)$.\n",
    "- **Conditional probability model**, or **likelihood model:**\n",
    "$$y \\mid x, \\theta = (\\omega_0, \\omega_1) \\sim \\mathcal{N}(\\omega_0 + \\omega_1 x 0.2^2).$$\n",
    "- **Prior distribution:** $\\theta = (\\omega_0, \\omega_1) \\sim \\mathcal{N}(0, {1 \\over 2}I)$\n",
    "<img src = \"./image_files/prior_posterior1.png\", width = 300>\n",
    "- On right, plots of $y = \\omega_0 + \\omega_1 x$ for random $(\\omega_0, \\omega_1) \\sim p(\\theta) = \\mathcal{N}(0, {1 \\over 2}I).$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example in 1-Dimension: 1 Observation\n",
    "\n",
    "<img src = \"./image_files/prior_posterior2.png\", width = 300>\n",
    "- On left, the white cross indicates the true parameter values.\n",
    "- On right, the blue circle indicates the training observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example in 1-Dimension: 2 and 20 Observations\n",
    "\n",
    "<img src = \"./image_files/prior_posterior3.png\", width = 300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Distribution\n",
    "\n",
    "- Given a new input point $x_{new}$, how to predict $y_{new}$?\n",
    "- **Predictive distribution**\n",
    "$$\\begin{array}{ll} & & p(y_{new} \\mid x_{new}, \\mathcal{D}) \\\\\n",
    "& = & \\int p(y_{new} \\mid x_{new}, \\theta, \\mathcal{D}) \\ p(\\theta \\mid \\mathcal{D}) \\ d\\theta \\\\\n",
    "& = & \\int p(y_{new} \\mid x_{new}, \\theta) \\ p(\\theta \\mid \\mathcal{D}) \\ d\\theta \\\\\n",
    "\\end{array}$$\n",
    "- For Gaussian regression, posterior and predictive distributions have closed forms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closed Form for Posterior\n",
    "\n",
    "- Model:\n",
    "$$\\begin{array}{ll} \\theta & \\sim & \\mathcal{N}(0, \\Sigma_0) \\\\\n",
    "y_i \\mid x, \\theta & i.i.d. & \\mathcal{N}(\\theta^T x_i, \\sigma^2) \\end{array}$$\n",
    "- Design matrix $X \\quad$ Response column vector $y$\n",
    "- **Posterior distribution is a Gaussian distribution:**\n",
    "$$\\begin{array}{ll} \\theta \\mid \\mathcal{D} & \\sim & \\mathcal{N}(\\mu_P, \\Sigma_P) \\\\\n",
    "\\Sigma_P & = & (\\sigma^{-2} X^T X + \\Sigma_0^{-1})^{-1} \\\\\n",
    "\\mu_P & = & \\sigma^{-2} \\sigma_P X^T y\n",
    "\\end{array}$$\n",
    "- **Posterior Variance** $\\Sigma_P$ gives us a natural **uncertainty measure.**\n",
    "- **Posterior distribution is a Gaussian distribution:**\n",
    "$$\\begin{array}{ll}\n",
    "\\theta \\mid \\mathcal{D} & \\sim & \\mathcal{N}(\\mu_P, \\Sigma_P) \\\\\n",
    "\\Sigma_P & = & (\\sigma^{-2} X^T X + \\Sigma_0^{-1})^{-1} \\\\\n",
    "\\mu_P & = & \\sigma^{-2} \\Sigma_P X^T y\n",
    "\\end{array}$$\n",
    "- Look familiar?\n",
    "- For the prior variance $\\Sigma_0 = {\\sigma^2 \\over \\lambda}l$, we get\n",
    "$$\\mu_P = (X^T X + \\lambda I)^{-1} X^T y,$$\n",
    "which is of course the ridge regression solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Posterior Mean and Posterior Mode (MAP)\n",
    "\n",
    "- **Posterior density for** $\\Sigma_0 = {\\sigma^2 \\over \\lambda}I$:\n",
    "$$p(\\theta \\mid \\mathcal{D}) \\propto \\exp{\\left( - {\\lambda \\over 2\\sigma^2} \\lVert \\theta \\rVert^2 \\right)} \\prod_{i=1}^n \\exp{\\left( - {(y_i - \\theta^T x_i)^2 \\over 2\\sigma^2}\\right)}$$\n",
    "- **To find MAP, sufficient to minimize the log posterior:**\n",
    "$$\\begin{array}{ll}\n",
    "\\hat \\theta_{MAP} & = & \\arg \\min_{\\theta \\in \\mathbf{R}^d} [-\\log{p(\\theta \\mid \\mathcal{D}}] \\\\\n",
    "& = & \\arg \\min_{\\theta \\in \\mathbf{R}^d} \\prod_{i=1}^n (y_i - \\theta^T x_i)^2 + \\lambda \\lVert \\theta \\rVert^2\n",
    "\\end{array}$$\n",
    "- Which is the ridge regression objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closed Form for Predictive Distribution\n",
    "\n",
    "- **Model:**\n",
    "$$\\begin{array}{ll}\n",
    "\\theta & \\sim & \\mathcal{N}(0, \\Sigma_0) \\\\\n",
    "y_i \\mid x, \\theta & i.i.d. & \\mathcal{N}(\\theta^T x_i, \\sigma^2)\n",
    "\\end{array}$$\n",
    "- **Predictive Distribution**\n",
    "$$p(y_{new} \\mid x_{new}, \\mathcal{D}) = \\int p(y_{new} \\mid x_{new}, \\theta) \\ p(\\theta \\mid \\mathcal{D}) \\ d\\theta$$\n",
    "- **Closed form:**\n",
    "$$\\begin{array}{ll}\n",
    "y_{new} \\mid x_{new}, \\mathcal{D} & \\sim & \\mathcal{N}(\\nu_{new}, \\sigma_{new} \\\\\n",
    "\\mu_{new} & = & \\mu_P^T x_{new} \\\\\n",
    "\\sigma_{new} & = & x_{new}^T \\Sigma_P x_{new} + \\sigma^2\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Distributions\n",
    "\n",
    "- With predictive distributions, can draw error bands:\n",
    "<img src = \"./image_files/predictive.png\", width = 300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Predictive Distributions vs GLMs\n",
    "- Gaussian regression with MLE, from our GLM lecture:\n",
    "    - produces a Gaussian for each input $x$.\n",
    "$$x \\mapsto \\mathcal{N}(x^T \\theta_{MLE}, \\sigma^2)$$\n",
    "- Bayesian predictive distributions:\n",
    "    - produce a Gaussian for each input $x$\n",
    "$$x \\mapsto \\mathcal{N} \\left( \\theta_{ridge}^T x, x_{new}^T \\Sigma_P x_{new} + \\sigma^2 \\right)$$\n",
    "- In Bayesian version\n",
    "    - equivalent to using a regularized least squares fit\n",
    "    - variance has additional piece from uncertainty in $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coin Flipping\n",
    "\n",
    "- **Parameter space $\\theta \\in \\Theta = [0, 1]$:**\n",
    "$$\\mathbb{P}(Heads \\mid \\theta) = \\theta.$$\n",
    "- Data $\\mathcal{D} = = {H,H,T,T,T,T,T,H,\\cdots,T}$\n",
    "    - $n_h$: number of heads\n",
    "    - $n_t$: number of tails\n",
    "- **Conditional Independence Assumption:**\n",
    "    - Conditioned on $\\theta$, repeated flips are independent\n",
    "- **Likelihood model** (Bernoulli Distribution):\n",
    "$$p(\\mathcal{D} \\mid \\theta) = \\theta^{n_h}(1 - \\theta)^{n_t}$$\n",
    "    - (probability of getting the flips in the order they were received)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coin Flipping: Beta Prior\n",
    "\n",
    "- **Prior:**\n",
    "$$\\begin{array}{ll}\n",
    "\\theta \\sim Beta(h,t)\n",
    "p(\\theta) \\propto \\theta^{h-1}(1-\\theta)^{t-1}\n",
    "\\end{array}\n",
    "$$\n",
    "- **Mean of Beta distribution:**\n",
    "$$\\mathbb{E} \\theta = {h \\over h + t}$$\n",
    "- Interpret h and t as the number of heads/tails received in a prior experiment.\n",
    "    - Then $\\mathbb{E}\\theta$ is the obvious MLE and plug-in estimate for $\\theta$.\n",
    "- For fixed $\\mathbb{E}\\theta$, $Var(\\theta)$ decreases as number of flips $n = h + t$ grows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coin Flipping: Posterior\n",
    "\n",
    "- **Prior:**\n",
    "$$\\begin{array}{ll}\n",
    "\\theta & \\sim & Beta(h, t) \\\\\n",
    "p(\\theta) & \\propto & \\theta^{h-1}(1-\\theta)^{h-1}\n",
    "\\end{array}\n",
    "$$\n",
    "- **Likelibood model:**\n",
    "$$p(\\mathcal{D} \\mid \\theta) = \\theta^{n_h} (1-\\theta)^{n_t}$$\n",
    "- **Posterior density:**\n",
    "$$\\begin{array}{ll}\n",
    "p(\\theta \\mid \\mathcal{D}) & \\propto & p(\\theta) \\ p(\\mathcal{D} \\mid \\theta) \\\\\n",
    "& \\propto & \\theta^{h-1} (1-\\theta)^{t-1} \\times \\theta^{n_h} (1-\\theta)^{n_t} \\\\\n",
    "& = & \\theta^{h-1+n_h} (1-\\theta)^{t-1+n_t}\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Posterior is Beta\n",
    "\n",
    "- **Prior**\n",
    "$$\\begin{array}{ll}\n",
    "\\theta & \\sim & Beta(h, t) \\\\\n",
    "p(\\theta) & \\propto & \\theta^{h-1} (1-\\theta)^{h-1}\n",
    "\\end{array}$$\n",
    "- **Posterior density:**\n",
    "$$p(\\theta \\mid \\mathcal{D}) \\propto \\theta^{h-1+n_h} (1-\\theta)^{t-1+n_t}$$\n",
    "- **So**\n",
    "$$\\theta \\mid \\mathcal{D} \\sim Beta(h+n_h, t+n_t)$$\n",
    "- It’s as though we continued our experiment by adding more flips."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conjugate Prior Examples\n",
    "\n",
    "- A prior is conjugate for a likelihood model if the posterior is in the same “family” as the prior.\n",
    "    - If prior is a beta distribution, and likelihood model is a Bernoulli distribution, then posterior is a beta distribution.\n",
    "        - Prior and posterior in the same family ⇒ **Beta is a conjugate prior for Bernoulli**\n",
    "    - If prior is a Gaussian distribution, and likelihood model is a Gaussian distribution, then posterior is a Gaussian distribution.\n",
    "        - Prior and posterior in the same family ⇒ **Gaussian is a conjugate prior for Gaussian**\n",
    "- Conjugacy of the prior is really a statement about the prior family."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conjugate Prior Family\n",
    "\n",
    "- Let $\\pi$ be a family of prior distributions on $\\Theta$.\n",
    "- Let $P$ be likelihood model with parameter space $\\Theta$.\n",
    "- We say that $\\pi$ **is conjugate to** $P$ if for any prior in $\\pi$, the posterior is always in $\\pi$.\n",
    "- Trivial Example:\n",
    "    - The family of all probability distributions is conjugate to any likelihood model.\n",
    "- Every exponential family has a nontrivial conjugate prior family. (KPM Section 9.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes: A Generative Model for Classification\n",
    "\n",
    "- $\\mathcal{X} = \\{ \\left( X_1, X_2, X_3, X_4 \\in \\{ 0, 1\\}^4 \\right) \\quad \\mathcal{Y} = \\{ 0, 1\\}$ be a class label.\n",
    "- Consider the Bayesian network depicted below:\n",
    "<img src = \"./image_files/naive.png\", width = 300>\n",
    "-  BN structure implies joint distribution factors as:\n",
    "$$p(x_1, x_2, x_3, x_4, y) = p(y) \\ p(x_1 \\mid y) \\ p(x_2 \\mid y) \\ p(x_3 \\mid y) \\ p(x_4 \\mid y)$$\n",
    "- Features $X_1, \\cdots, X_4$ are independent given the class label $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameterized Expression for Joint Distribution\n",
    "\n",
    "- **Parameters:**\n",
    "$$\\mathbf{P}(Y=1)=\\theta_y \\qquad \\mathbf{P}(X_i=1 \\mid Y=1) = \\theta_{i1} \\qquad \\mathbf{P}(X_i=1 \\mid Y=0)=\\theta_{i0}$$\n",
    "- **Joint distribution** is\n",
    "$$\\begin{array}{ll}\n",
    "& & p(x_1, \\cdots, x_d, y) \\\\\n",
    "& = & p(y) \\prod_{i=1}^n p(x_i \\mid y) \\\\\n",
    "& = & (\\theta_y)^y (1-\\theta_y)^{1-y} \\times \\prod_{i=1}^n (\\theta_{i1}^{yx_i})(1-\\theta_{i1})^{y(1-x_i)} (\\theta_{i0}^{(1-y)x_i} (1-\\theta_{i0})^{(1-y)(1-x_i)}\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimators for Naive Bayes\n",
    "\n",
    "- Training set $\\mathcal{D} = \\{ (x^1, y^1), \\cdots, (x^n, y^n)\\}$.\n",
    "- Obvious “plug-in” estimators for the Naive Bayes model are also MLEs:\n",
    "$$\\begin{array}{ll}\n",
    "\\mathbb{P}(Y=1) & \\approx & \\hat \\theta_y & = & {1 \\over n} \\sum_{i=1}^n 1(y^i = 1) \\\\\n",
    "\\mathbb{P}(X_i = 1 \\mid Y = 1) & \\approx & \\hat \\theta_{i1} & = & {\\sum_{j=1}^n 1(y^j = 1 \\ \\text{and} \\ x_i^j = 1) \\over \\sum_{j=1}^n 1(y^j = 1)} \\\\\n",
    "\\mathbb{P}(X_i = 1 \\mid Y = 0) & = & \\hat \\theta_{i0} & = & {\\sum_{j=1}^n 1(y^j = 0 \\ \\text{and} \\ x_i^j = 1) \\over \\sum_{j=1}^n 1(y^j = 0)}\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: SPAM Classification\n",
    "\n",
    "- Label $Y \\in \\mathcal{Y} = \\{SPAM, HAM\\}$.\n",
    "- Features $X_i \\in \\{ 0, 1\\}$.\n",
    "- Bag of words representation:\n",
    "$$X_i = 1(\\text{Email contains word \"Private_Jet\"})$$p\n",
    "- After parameter estimation, prediction done with\n",
    "$$p(SPAM \\mid x) \\propto p(SPAM) \\sum_{i=1}^d \\hat p(x_i \\mid SPAM)$$.\n",
    "- Each $\\hat p(x_i \\mid y)$ is the estimated probability that $x_i$ would be observed (or not) in a SPAM message.\n",
    "- Issue: What if we never see $X_1 = 1$ when $Y = SPAM$ in $\\mathcal{D}$?\n",
    "    - Then whenever we see $X_1 = 1$, we will predict $p(SPAM \\mid x) = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Zero Count Issue\n",
    "\n",
    "- If any conditional probabilities $\\mathbb{P} (X_i = x_i \\mid y)$ get estimated as 0,\n",
    "    - we'll predict 0 probability for some $y$ whenever $x_i$ is observed.\n",
    "- This is bad:\n",
    "    - Never want to predict probability 0 if something is possible.\n",
    "- Worse: This occurrence is not unusual at all for small sample sizes or rare features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laplace Smoothing\n",
    "\n",
    "- One traditional fix to the 0 count issue is called **Laplace Smoothing.**\n",
    "- Idea is to add 1 to every empirical count.\n",
    "- To estimate $\\mathbb{P}(X_i = 1 \\mid Y = 1)$, use\n",
    "$$ \\hat \\theta_{i1} = {1 + \\sum_{j=1}^n 1(y^j = 1 \\ \\text{and} \\ x_i^j = 1) \\over 1+\\sum_{j=1}^n 1(y^j = 1)}$$\n",
    "- The added 1 is called a **pseudocount.**\n",
    "- Like assuming every outcome that can occur was observed at least once.\n",
    "- Seems to solve the problem – but is there a more principled approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Naive Bayes\n",
    "- **Parameters:**\n",
    "$$ \\mathbb{P}(Y = 1) = \\theta_y \\qquad \\mathbb{P}(X_i = 1\\mid Y = 1) = \\theta_{i1} \\qquad \\mathbb{P} (X_i = 1 \\mid Y = 0) = \\theta_{i0}$$\n",
    "- Put a Beta prior distribution on each parameter.\n",
    "- **Option 1:** Use posterior mean as point estimate for each parameter, then continue as before.\n",
    "    - Laplace smoothing is a special case, in which priors are all $Beta(1,1)$.\n",
    "- **Option 2: Go full Bayesian.**\n",
    "    - No parameter estimates. Base everything on posterior $\\theta \\mid \\mathcal{D}$.\n",
    "- Predict with the predictive distribution:\n",
    "$$y \\mid x, \\mathcal{D}$$\n",
    "    - Recall, this is integrating out the parameter θ w.r.t. the posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
