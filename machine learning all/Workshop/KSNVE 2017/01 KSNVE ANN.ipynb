{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<font size='6'><b>(Artificial) Neural Networks\n",
    "</b></font><br><br>\n",
    "\n",
    "<table style=\"border-style: hidden; border-collapse: collapse;\" width = \"100%\"> \n",
    "    <tr style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "        <td width = 65% style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "\n",
    "        </td>\n",
    "        <td width = 35%>\n",
    "        By Prof. Seungchul Lee<br>iSystems Design Lab<br>http://isystems.unist.ac.kr/<br>UNIST\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "</table>\n",
    "\n",
    "Table of Contents\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Artificial Neural Networks (ANN)\n",
    "\n",
    "## 1.1. Recall supervised learning setup\n",
    "\n",
    "- Input features $x^{(i)} \\in \\mathbb{R}^n$\n",
    "\n",
    "- Ouput $y^{(i)}$\n",
    "\n",
    "- Model parameters $\\theta \\in \\mathbb{R}^k$\n",
    "\n",
    "- Hypothesis function $h_{\\theta}: \\mathbb{R}^n \\rightarrow y$\n",
    "\n",
    "- Loss function $\\ell: y \\times y \\rightarrow \\mathbb{R}_+$\n",
    "\n",
    "<br>\n",
    "- Machine learning optimization problem\n",
    "\n",
    "$$ \\min_{\\theta} \\sum_{i=1}^{m}\\ell\\left( h_{\\theta}\\left(x^{(i)}\\right),y^{(i)}\\right)$$\n",
    "\n",
    "$\\quad \\;$(possibly plus some additional regularization)\n",
    "\n",
    "- But, many specialized domains required highly engineered special features\n",
    "\n",
    "## 1.2. Neural networks\n",
    "\n",
    "Neural networks are a simply a machine learning algorithm with a more complex hypothesis class, directly incorporating non-linearity (in the parameters)\n",
    "\n",
    "<br>\n",
    "__Example__: neural network with one hidden layer\n",
    "\n",
    "$$h_{\\theta}(x) = \\Theta^{(2)} f\\left( \\Theta^{(1)}x\\right)$$\n",
    "\n",
    "where $\\Theta^{(1)} \\in \\mathbb{R}^{k \\times n}, \\Theta^{(2)} \\in \\mathbb{R}^{1 \\times k}$ and $f$ is some non-linear function applied elementwise to a vector\n",
    "\n",
    "<table style=\"border-style: hidden; border-collapse: collapse;\" width = \"96%\"> \n",
    "    <tr style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "        <td width = 28% style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "            <img src=\"./image_files/sigmoid_function.png\" width = 250>\n",
    "            $$ g(x) = \\frac{1}{1+e^{-x}}$$\n",
    "        </td>\n",
    "        <td width = 28% style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "            <img src=\"./image_files/tanh_function.png\" width = 250>\n",
    "            <br>\n",
    "            $$ g(x) = \\tanh (x)$$\n",
    "        </td>\n",
    "        <td width = 28% style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "            <img src=\"./image_files/relu_function.png\" width = 250>\n",
    "            <br>\n",
    "            $$ g(x) = \\max (0, x)$$\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Architectures are often shown graphically\n",
    "\n",
    "<img src=\"./image_files/nn_structure.png\" width = 270>\n",
    "\n",
    "- Middle layer $a$ is referred to as the hidden layer, there is nothing in the data that prescribes what values these should take, left up to the algorithm to decide\n",
    "\n",
    "- __Viewed another way: neural networks are like classifiers where the features themselves are also learned__\n",
    "\n",
    "\n",
    "- __Pros__\n",
    "    - No need to manually engineer good features, just let the neural networks handle this part\n",
    "\n",
    "- __Cons__\n",
    "\n",
    "    - Minimizing loss on training data is no longer a convex optimization problem in parameters $\\theta$\n",
    "\n",
    "    - Still need to engineer a good architecture \n",
    "\n",
    "## 1.3. Deep learning\n",
    "\n",
    "\"Deep\" neural networks typically refer to networks with multiple hidden layers\n",
    "\n",
    "<img src=\"./image_files/deep_structure.png\" width = 450>\n",
    "\n",
    "## 1.4. Machine learning and Neural networks (or Deep learning)\n",
    "\n",
    "__Machine Learning__\n",
    "- Hand-crafted features\n",
    "- Depends on expertise\n",
    "    \n",
    "__Deep Learning__\n",
    "- Automatic feature extraction\n",
    "- Depends on network structure\n",
    "\n",
    "<img src=\"./image_files/ML_DL.png\" width = 600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Structure of Neural Networks\n",
    "\n",
    "__The neuron__\n",
    "\n",
    "- The sigmoid equation is what is typically used as a transfer function between neurons. It is similar to the step fuction, but is continuous and differentiable.\n",
    "\n",
    "$$ \\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "- One useful property of this transfer function is the simplicity of computing its derivative.\n",
    "\n",
    "$$\\frac{d}{dx}\\sigma(x) = \\sigma' = \\sigma(x) (1-\\sigma(x))$$\n",
    "\n",
    "__Single input neuron__\n",
    "\n",
    "<img src=\"./image_files/single_neuron.png\" width = 300>\n",
    "\n",
    "$$ O = \\sigma(\\xi \\omega + \\theta) $$\n",
    "\n",
    "__Multiple input neuron__\n",
    "\n",
    "<img src=\"./image_files/multiple_neuron.png\" width = 300>\n",
    "\n",
    "$$ O = \\sigma(\\xi_1 \\omega_1 + \\xi_2 \\omega_2 + \\xi_3 \\omega_3 +\\theta) $$\n",
    "\n",
    "__A neural network__\n",
    "\n",
    "<img src=\"./image_files/nn_03.png\" width = 300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Learning: Backpropagation Algorithm (Optional)\n",
    "\n",
    "__Notation__\n",
    "\n",
    "- $x_j^\\ell$: Input to node $j$ of layer $\\ell$\n",
    "\n",
    "- $W_{ij}^\\ell$: Weight from layer $\\ell - 1$ node $i$ to layer $\\ell$ node $j$\n",
    "\n",
    "- $\\sigma(x) = \\frac{1}{1+e^{-x}}$: Sigmoid transfer function\n",
    "\n",
    "- $\\theta_j^{\\ell}$: Bias of node $j$ of layer $\\ell$\n",
    "\n",
    "- $O_j^{\\ell}$: Output of node $j$ in layer $\\ell$\n",
    "\n",
    "- $t_j$: Target value of node $j$ of the output layer\n",
    "\n",
    "<br>\n",
    "<font size='4'><b>The error calculation</b></font>\n",
    "\n",
    "Given a set of training data points $t_k$ and output layer output $O_k$ we can write the error as\n",
    "\n",
    "$$ E = \\frac{1}{2} \\sum_{k \\in K} (O_k - t_k)^2$$\n",
    "\n",
    "\n",
    "- Forward propagation \n",
    "    - the initial information propagates up to the hidden units at each layer and finally produces output\n",
    "- Backpropagation\n",
    "    - allows the information from the cost to flow backwards through the network in order to compute the gradients\n",
    "\n",
    "<img src=\"./image_files/animate_backpropa.gif\" width = 450>\n",
    "\n",
    "\n",
    "We want to calculate $\\frac{\\partial E}{\\partial W_{jk}^{\\ell}}$, the rate of change of the error with respect to the given connective weight, so we can minimize it.\n",
    "\n",
    "Now we consider two cases: the node is an output node, or it is in a hidden layer\n",
    "\n",
    "__1) Output layer node__\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial E}{\\partial W_{jk}} &= \\frac{\\partial}{\\partial W_{jk}} \\frac{1}{2} (O_k - t_k)^2 = (O_k - t_k)\\frac{\\partial}{\\partial W_{jk}} O_k = (O_k - t_k)\\frac{\\partial}{\\partial W_{jk}} \\sigma(x_k)\\\\\n",
    "&= (O_k - t_k) \\sigma(x_k) (1-\\sigma(x_k)) \\frac{\\partial}{\\partial W_{jk}} x_k \\\\\n",
    "&= (O_k - t_k) O_k (1 - O_k) O_j\n",
    "\\end{align*}\n",
    "\n",
    "$\\quad$For notation purposes, I will define $\\delta_k$ to be the expression $(O_k - t_k) O_k (1 - O_k)$, so we can rewrite the equation above as\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial W_{jk}} = O_j \\delta_k $$\n",
    "\n",
    "__2) Hidden layer node__\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial E}{\\partial W_{ij}} &= \\frac{\\partial}{\\partial W_{ij}} \\frac{1}{2} \\sum_{k \\in K} (O_k - t_k)^2 = \\sum_{k \\in K} (O_k - t_k)\\frac{\\partial}{\\partial W_{ij}} O_k = \\sum_{k \\in K} (O_k - t_k)\\frac{\\partial}{\\partial W_{ij}} \\sigma(x_k)\\\\\n",
    "&= \\sum_{k \\in K} (O_k - t_k) \\sigma(x_k) (1-\\sigma(x_k)) \\frac{\\partial}{\\partial W_{ij}} x_k \\\\\n",
    "&= \\sum_{k \\in K} (O_k - t_k) O_k (1 - O_k) \\frac{\\partial x_k}{\\partial O_j}\\cdot \\frac{\\partial O_j}{\\partial W_{ij}} = \\sum_{k \\in K} (O_k - t_k) O_k (1 - O_k) W_{jk}\\cdot \\frac{\\partial O_j}{\\partial W_{ij}}\\\\\n",
    "&= \\frac{\\partial O_j}{\\partial W_{ij}} \\cdot \\sum_{k \\in K} (O_k - t_k) O_k (1 - O_k) W_{jk}\\\\\n",
    "&= O_j (1-O_j)\\frac{\\partial x_j}{\\partial W_{ij}} \\cdot \\sum_{k \\in K} (O_k - t_k) O_k (1 - O_k) W_{jk}\\\\\n",
    "&= O_j (1-O_j)O_i \\cdot \\sum_{k \\in K} (O_k - t_k) O_k (1 - O_k) W_{jk}\\\\\n",
    "&= O_i O_j (1-O_j) \\sum_{k \\in K} \\delta_k W_{jk}\n",
    "\\end{align*}\n",
    "\n",
    "$\\quad$Similar to before we will now define all terms besides $O_i$ to be $\\delta_j = O_j (1-O_j) \\sum_{k \\in K} \\delta_k W_{jk}$, so we have\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial W_{ij}} = O_i \\delta_j$$\n",
    "\n",
    "\n",
    "__How weights affect errors__\n",
    "\n",
    "- For an output layer node $k \\in K$\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial W_{jk}} = O_j \\delta_k $$\n",
    "\n",
    "$\\quad \\;\\,$where $$\\delta_k = (O_k - t_k) O_k (1 - O_k)$$\n",
    "\n",
    "- For a hidden layer node $j \\in J$\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial W_{ij}} = O_i \\delta_j$$\n",
    "\n",
    "$\\quad \\;\\,$where $$\\delta_j = O_j (1-O_j) \\sum_{k \\in K} \\delta_k W_{jk}$$\n",
    "\n",
    "__What about the bias?__\n",
    "\n",
    "If we incorporate the bias term $\\theta$ into the equation you will find that\n",
    "\n",
    "$$ \\frac{\\partial O}{\\partial \\theta} = 1$$\n",
    "\n",
    "This is why we view the bias term as output from a node which is always one. This holds for any layer $\\ell$, a substitution into the previous equations gives us that\n",
    "\n",
    "$$ \\frac{\\partial E}{\\partial \\theta} = \\delta_{\\ell}$$\n",
    "\n",
    "__(Stochastic) Gradient Descent__\n",
    "\n",
    "- Negative gradients points directly downhill of cost function\n",
    "- We can decrease cost by moving in the direction of the negative gradient ($\\eta$ is a learning rate)\n",
    "\n",
    "$$ W:= W - \\eta \\nabla_{W} \\left( h_{W} \\left(x^{(i)}\\right),y^{(i)}\\right)$$\n",
    "\n",
    "<br>\n",
    "<table style=\"border-style: hidden; border-collapse: collapse;\" width = \"96%\"> \n",
    "    <tr style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "        <td width = 48% style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "            <img src=\"./image_files/GradientDescent.png\" width = 450>\n",
    "        </td>\n",
    "        <td width = 48% style=\"border-style: hidden; border-collapse: collapse;\">\n",
    "            <img src=\"./image_files/GradientDescent_3d.png\" width = 450>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "<br>\n",
    "<font size='4'><b>The backpropagation algorithm using gradient descent</b></font>\n",
    "\n",
    "1. Run the network forward with your input data to get the netwrok output\n",
    "\n",
    "2. For each output node compute\n",
    "$$\\delta_k = (O_k - t_k) O_k (1 - O_k)$$\n",
    "3. For eatch hidden node calculate\n",
    "$$\\delta_j = O_j (1-O_j) \\sum_{k \\in K} \\delta_k W_{jk}$$\n",
    "4. Update the weights and biases as follows<br>\n",
    "Given\n",
    "$$\\begin{align*}\n",
    "\\Delta W &= -\\eta \\delta_{\\ell} O_{\\ell -1}\\\\\n",
    "\\Delta \\theta &= -\\eta \\delta_{\\ell}\n",
    "\\end{align*}$$\n",
    "apply\n",
    "$$\\begin{align*}\n",
    "W &\\leftarrow W + \\Delta W \\\\\n",
    "\\theta &\\leftarrow \\theta + \\Delta \\theta\n",
    "\\end{align*}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Deep Learning Libraries\n",
    "\n",
    "__Caffe__\n",
    "\n",
    "<img src=\"./image_files/Caffe_logo.png\" width = 200>\n",
    "\n",
    "- Platform: Linux, Mac OS, Windows\n",
    "- Written in: C++\n",
    "- Interface: Python, MATLAB\n",
    "\n",
    "<br>\n",
    "__Theano__\n",
    "<img src=\"./image_files/Theano_logo.png\" width = 200>\n",
    "\n",
    "- Platform: Cross-platform\n",
    "- Written in: Python\n",
    "- Interface: Python\n",
    "\n",
    "<br>\n",
    "__Tensorflow__\n",
    "\n",
    "<img src=\"./image_files/Tensorflow_logo.png\" width = 250>\n",
    "\n",
    "- Platform: Linux, Mac OS, Windows\n",
    "- Written in: C++, Python\n",
    "- Interface: Python, C/C++, Java, Go, R\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. TensorFlow\n",
    "\n",
    "- `tensorflow` is an open-source software library for deep learning.\n",
    "\n",
    "## 5.1. Computational Graph\n",
    "- `tf.constant`\n",
    "- `tf.Variable`\n",
    "- `tf.placeholder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "a = tf.constant([1, 2, 3])\n",
    "b = tf.constant([4, 5, 6])\n",
    "\n",
    "A = a + b\n",
    "B = a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add:0' shape=(3,) dtype=int32>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'mul:0' shape=(3,) dtype=int32>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run any of the three defined operations, we need to create a session for that graph. The session will also allocate memory to store the current value of the variable.\n",
    "\n",
    "<img src=\"./image_files/tf_session.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 7, 9], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4, 10, 18], dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.Variable` is regarded as the decision variable in optimization. We should initialize variables to use `tf.Variable`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = tf.Variable([1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of __`tf.placeholder`__ must be fed using the `feed_dict` optional argument to `Session.run()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  2.],\n",
       "       [ 3.,  4.]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(x, feed_dict={x : [[1,2],[3,4]]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. ANN with TensorFlow\n",
    "\n",
    "- MNIST (Mixed National Institute of Standards and Technology database) database\n",
    "    - Handwritten digit database\n",
    "    - $28 \\times 28$ gray scaled image\n",
    "    - Flattened array into a vector of $28 \\times 28 = 784$\n",
    "    \n",
    "<img src=\"./image_files/mnist_digits.png\" width = 450>\n",
    "\n",
    "<img src=\"./image_files/MNIST-Matrix.png\" width = 450>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<img src=\"./image_files/MNIST_neuralnet_image.png\" width = 500>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<center><iframe src=\"https://www.youtube.com/embed/z0bynQjEpII?start=2088&end=3137\" \n",
       "width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen></iframe></center>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<center><iframe src=\"https://www.youtube.com/embed/z0bynQjEpII?start=2088&end=3137\" \n",
    "width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen></iframe></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Library\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Load MNIST Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Download MNIST data from tensorflow tutorial example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALgAAADECAYAAAAs0+t9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1162918d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_x, train_y = mnist.train.next_batch(10)\n",
    "img = train_x[3,:].reshape(28,28)\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.imshow(img,'gray')\n",
    "plt.title(\"Label : {}\".format(np.argmax(train_y[3])))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels : [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print ('Train labels : {}'.format(train_y[3, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Build a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ First, the layer performs several matrix multiplication to produce a set of linear activations __\n",
    "\n",
    "<img src=\"./image_files/linear_sum2.png\" width = 320>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y_j = \\left(\\sum\\limits_i \\omega_{ij}x_i\\right) + b_j$$\n",
    "\n",
    "$$\\mathcal{y} = \\omega^T \\mathcal{x} + \\mathcal{b}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```python\n",
    "# hidden1 = tf.matmul(x, weights['hidden1']) + biases['hidden1']\n",
    "hidden1 = tf.add(tf.matmul(x, weights['hidden1']), biases['hidden1'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ Second, each linear activation is running through a nonlinear activation function __\n",
    "\n",
    "<img src=\"./image_files/ReLU.png\" width = 500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```python\n",
    "hidden1 = tf.nn.relu(hidden1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Third, predict values with an affine transformation__\n",
    "\n",
    "<img src=\"./image_files/classification.png\" width = 450>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```python\n",
    "# output = tf.matmul(hidden1, weights['output']) + biases['output']\n",
    "output = tf.add(tf.matmul(hidden1, weights['output']), biases['output'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4. Define the ANN's Shape\n",
    "\n",
    "- Input size\n",
    "- Hidden layer size\n",
    "- The number of classes\n",
    "\n",
    "<img src=\"./image_files/MNIST_neuralnet_image.png\" width = 500>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_input = 28*28\n",
    "n_hidden1 = 100\n",
    "n_output = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5. Define Weights, Biases and Network\n",
    "- Define parameters based on predefined layer size\n",
    "- Initialize with normal distribution with $\\mu = 0$ and $\\sigma = 0.1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'hidden1' : tf.Variable(tf.random_normal([n_input, n_hidden1], stddev = 0.1)),\n",
    "    'output' : tf.Variable(tf.random_normal([n_hidden1, n_output], stddev = 0.1)),\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'hidden1' : tf.Variable(tf.random_normal([n_hidden1], stddev = 0.1)),\n",
    "    'output' : tf.Variable(tf.random_normal([n_output], stddev = 0.1)),\n",
    "}\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define Network\n",
    "def build_model(x, weights, biases):\n",
    "    # first hidden layer\n",
    "    hidden1 = tf.add(tf.matmul(x, weights['hidden1']), biases['hidden1'])\n",
    "    # non linear activate function\n",
    "    hidden1 = tf.nn.relu(hidden1)\n",
    "    \n",
    "    # Output layer with linear activation\n",
    "    output = tf.add(tf.matmul(hidden1, weights['output']), biases['output'])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6. Define Cost, Initializer and Optimizer\n",
    "\n",
    "__Loss__\n",
    "- Classification: Cross entropy\n",
    "    - Equivalent to apply logistic regression\n",
    "$$ -\\frac{1}{N}\\sum_{i=1}^{N}y^{(i)}\\log(h_{\\theta}\\left(x^{(i)}\\right)) + (1-y^{(i)})\\log(1-h_{\\theta}\\left(x^{(i)}\\right)) $$\n",
    "\n",
    "__Initializer__\n",
    "- Initialize all the empty variables\n",
    "    \n",
    "__Optimizer__\n",
    "- AdamOptimizer: the most popular optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define Cost\n",
    "LR = 0.0001\n",
    "\n",
    "pred = build_model(x, weights, biases)\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y)\n",
    "loss = tf.reduce_mean(loss)\n",
    "\n",
    "# optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "optm = tf.train.AdamOptimizer(LR).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.7. Summary of Model\n",
    "\n",
    "<br>\n",
    "<img src=\"./image_files/cnn_summary of model.png\" width = 500>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.8. Define Configuration\n",
    "- Define parameters for training ANN\n",
    "     - `n_batch`: batch size for stochastic gradient descent\n",
    "     - `n_iter`: the number of learning steps\n",
    "     - `n_prt`: check loss for every `n_prt` iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_batch = 50     # Batch Size\n",
    "n_iter = 2500    # Learning Iteration\n",
    "n_prt = 250      # Print Cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.9. Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter : 0\n",
      "Cost : 2.4568586349487305\n",
      "Iter : 250\n",
      "Cost : 1.4568665027618408\n",
      "Iter : 500\n",
      "Cost : 0.7992963194847107\n",
      "Iter : 750\n",
      "Cost : 0.6279309988021851\n",
      "Iter : 1000\n",
      "Cost : 0.4135037958621979\n",
      "Iter : 1250\n",
      "Cost : 0.4587893784046173\n",
      "Iter : 1500\n",
      "Cost : 0.3380467891693115\n",
      "Iter : 1750\n",
      "Cost : 0.4487552344799042\n",
      "Iter : 2000\n",
      "Cost : 0.39212024211883545\n",
      "Iter : 2250\n",
      "Cost : 0.3634752631187439\n"
     ]
    }
   ],
   "source": [
    "# Run initialize\n",
    "# config = tf.ConfigProto(allow_soft_placement=True)  # GPU Allocating policy\n",
    "# sess = tf.Session(config=config)\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Training cycle\n",
    "for epoch in range(n_iter):\n",
    "    train_x, train_y = mnist.train.next_batch(n_batch)\n",
    "    sess.run(optm, feed_dict={x: train_x,  y: train_y}) \n",
    "    \n",
    "    if epoch % n_prt == 0:\n",
    "        c = sess.run(loss, feed_dict={x : train_x,  y : train_y})\n",
    "        print (\"Iter : {}\".format(epoch))\n",
    "        print (\"Cost : {}\".format(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.10. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 92.0%\n"
     ]
    }
   ],
   "source": [
    "test_x, test_y = mnist.test.next_batch(100)\n",
    "\n",
    "my_pred = sess.run(pred, feed_dict={x : test_x})\n",
    "my_pred = np.argmax(my_pred, axis=1)\n",
    "\n",
    "labels = np.argmax(test_y, axis=1)\n",
    "\n",
    "accr = np.mean(np.equal(my_pred, labels))\n",
    "print(\"Accuracy : {}%\".format(accr*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO4AAADuCAYAAAA+7jsiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAABsBJREFUeJzt3U+IzP8Dx/EZf5bSulAo7e8mJxyUg7AnraT2sraEo5ZS\nK5GbmziQg9LaIgeHvTmyJdok5U+ODlI/EuWbVlssLfM9fY/znpndMbuv9XhcX/Pho+3po95mplqr\n1SpAliXzfQNA64QLgYQLgYQLgYQLgYQLgYQLgYQLgYQLgZa18uJqteq/WcEfVqvVqo1e44kLgYQL\ngYQLgYQLgYQLgYQLgYQLgYQLgYQLgYQLgYQLgYQLgYQLgYQLgVp6Wx+dd/78+eJ+9OjR4j44OFjc\nnz9/3vI9Mf88cSGQcCGQcCGQcCGQcCGQcCGQ46AFoLe3t+527Nix4rXfvn0r7tu3by/ujoMyeeJC\nIOFCIOFCIOFCIOFCIOFCIOFCoGqt1vwX8Pm2vtnp7u4u7m/fvq273b59u3jtuXPninujn++vX7+K\nO53n2/pgkRIuBBIuBBIuBBIuBBIuBBIuBPJ+3A44fvx4cZ+enq67Xb58uXjtzMzMrO6JbJ64EEi4\nEEi4EEi4EEi4EEi4EEi4EMg5bgecPXu2uI+MjNTdPn782O7bYRHwxIVAwoVAwoVAwoVAwoVAwoVA\njoPaoNHHr65YsaK4v379up23w1/AExcCCRcCCRcCCRcCCRcCCRcCCRcCOcdtg76+vjldf+/evTbd\nCX8LT1wIJFwIJFwIJFwIJFwIJFwIJFwI5By3DYaGhor7jx8/ivvnz5/beTv8BTxxIZBwIZBwIZBw\nIZBwIZBwIZBwIZBz3CZUq9XivmbNmuL+4MGDdt7OgtHb21vcBwcH5/TrT05O1t0mJiaK1zZ6j3Ot\nVpvVPS0UnrgQSLgQSLgQSLgQSLgQSLgQSLgQyDluEzZs2FDct2zZUtwvXbrUzttpq66uruJ+8eLF\nutvw8HDx2nfv3hX3qampWV9/4sSJ4rUDAwPFfXx8vLgvdJ64EEi4EEi4EEi4EEi4EEi4EMhxUAfM\n58evLllS/rt5dHS0uB85cqTu1uhI5tatW8W90cfWlvT39xf3kZGR4r5t27bi/vXr15bvqZM8cSGQ\ncCGQcCGQcCGQcCGQcCGQcCGQc9wm9PT0zOn6Z8+etelOWnft2rXivnfv3lnvjT529k9+BOr9+/eL\n+8qVK4v7qlWrirtzXKDthAuBhAuBhAuBhAuBhAuBhAuBnOM2Yd26dfN9C3WtX7++uB84cKC4Hzp0\nqLg/fPiw5XvqhO/fvxf3N2/eFPddu3YV97GxsZbvqZM8cSGQcCGQcCGQcCGQcCGQcCGQcCGQc9wm\n/Pz5c07Xb9y4sbjP5b2fhw8fLu6NznmfPHky6987WXd393zfwpx44kIg4UIg4UIg4UIg4UIg4UIg\n4UIg57hNePz4cXH/9OlTcR8aGiruJ0+ebPme/vP06dPivmxZ+Ue8Z8+e4j4+Pt7yPXVCoz/X6tWr\ni/vk5GQ7b6fjPHEhkHAhkHAhkHAhkHAhkHAhkOOgJkxNTRX3Dx8+FPeBgYHifurUqbrbzMxM8dov\nX74U99+/fxf3pUuXFveFqtERWqO3Mzb6itCFzhMXAgkXAgkXAgkXAgkXAgkXAgkXAlVrtVrzL65W\nm3/xX2RwcLC437lzp7hfv3697jaXt/xVKpXKjRs3ivv+/fuL+82bN+tu09PTs7qn/zR6u2RPT0/d\nbXR0tHjtvn37ivtC/frQSqVSqdVq1Uav8cSFQMKFQMKFQMKFQMKFQMKFQMKFQM5xO2BsbKy49/f3\n192uXr1avPbKlSvFvdFXePb19RX3tWvX1t2q1fJxY1dXV3HftGlTcd+6dWvd7fTp08VrX7x4UdwX\nMue4sEgJFwIJFwIJFwIJFwIJFwIJFwI5x+2A5cuXF/cLFy7U3YaHh4vXNvpM57t37xb39+/fF/eS\n0vlzpVKp7Ny5s7g3+mzjM2fO1N1evXpVvDaZc1xYpIQLgYQLgYQLgYQLgYQLgRwHLXA7duwo7gcP\nHizuu3fvLu6bN28u7o8ePaq7vXz5snjtxMREcW/0EamNviJ0sXIcBIuUcCGQcCGQcCGQcCGQcCGQ\ncCGQc1xYYJzjwiIlXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgk\nXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgk\nXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAi0rMXX/1OpVP7/J24EqFQqlcr/mnlRtVar\n/ekbAdrMP5UhkHAhkHAhkHAhkHAhkHAhkHAhkHAhkHAh0L+lfC/Jvaa25wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11f810b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction : 6\n",
      "Probability : [ 0.    0.01  0.04  0.    0.01  0.    0.93  0.    0.01  0.  ]\n"
     ]
    }
   ],
   "source": [
    "test_x, test_y = mnist.test.next_batch(1)\n",
    "logits = sess.run(tf.nn.softmax(pred), feed_dict={x : test_x})\n",
    "predict = np.argmax(logits)\n",
    "\n",
    "plt.imshow(test_x.reshape(28,28), 'gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()\n",
    "\n",
    "print('Prediction : {}'.format(predict))\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "print('Probability : {}'.format(logits.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
